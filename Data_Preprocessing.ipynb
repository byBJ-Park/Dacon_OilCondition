{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install verstack","metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"scrolled":true,"execution":{"iopub.status.busy":"2022-12-10T06:33:14.206557Z","iopub.execute_input":"2022-12-10T06:33:14.208094Z","iopub.status.idle":"2022-12-10T06:34:53.537438Z","shell.execute_reply.started":"2022-12-10T06:33:14.207978Z","shell.execute_reply":"2022-12-10T06:34:53.535613Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Collecting verstack\n  Downloading verstack-3.4.0.tar.gz (9.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from verstack) (1.3.5)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from verstack) (1.21.6)\nRequirement already satisfied: xgboost in /opt/conda/lib/python3.7/site-packages (from verstack) (1.6.2)\nRequirement already satisfied: scikit-learn<=1.1.3,>=0.23.2 in /opt/conda/lib/python3.7/site-packages (from verstack) (1.0.2)\nRequirement already satisfied: lightgbm<=3.3.2,>=3.3.0 in /opt/conda/lib/python3.7/site-packages (from verstack) (3.3.2)\nRequirement already satisfied: optuna<=3.0.4,>=2.10.0 in /opt/conda/lib/python3.7/site-packages (from verstack) (3.0.3)\nRequirement already satisfied: plotly<=5.11.0,>=5.3.1 in /opt/conda/lib/python3.7/site-packages (from verstack) (5.10.0)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from verstack) (3.5.3)\nRequirement already satisfied: python-dateutil<=2.8.2,>=2.8.1 in /opt/conda/lib/python3.7/site-packages (from verstack) (2.8.2)\nCollecting holidays==0.11.3.1\n  Downloading holidays-0.11.3.1-py3-none-any.whl (155 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.7/155.7 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: mlxtend in /opt/conda/lib/python3.7/site-packages (from verstack) (0.21.0)\nCollecting tensorflow<=2.11.0,>=2.7.0\n  Downloading tensorflow-2.11.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m588.3/588.3 MB\u001b[0m \u001b[31m975.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting keras<=2.11.0,>=2.7.0\n  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting category_encoders<=2.5.1,>=2.4.0\n  Downloading category_encoders-2.5.1-py2.py3-none-any.whl (71 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.3/71.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: convertdate>=2.3.0 in /opt/conda/lib/python3.7/site-packages (from holidays==0.11.3.1->verstack) (2.4.0)\nRequirement already satisfied: korean-lunar-calendar in /opt/conda/lib/python3.7/site-packages (from holidays==0.11.3.1->verstack) (0.3.1)\nRequirement already satisfied: hijri-converter in /opt/conda/lib/python3.7/site-packages (from holidays==0.11.3.1->verstack) (2.2.4)\nRequirement already satisfied: statsmodels>=0.9.0 in /opt/conda/lib/python3.7/site-packages (from category_encoders<=2.5.1,>=2.4.0->verstack) (0.13.2)\nRequirement already satisfied: scipy>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from category_encoders<=2.5.1,>=2.4.0->verstack) (1.7.3)\nRequirement already satisfied: patsy>=0.5.1 in /opt/conda/lib/python3.7/site-packages (from category_encoders<=2.5.1,>=2.4.0->verstack) (0.5.2)\nRequirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from lightgbm<=3.3.2,>=3.3.0->verstack) (0.37.1)\nRequirement already satisfied: importlib-metadata<5.0.0 in /opt/conda/lib/python3.7/site-packages (from optuna<=3.0.4,>=2.10.0->verstack) (4.13.0)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from optuna<=3.0.4,>=2.10.0->verstack) (21.3)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.7/site-packages (from optuna<=3.0.4,>=2.10.0->verstack) (6.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from optuna<=3.0.4,>=2.10.0->verstack) (4.64.0)\nRequirement already satisfied: cliff in /opt/conda/lib/python3.7/site-packages (from optuna<=3.0.4,>=2.10.0->verstack) (3.10.1)\nRequirement already satisfied: colorlog in /opt/conda/lib/python3.7/site-packages (from optuna<=3.0.4,>=2.10.0->verstack) (6.7.0)\nRequirement already satisfied: cmaes>=0.8.2 in /opt/conda/lib/python3.7/site-packages (from optuna<=3.0.4,>=2.10.0->verstack) (0.8.2)\nRequirement already satisfied: alembic>=1.5.0 in /opt/conda/lib/python3.7/site-packages (from optuna<=3.0.4,>=2.10.0->verstack) (1.8.1)\nRequirement already satisfied: sqlalchemy>=1.3.0 in /opt/conda/lib/python3.7/site-packages (from optuna<=3.0.4,>=2.10.0->verstack) (1.4.39)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->verstack) (2022.1)\nRequirement already satisfied: tenacity>=6.2.0 in /opt/conda/lib/python3.7/site-packages (from plotly<=5.11.0,>=5.3.1->verstack) (8.0.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil<=2.8.2,>=2.8.1->verstack) (1.15.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn<=1.1.3,>=0.23.2->verstack) (3.1.0)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn<=1.1.3,>=0.23.2->verstack) (1.0.1)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow<=2.11.0,>=2.7.0->verstack) (0.2.0)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<=2.11.0,>=2.7.0->verstack) (1.1.0)\nCollecting libclang>=13.0.0\n  Downloading libclang-14.0.6-py2.py3-none-manylinux2010_x86_64.whl (14.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from tensorflow<=2.11.0,>=2.7.0->verstack) (59.8.0)\nRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<=2.11.0,>=2.7.0->verstack) (3.7.0)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<=2.11.0,>=2.7.0->verstack) (1.6.3)\nCollecting tensorboard<2.12,>=2.11\n  Downloading tensorboard-2.11.0-py3-none-any.whl (6.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hCollecting tensorflow-estimator<2.12,>=2.11.0\n  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.2/439.2 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting flatbuffers>=2.0\n  Downloading flatbuffers-22.12.6-py2.py3-none-any.whl (26 kB)\nCollecting tensorflow-io-gcs-filesystem>=0.23.1\n  Downloading tensorflow_io_gcs_filesystem-0.28.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m63.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow<=2.11.0,>=2.7.0->verstack) (1.43.0)\nRequirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<=2.11.0,>=2.7.0->verstack) (1.12.1)\nRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow<=2.11.0,>=2.7.0->verstack) (4.4.0)\nCollecting absl-py>=1.0.0\n  Downloading absl_py-1.3.0-py3-none-any.whl (124 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.6/124.6 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow<=2.11.0,>=2.7.0->verstack) (0.4.0)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow<=2.11.0,>=2.7.0->verstack) (3.3.0)\nRequirement already satisfied: protobuf<3.20,>=3.9.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow<=2.11.0,>=2.7.0->verstack) (3.19.4)\nRequirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->verstack) (3.0.9)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->verstack) (0.11.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->verstack) (1.4.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->verstack) (9.1.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->verstack) (4.33.3)\nRequirement already satisfied: importlib-resources in /opt/conda/lib/python3.7/site-packages (from alembic>=1.5.0->optuna<=3.0.4,>=2.10.0->verstack) (5.8.0)\nRequirement already satisfied: Mako in /opt/conda/lib/python3.7/site-packages (from alembic>=1.5.0->optuna<=3.0.4,>=2.10.0->verstack) (1.2.3)\nRequirement already satisfied: pymeeus<=1,>=0.3.13 in /opt/conda/lib/python3.7/site-packages (from convertdate>=2.3.0->holidays==0.11.3.1->verstack) (0.5.11)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata<5.0.0->optuna<=3.0.4,>=2.10.0->verstack) (3.8.0)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.7/site-packages (from sqlalchemy>=1.3.0->optuna<=3.0.4,>=2.10.0->verstack) (1.1.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow<=2.11.0,>=2.7.0->verstack) (2.2.2)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow<=2.11.0,>=2.7.0->verstack) (1.35.0)\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow<=2.11.0,>=2.7.0->verstack) (1.8.1)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow<=2.11.0,>=2.7.0->verstack) (3.3.7)\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow<=2.11.0,>=2.7.0->verstack) (0.4.6)\nRequirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow<=2.11.0,>=2.7.0->verstack) (0.6.1)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow<=2.11.0,>=2.7.0->verstack) (2.28.1)\nRequirement already satisfied: autopage>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from cliff->optuna<=3.0.4,>=2.10.0->verstack) (0.5.1)\nRequirement already satisfied: pbr!=2.1.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from cliff->optuna<=3.0.4,>=2.10.0->verstack) (5.10.0)\nRequirement already satisfied: PrettyTable>=0.7.2 in /opt/conda/lib/python3.7/site-packages (from cliff->optuna<=3.0.4,>=2.10.0->verstack) (3.3.0)\nRequirement already satisfied: stevedore>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from cliff->optuna<=3.0.4,>=2.10.0->verstack) (3.5.1)\nRequirement already satisfied: cmd2>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from cliff->optuna<=3.0.4,>=2.10.0->verstack) (2.4.2)\nRequirement already satisfied: attrs>=16.3.0 in /opt/conda/lib/python3.7/site-packages (from cmd2>=1.0.0->cliff->optuna<=3.0.4,>=2.10.0->verstack) (21.4.0)\nRequirement already satisfied: pyperclip>=1.6 in /opt/conda/lib/python3.7/site-packages (from cmd2>=1.0.0->cliff->optuna<=3.0.4,>=2.10.0->verstack) (1.8.2)\nRequirement already satisfied: wcwidth>=0.1.7 in /opt/conda/lib/python3.7/site-packages (from cmd2>=1.0.0->cliff->optuna<=3.0.4,>=2.10.0->verstack) (0.2.5)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<=2.11.0,>=2.7.0->verstack) (4.2.4)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<=2.11.0,>=2.7.0->verstack) (4.8)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<=2.11.0,>=2.7.0->verstack) (0.2.7)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow<=2.11.0,>=2.7.0->verstack) (1.3.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<=2.11.0,>=2.7.0->verstack) (3.3)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<=2.11.0,>=2.7.0->verstack) (1.26.12)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<=2.11.0,>=2.7.0->verstack) (2.1.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<=2.11.0,>=2.7.0->verstack) (2022.9.24)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.7/site-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow<=2.11.0,>=2.7.0->verstack) (2.1.1)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<=2.11.0,>=2.7.0->verstack) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow<=2.11.0,>=2.7.0->verstack) (3.2.0)\nBuilding wheels for collected packages: verstack\n  Building wheel for verstack (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for verstack: filename=verstack-3.4.0-py3-none-any.whl size=88899 sha256=f1d87cbc102bdeae0b649654c1eccaf0b5fdd8872bc482ca9553f8d276f988a4\n  Stored in directory: /root/.cache/pip/wheels/eb/74/c3/8442e6dca42aca504ba47680ddb0a1ea52cd151117d0f00dfb\nSuccessfully built verstack\nInstalling collected packages: libclang, flatbuffers, tensorflow-io-gcs-filesystem, tensorflow-estimator, keras, absl-py, holidays, tensorboard, category_encoders, tensorflow, verstack\n  Attempting uninstall: flatbuffers\n    Found existing installation: flatbuffers 1.12\n    Uninstalling flatbuffers-1.12:\n      Successfully uninstalled flatbuffers-1.12\n  Attempting uninstall: tensorflow-estimator\n    Found existing installation: tensorflow-estimator 2.6.0\n    Uninstalling tensorflow-estimator-2.6.0:\n      Successfully uninstalled tensorflow-estimator-2.6.0\n  Attempting uninstall: keras\n    Found existing installation: keras 2.6.0\n    Uninstalling keras-2.6.0:\n      Successfully uninstalled keras-2.6.0\n  Attempting uninstall: absl-py\n    Found existing installation: absl-py 0.15.0\n    Uninstalling absl-py-0.15.0:\n      Successfully uninstalled absl-py-0.15.0\n  Attempting uninstall: holidays\n    Found existing installation: holidays 0.16\n    Uninstalling holidays-0.16:\n      Successfully uninstalled holidays-0.16\n  Attempting uninstall: tensorboard\n    Found existing installation: tensorboard 2.10.1\n    Uninstalling tensorboard-2.10.1:\n      Successfully uninstalled tensorboard-2.10.1\n  Attempting uninstall: category_encoders\n    Found existing installation: category-encoders 2.5.1.post0\n    Uninstalling category-encoders-2.5.1.post0:\n      Successfully uninstalled category-encoders-2.5.1.post0\n  Attempting uninstall: tensorflow\n    Found existing installation: tensorflow 2.6.4\n    Uninstalling tensorflow-2.6.4:\n      Successfully uninstalled tensorflow-2.6.4\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntfx-bsl 1.9.0 requires pyarrow<6,>=1, but you have pyarrow 8.0.0 which is incompatible.\ntensorflow-transform 1.9.0 requires pyarrow<6,>=1, but you have pyarrow 8.0.0 which is incompatible.\ntensorflow-transform 1.9.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<2.10,>=1.15.5, but you have tensorflow 2.11.0 which is incompatible.\ntensorflow-io 0.21.0 requires tensorflow<2.7.0,>=2.6.0, but you have tensorflow 2.11.0 which is incompatible.\ntensorflow-io 0.21.0 requires tensorflow-io-gcs-filesystem==0.21.0, but you have tensorflow-io-gcs-filesystem 0.28.0 which is incompatible.\nprophet 1.1.1 requires holidays>=0.14.2, but you have holidays 0.11.3.1 which is incompatible.\nfeaturetools 1.11.1 requires holidays>=0.13, but you have holidays 0.11.3.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed absl-py-1.3.0 category_encoders-2.5.1 flatbuffers-22.12.6 holidays-0.11.3.1 keras-2.11.0 libclang-14.0.6 tensorboard-2.11.0 tensorflow-2.11.0 tensorflow-estimator-2.11.0 tensorflow-io-gcs-filesystem-0.28.0 verstack-3.4.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \npd.set_option('max_columns', None)\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport joblib","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-12-10T06:34:53.540431Z","iopub.execute_input":"2022-12-10T06:34:53.540954Z","iopub.status.idle":"2022-12-10T06:34:53.617865Z","shell.execute_reply.started":"2022-12-10T06:34:53.540893Z","shell.execute_reply":"2022-12-10T06:34:53.616721Z"},"trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/input/michaels-order1/Stacker_2.pkl\n/kaggle/input/michaels-order1/Stacker_1.pkl\n/kaggle/input/michaels-order1/stacked_valid_2\n/kaggle/input/michaels-order1/stacked_valid_1\n/kaggle/input/michaels-order1/stacked_train_1\n/kaggle/input/michaels-order1/stacked_train_2\n/kaggle/input/daconio-117/sample_submission.csv\n/kaggle/input/daconio-117/train_df.csv\n/kaggle/input/daconio-117/train.csv\n/kaggle/input/daconio-117/test.csv\n/kaggle/input/daconio-117/test_df.csv\n/kaggle/input/only-stocked-df/only_stacked_train_full_feature.csv\n/kaggle/input/only-stocked-df/only_stacked_train_test_feature.csv\n/kaggle/input/only-stocked-df/only_stacked_test.csv\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"1. train null 값 처리와 feature에 대한 scaling\n\n2. train_valid 나누기: train_set과 valid set나눈 후 저장 하던지 random_state로 통일\n\n3. 전체 특성을 이용하여 train_set을 Stacker로 fit_transform해 줌 => Stacker_1, stacked_train_1라 명명 후 둘 다 저장\n\n4. 테스트 특성만 이용하여 train_set을 Stacker로 fit_transform해 줌 => Stacker_2, stacked_train_2라 명명 후 둘 다 저장\n\n5. Stacker_1을 이용하여 valid_set transform해주어 stacked_valid_1을 만듦 => stacked_valid_1 저장 + Stacker_1을 저장 후 호출하는 함수 만들기\n\n6. Stacker_2를 이용하여 valid_set transform해주어 stacked_valid_2를 만듦 => stacked_valid_2 저장 + Stacker_2를 저장 후 호출하는 함수 만들기\n\n7. stacked_train_1을 이용하여 ml_1모델(1차적으로 LGBM Tuner로, 최종적으로 앙상블한 최종 모델 사용할 예정)을 훈련시킨 후 \n   stacked_valid_1을 이용하여 valid_score를 작성=> valid_score 저장\n   \n8. stacked_train_2를 이용하여 ml_2모델(7번과 동일)을 훈련시킨 후\n   stacked_valid_2를 이용하여 valid_score를 작성=> valid_score 저장\n   \n9. Stacker_2를 이용하여 test셋을 transform해준 뒤 ml_2를 이용하여 예측한 값을 KD 전 submission으로 제출함=>submission 저장","metadata":{}},{"cell_type":"markdown","source":"## **1. train null 값 처리와 feature에 대한 scaling**","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/daconio-117/train_df.csv')\nprint(train_df.shape)\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-10T06:34:53.620668Z","iopub.execute_input":"2022-12-10T06:34:53.621158Z","iopub.status.idle":"2022-12-10T06:34:54.101462Z","shell.execute_reply.started":"2022-12-10T06:34:53.621123Z","shell.execute_reply":"2022-12-10T06:34:54.100226Z"},"trusted":true},"outputs":[{"name":"stdout","text":"(14067, 56)\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"   ANONYMOUS_1      YEAR  SAMPLE_TRANSFER_DAY  ANONYMOUS_2       AG        AL  \\\n0    -0.915023 -0.669031             0.320692    -0.623976 -0.15342  0.144691   \n1    -1.079784  1.852946             3.161795     0.405744 -0.15342 -0.117202   \n2    -0.081302  0.339760            -1.168053    -0.623976 -0.15342  3.170023   \n3     1.839183 -0.921229            -1.168053    -0.623976 -0.15342  0.882927   \n4     0.765327  0.339760            -0.392700    -0.623976 -0.15342 -0.486320   \n\n          B        BA        BE        CA        CD        CO        CR  \\\n0  0.963226 -0.494248 -0.054482  1.142569 -0.103225 -0.139071  2.155068   \n1  0.116948 -0.494248 -0.054482  1.087883 -0.103225 -0.139071 -0.894855   \n2 -1.142211  0.748363 -0.054482 -0.911187 -0.103225 -0.139071 -0.093794   \n3 -0.763166 -0.494248 -0.054482  0.400597 -0.103225 -0.139071 -0.894855   \n4  1.247203 -0.494248 -0.054482 -0.874730 -0.103225 -0.139071 -0.894855   \n\n         CU      FH2O      FNOX  FOPTIMETHGLY     FOXID      FSO4     FTBN  \\\n0  1.482724  0.142333  0.015145     -0.393591 -0.019127 -0.010975 -0.12846   \n1  0.913611  0.142333  0.015145     -0.393591 -0.019127 -0.010975 -0.12846   \n2 -0.577088  0.142333  0.015145     -0.393591 -0.019127 -0.010975 -0.12846   \n3 -0.832430  0.142333  0.015145     -0.393591 -0.019127 -0.010975 -0.12846   \n4 -1.268940  0.142333  0.015145     -0.393591 -0.019127 -0.010975 -0.12846   \n\n         FE      FUEL       H2O         K        LI        MG        MN  \\\n0  1.856659 -0.132286 -0.097601  2.878300  1.858988  0.524420  2.377223   \n1 -1.762930 -0.132286 -0.097601 -0.931546 -0.260909 -0.838554 -0.788343   \n2 -1.438062 -0.132286 -0.097601 -0.931546 -0.260909 -1.307430 -0.788343   \n3 -0.148229 -0.132286 -0.097601 -0.931546 -0.260909 -0.218732 -0.013886   \n4  0.258203 -0.132286 -0.097601 -0.931546 -0.260909 -1.307430 -0.788343   \n\n         MO        NA        NI         P        PB   PQINDEX         S  \\\n0 -0.372594  1.982981  3.081712  1.417535 -0.695598  2.764586  1.001988   \n1 -0.776308 -0.434150 -0.445175 -0.378768  0.815365 -0.523272 -1.169611   \n2 -0.776308  0.023809 -0.445175 -1.191871 -0.695598 -0.580505 -1.146343   \n3 -0.776308 -1.217035 -0.445175  0.294274  0.257713 -0.082769  1.045306   \n4 -0.776308  0.023809 -0.445175 -1.279047 -0.695598  0.774321  0.691040   \n\n         SB        SI        SN  SOOTPERCENTAGE        TI      U100       U75  \\\n0 -0.457625  3.175785  1.236100       -0.463603  3.267063 -0.109683 -0.153248   \n1 -0.457625 -1.689316 -0.550961       -0.463603 -0.264231 -0.109683 -0.153248   \n2 -0.457625 -1.689316  0.576549       -0.463603 -0.264231 -0.109683 -0.153248   \n3  1.984632 -1.132762  0.576549       -0.463603 -0.264231 -0.109683 -0.153248   \n4 -0.457625 -0.807199 -0.550961       -0.463603 -0.264231 -0.109683 -0.153248   \n\n        U50       U25       U20       U14        U6        U4         V  \\\n0 -0.258734 -0.377342 -0.391875 -0.393032 -0.411170 -0.416807 -0.154662   \n1  1.051451  0.721457  0.970117  0.818675  1.550216  1.762718 -0.154662   \n2 -0.258734 -0.377342  0.781281  1.904013  2.990037  2.762455 -0.154662   \n3 -0.258734 -0.377342 -0.391875 -0.393032 -0.411170 -0.416807 -0.154662   \n4 -0.258734 -0.377342 -0.391875 -0.393032 -0.411170 -0.416807 -0.154662   \n\n       V100       V40        ZN  COMPONENT_ARBITRARY_COMPONENT1  \\\n0 -0.057476  1.012713 -0.966497                               0   \n1 -0.057476 -1.943832  0.118657                               0   \n2 -0.057476 -0.767722 -0.332706                               0   \n3 -0.057476  0.670030 -1.094383                               0   \n4 -0.057476  0.666467 -0.866820                               0   \n\n   COMPONENT_ARBITRARY_COMPONENT2  COMPONENT_ARBITRARY_COMPONENT3  \\\n0                               0                               1   \n1                               1                               0   \n2                               1                               0   \n3                               0                               1   \n4                               0                               1   \n\n   COMPONENT_ARBITRARY_COMPONENT4  Y_LABEL  \n0                               0        0  \n1                               0        0  \n2                               0        1  \n3                               0        0  \n4                               0        0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ANONYMOUS_1</th>\n      <th>YEAR</th>\n      <th>SAMPLE_TRANSFER_DAY</th>\n      <th>ANONYMOUS_2</th>\n      <th>AG</th>\n      <th>AL</th>\n      <th>B</th>\n      <th>BA</th>\n      <th>BE</th>\n      <th>CA</th>\n      <th>CD</th>\n      <th>CO</th>\n      <th>CR</th>\n      <th>CU</th>\n      <th>FH2O</th>\n      <th>FNOX</th>\n      <th>FOPTIMETHGLY</th>\n      <th>FOXID</th>\n      <th>FSO4</th>\n      <th>FTBN</th>\n      <th>FE</th>\n      <th>FUEL</th>\n      <th>H2O</th>\n      <th>K</th>\n      <th>LI</th>\n      <th>MG</th>\n      <th>MN</th>\n      <th>MO</th>\n      <th>NA</th>\n      <th>NI</th>\n      <th>P</th>\n      <th>PB</th>\n      <th>PQINDEX</th>\n      <th>S</th>\n      <th>SB</th>\n      <th>SI</th>\n      <th>SN</th>\n      <th>SOOTPERCENTAGE</th>\n      <th>TI</th>\n      <th>U100</th>\n      <th>U75</th>\n      <th>U50</th>\n      <th>U25</th>\n      <th>U20</th>\n      <th>U14</th>\n      <th>U6</th>\n      <th>U4</th>\n      <th>V</th>\n      <th>V100</th>\n      <th>V40</th>\n      <th>ZN</th>\n      <th>COMPONENT_ARBITRARY_COMPONENT1</th>\n      <th>COMPONENT_ARBITRARY_COMPONENT2</th>\n      <th>COMPONENT_ARBITRARY_COMPONENT3</th>\n      <th>COMPONENT_ARBITRARY_COMPONENT4</th>\n      <th>Y_LABEL</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.915023</td>\n      <td>-0.669031</td>\n      <td>0.320692</td>\n      <td>-0.623976</td>\n      <td>-0.15342</td>\n      <td>0.144691</td>\n      <td>0.963226</td>\n      <td>-0.494248</td>\n      <td>-0.054482</td>\n      <td>1.142569</td>\n      <td>-0.103225</td>\n      <td>-0.139071</td>\n      <td>2.155068</td>\n      <td>1.482724</td>\n      <td>0.142333</td>\n      <td>0.015145</td>\n      <td>-0.393591</td>\n      <td>-0.019127</td>\n      <td>-0.010975</td>\n      <td>-0.12846</td>\n      <td>1.856659</td>\n      <td>-0.132286</td>\n      <td>-0.097601</td>\n      <td>2.878300</td>\n      <td>1.858988</td>\n      <td>0.524420</td>\n      <td>2.377223</td>\n      <td>-0.372594</td>\n      <td>1.982981</td>\n      <td>3.081712</td>\n      <td>1.417535</td>\n      <td>-0.695598</td>\n      <td>2.764586</td>\n      <td>1.001988</td>\n      <td>-0.457625</td>\n      <td>3.175785</td>\n      <td>1.236100</td>\n      <td>-0.463603</td>\n      <td>3.267063</td>\n      <td>-0.109683</td>\n      <td>-0.153248</td>\n      <td>-0.258734</td>\n      <td>-0.377342</td>\n      <td>-0.391875</td>\n      <td>-0.393032</td>\n      <td>-0.411170</td>\n      <td>-0.416807</td>\n      <td>-0.154662</td>\n      <td>-0.057476</td>\n      <td>1.012713</td>\n      <td>-0.966497</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-1.079784</td>\n      <td>1.852946</td>\n      <td>3.161795</td>\n      <td>0.405744</td>\n      <td>-0.15342</td>\n      <td>-0.117202</td>\n      <td>0.116948</td>\n      <td>-0.494248</td>\n      <td>-0.054482</td>\n      <td>1.087883</td>\n      <td>-0.103225</td>\n      <td>-0.139071</td>\n      <td>-0.894855</td>\n      <td>0.913611</td>\n      <td>0.142333</td>\n      <td>0.015145</td>\n      <td>-0.393591</td>\n      <td>-0.019127</td>\n      <td>-0.010975</td>\n      <td>-0.12846</td>\n      <td>-1.762930</td>\n      <td>-0.132286</td>\n      <td>-0.097601</td>\n      <td>-0.931546</td>\n      <td>-0.260909</td>\n      <td>-0.838554</td>\n      <td>-0.788343</td>\n      <td>-0.776308</td>\n      <td>-0.434150</td>\n      <td>-0.445175</td>\n      <td>-0.378768</td>\n      <td>0.815365</td>\n      <td>-0.523272</td>\n      <td>-1.169611</td>\n      <td>-0.457625</td>\n      <td>-1.689316</td>\n      <td>-0.550961</td>\n      <td>-0.463603</td>\n      <td>-0.264231</td>\n      <td>-0.109683</td>\n      <td>-0.153248</td>\n      <td>1.051451</td>\n      <td>0.721457</td>\n      <td>0.970117</td>\n      <td>0.818675</td>\n      <td>1.550216</td>\n      <td>1.762718</td>\n      <td>-0.154662</td>\n      <td>-0.057476</td>\n      <td>-1.943832</td>\n      <td>0.118657</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.081302</td>\n      <td>0.339760</td>\n      <td>-1.168053</td>\n      <td>-0.623976</td>\n      <td>-0.15342</td>\n      <td>3.170023</td>\n      <td>-1.142211</td>\n      <td>0.748363</td>\n      <td>-0.054482</td>\n      <td>-0.911187</td>\n      <td>-0.103225</td>\n      <td>-0.139071</td>\n      <td>-0.093794</td>\n      <td>-0.577088</td>\n      <td>0.142333</td>\n      <td>0.015145</td>\n      <td>-0.393591</td>\n      <td>-0.019127</td>\n      <td>-0.010975</td>\n      <td>-0.12846</td>\n      <td>-1.438062</td>\n      <td>-0.132286</td>\n      <td>-0.097601</td>\n      <td>-0.931546</td>\n      <td>-0.260909</td>\n      <td>-1.307430</td>\n      <td>-0.788343</td>\n      <td>-0.776308</td>\n      <td>0.023809</td>\n      <td>-0.445175</td>\n      <td>-1.191871</td>\n      <td>-0.695598</td>\n      <td>-0.580505</td>\n      <td>-1.146343</td>\n      <td>-0.457625</td>\n      <td>-1.689316</td>\n      <td>0.576549</td>\n      <td>-0.463603</td>\n      <td>-0.264231</td>\n      <td>-0.109683</td>\n      <td>-0.153248</td>\n      <td>-0.258734</td>\n      <td>-0.377342</td>\n      <td>0.781281</td>\n      <td>1.904013</td>\n      <td>2.990037</td>\n      <td>2.762455</td>\n      <td>-0.154662</td>\n      <td>-0.057476</td>\n      <td>-0.767722</td>\n      <td>-0.332706</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.839183</td>\n      <td>-0.921229</td>\n      <td>-1.168053</td>\n      <td>-0.623976</td>\n      <td>-0.15342</td>\n      <td>0.882927</td>\n      <td>-0.763166</td>\n      <td>-0.494248</td>\n      <td>-0.054482</td>\n      <td>0.400597</td>\n      <td>-0.103225</td>\n      <td>-0.139071</td>\n      <td>-0.894855</td>\n      <td>-0.832430</td>\n      <td>0.142333</td>\n      <td>0.015145</td>\n      <td>-0.393591</td>\n      <td>-0.019127</td>\n      <td>-0.010975</td>\n      <td>-0.12846</td>\n      <td>-0.148229</td>\n      <td>-0.132286</td>\n      <td>-0.097601</td>\n      <td>-0.931546</td>\n      <td>-0.260909</td>\n      <td>-0.218732</td>\n      <td>-0.013886</td>\n      <td>-0.776308</td>\n      <td>-1.217035</td>\n      <td>-0.445175</td>\n      <td>0.294274</td>\n      <td>0.257713</td>\n      <td>-0.082769</td>\n      <td>1.045306</td>\n      <td>1.984632</td>\n      <td>-1.132762</td>\n      <td>0.576549</td>\n      <td>-0.463603</td>\n      <td>-0.264231</td>\n      <td>-0.109683</td>\n      <td>-0.153248</td>\n      <td>-0.258734</td>\n      <td>-0.377342</td>\n      <td>-0.391875</td>\n      <td>-0.393032</td>\n      <td>-0.411170</td>\n      <td>-0.416807</td>\n      <td>-0.154662</td>\n      <td>-0.057476</td>\n      <td>0.670030</td>\n      <td>-1.094383</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.765327</td>\n      <td>0.339760</td>\n      <td>-0.392700</td>\n      <td>-0.623976</td>\n      <td>-0.15342</td>\n      <td>-0.486320</td>\n      <td>1.247203</td>\n      <td>-0.494248</td>\n      <td>-0.054482</td>\n      <td>-0.874730</td>\n      <td>-0.103225</td>\n      <td>-0.139071</td>\n      <td>-0.894855</td>\n      <td>-1.268940</td>\n      <td>0.142333</td>\n      <td>0.015145</td>\n      <td>-0.393591</td>\n      <td>-0.019127</td>\n      <td>-0.010975</td>\n      <td>-0.12846</td>\n      <td>0.258203</td>\n      <td>-0.132286</td>\n      <td>-0.097601</td>\n      <td>-0.931546</td>\n      <td>-0.260909</td>\n      <td>-1.307430</td>\n      <td>-0.788343</td>\n      <td>-0.776308</td>\n      <td>0.023809</td>\n      <td>-0.445175</td>\n      <td>-1.279047</td>\n      <td>-0.695598</td>\n      <td>0.774321</td>\n      <td>0.691040</td>\n      <td>-0.457625</td>\n      <td>-0.807199</td>\n      <td>-0.550961</td>\n      <td>-0.463603</td>\n      <td>-0.264231</td>\n      <td>-0.109683</td>\n      <td>-0.153248</td>\n      <td>-0.258734</td>\n      <td>-0.377342</td>\n      <td>-0.391875</td>\n      <td>-0.393032</td>\n      <td>-0.411170</td>\n      <td>-0.416807</td>\n      <td>-0.154662</td>\n      <td>-0.057476</td>\n      <td>0.666467</td>\n      <td>-0.866820</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"**null값 drop 및 scaling 모두 마친 train set**","metadata":{}},{"cell_type":"markdown","source":"## **2. train_valid 나누기: train_set과 valid set나눈 후 저장 하던지 random_state로 통일**","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nfeature = train_df.drop(['Y_LABEL'], axis=1)\ntarget = train_df['Y_LABEL']\n\nX_train, X_valid, y_train, y_valid = train_test_split(feature, target, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2022-12-10T06:34:54.104453Z","iopub.execute_input":"2022-12-10T06:34:54.105489Z","iopub.status.idle":"2022-12-10T06:34:54.691867Z","shell.execute_reply.started":"2022-12-10T06:34:54.105438Z","shell.execute_reply":"2022-12-10T06:34:54.690655Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"**random_state를 비롯한 seed들은 앞으로 42로 고정 이러면 굳이 valid set 저장할 필요 없을 듯**","metadata":{}},{"cell_type":"markdown","source":"## **3. 전체 특성을 이용하여 train_set을 Stacker로 fit_transform해 줌 => Stacker_1, stacked_train_1라 명명 후 둘 다 저장**","metadata":{}},{"cell_type":"code","source":"#index reset\nX_train.reset_index(drop=True, inplace=True)\ny_train.reset_index(drop=True, inplace=True)\nX_train.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-12-10T06:34:54.693209Z","iopub.execute_input":"2022-12-10T06:34:54.693586Z","iopub.status.idle":"2022-12-10T06:34:54.740565Z","shell.execute_reply.started":"2022-12-10T06:34:54.693552Z","shell.execute_reply":"2022-12-10T06:34:54.739758Z"},"trusted":true},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"   ANONYMOUS_1      YEAR  SAMPLE_TRANSFER_DAY  ANONYMOUS_2       AG        AL  \\\n0    -0.023281 -1.173427             1.711478     1.034058 -0.15342 -0.117202   \n1    -1.105403 -0.416834            -0.731397    -0.623976 -0.15342  0.347832   \n2     0.153575 -0.164636             0.659389    -0.623976 -0.15342  3.425744   \n\n          B        BA        BE        CA        CD        CO        CR  \\\n0 -0.641141 -0.494248 -0.054482  0.592335 -0.103225 -0.139071 -0.894855   \n1  1.110267  0.748363 -0.054482  1.230336 -0.103225 -0.139071  0.707267   \n2  1.049182 -0.494248 -0.054482 -0.882156 -0.103225 -0.139071 -0.093794   \n\n         CU      FH2O      FNOX  FOPTIMETHGLY     FOXID      FSO4     FTBN  \\\n0  1.969028  0.142333  0.015145     -0.393591 -0.019127 -0.010975 -0.12846   \n1  1.592205  0.142333  0.015145     -0.393591 -0.019127 -0.010975 -0.12846   \n2 -0.255395  0.142333  0.015145     -0.393591 -0.019127 -0.010975 -0.12846   \n\n         FE      FUEL       H2O         K        LI        MG        MN  \\\n0 -0.556427 -0.132286 -0.097601 -0.931546 -0.260909 -1.307430 -0.788343   \n1  1.981607 -0.132286 -0.097601  0.324543 -0.260909  0.869967  3.184068   \n2  0.749926 -0.132286 -0.097601  0.653461 -0.260909 -0.369677  0.439143   \n\n         MO        NA        NI         P        PB   PQINDEX         S  \\\n0 -0.372594 -1.217035 -0.445175 -0.759178  0.257713 -0.800756 -1.148595   \n1  0.800960  0.023809  1.546017  0.590266  0.257713  2.409786 -0.435880   \n2 -0.776308  0.600769 -0.445175  1.728853 -0.695598  0.886489  2.487035   \n\n         SB        SI        SN  SOOTPERCENTAGE        TI      U100       U75  \\\n0 -0.457625 -1.689316 -0.550961       -0.463603 -0.264231 -0.109683 -0.153248   \n1  1.083268  0.631473  2.363609       -0.463603 -0.264231 -0.109683 -0.153248   \n2 -0.457625  0.631473 -0.550961       -0.463603 -0.264231 -0.109683 -0.153248   \n\n        U50       U25       U20       U14        U6        U4         V  \\\n0 -0.258734  1.702452  1.467534  2.480778  2.284467  2.529955 -0.154662   \n1 -0.258734 -0.377342 -0.391875 -0.393032 -0.411170 -0.416807 -0.154662   \n2 -0.258734 -0.377342 -0.391875 -0.393032 -0.411170 -0.416807 -0.154662   \n\n       V100       V40        ZN  COMPONENT_ARBITRARY_COMPONENT1  \\\n0 -0.057476 -1.927948  0.118657                               0   \n1 -0.057476 -1.570778  1.369311                               0   \n2 -0.057476  0.724583 -1.054889                               0   \n\n   COMPONENT_ARBITRARY_COMPONENT2  COMPONENT_ARBITRARY_COMPONENT3  \\\n0                               1                               0   \n1                               0                               1   \n2                               0                               1   \n\n   COMPONENT_ARBITRARY_COMPONENT4  \n0                               0  \n1                               0  \n2                               0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ANONYMOUS_1</th>\n      <th>YEAR</th>\n      <th>SAMPLE_TRANSFER_DAY</th>\n      <th>ANONYMOUS_2</th>\n      <th>AG</th>\n      <th>AL</th>\n      <th>B</th>\n      <th>BA</th>\n      <th>BE</th>\n      <th>CA</th>\n      <th>CD</th>\n      <th>CO</th>\n      <th>CR</th>\n      <th>CU</th>\n      <th>FH2O</th>\n      <th>FNOX</th>\n      <th>FOPTIMETHGLY</th>\n      <th>FOXID</th>\n      <th>FSO4</th>\n      <th>FTBN</th>\n      <th>FE</th>\n      <th>FUEL</th>\n      <th>H2O</th>\n      <th>K</th>\n      <th>LI</th>\n      <th>MG</th>\n      <th>MN</th>\n      <th>MO</th>\n      <th>NA</th>\n      <th>NI</th>\n      <th>P</th>\n      <th>PB</th>\n      <th>PQINDEX</th>\n      <th>S</th>\n      <th>SB</th>\n      <th>SI</th>\n      <th>SN</th>\n      <th>SOOTPERCENTAGE</th>\n      <th>TI</th>\n      <th>U100</th>\n      <th>U75</th>\n      <th>U50</th>\n      <th>U25</th>\n      <th>U20</th>\n      <th>U14</th>\n      <th>U6</th>\n      <th>U4</th>\n      <th>V</th>\n      <th>V100</th>\n      <th>V40</th>\n      <th>ZN</th>\n      <th>COMPONENT_ARBITRARY_COMPONENT1</th>\n      <th>COMPONENT_ARBITRARY_COMPONENT2</th>\n      <th>COMPONENT_ARBITRARY_COMPONENT3</th>\n      <th>COMPONENT_ARBITRARY_COMPONENT4</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.023281</td>\n      <td>-1.173427</td>\n      <td>1.711478</td>\n      <td>1.034058</td>\n      <td>-0.15342</td>\n      <td>-0.117202</td>\n      <td>-0.641141</td>\n      <td>-0.494248</td>\n      <td>-0.054482</td>\n      <td>0.592335</td>\n      <td>-0.103225</td>\n      <td>-0.139071</td>\n      <td>-0.894855</td>\n      <td>1.969028</td>\n      <td>0.142333</td>\n      <td>0.015145</td>\n      <td>-0.393591</td>\n      <td>-0.019127</td>\n      <td>-0.010975</td>\n      <td>-0.12846</td>\n      <td>-0.556427</td>\n      <td>-0.132286</td>\n      <td>-0.097601</td>\n      <td>-0.931546</td>\n      <td>-0.260909</td>\n      <td>-1.307430</td>\n      <td>-0.788343</td>\n      <td>-0.372594</td>\n      <td>-1.217035</td>\n      <td>-0.445175</td>\n      <td>-0.759178</td>\n      <td>0.257713</td>\n      <td>-0.800756</td>\n      <td>-1.148595</td>\n      <td>-0.457625</td>\n      <td>-1.689316</td>\n      <td>-0.550961</td>\n      <td>-0.463603</td>\n      <td>-0.264231</td>\n      <td>-0.109683</td>\n      <td>-0.153248</td>\n      <td>-0.258734</td>\n      <td>1.702452</td>\n      <td>1.467534</td>\n      <td>2.480778</td>\n      <td>2.284467</td>\n      <td>2.529955</td>\n      <td>-0.154662</td>\n      <td>-0.057476</td>\n      <td>-1.927948</td>\n      <td>0.118657</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-1.105403</td>\n      <td>-0.416834</td>\n      <td>-0.731397</td>\n      <td>-0.623976</td>\n      <td>-0.15342</td>\n      <td>0.347832</td>\n      <td>1.110267</td>\n      <td>0.748363</td>\n      <td>-0.054482</td>\n      <td>1.230336</td>\n      <td>-0.103225</td>\n      <td>-0.139071</td>\n      <td>0.707267</td>\n      <td>1.592205</td>\n      <td>0.142333</td>\n      <td>0.015145</td>\n      <td>-0.393591</td>\n      <td>-0.019127</td>\n      <td>-0.010975</td>\n      <td>-0.12846</td>\n      <td>1.981607</td>\n      <td>-0.132286</td>\n      <td>-0.097601</td>\n      <td>0.324543</td>\n      <td>-0.260909</td>\n      <td>0.869967</td>\n      <td>3.184068</td>\n      <td>0.800960</td>\n      <td>0.023809</td>\n      <td>1.546017</td>\n      <td>0.590266</td>\n      <td>0.257713</td>\n      <td>2.409786</td>\n      <td>-0.435880</td>\n      <td>1.083268</td>\n      <td>0.631473</td>\n      <td>2.363609</td>\n      <td>-0.463603</td>\n      <td>-0.264231</td>\n      <td>-0.109683</td>\n      <td>-0.153248</td>\n      <td>-0.258734</td>\n      <td>-0.377342</td>\n      <td>-0.391875</td>\n      <td>-0.393032</td>\n      <td>-0.411170</td>\n      <td>-0.416807</td>\n      <td>-0.154662</td>\n      <td>-0.057476</td>\n      <td>-1.570778</td>\n      <td>1.369311</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.153575</td>\n      <td>-0.164636</td>\n      <td>0.659389</td>\n      <td>-0.623976</td>\n      <td>-0.15342</td>\n      <td>3.425744</td>\n      <td>1.049182</td>\n      <td>-0.494248</td>\n      <td>-0.054482</td>\n      <td>-0.882156</td>\n      <td>-0.103225</td>\n      <td>-0.139071</td>\n      <td>-0.093794</td>\n      <td>-0.255395</td>\n      <td>0.142333</td>\n      <td>0.015145</td>\n      <td>-0.393591</td>\n      <td>-0.019127</td>\n      <td>-0.010975</td>\n      <td>-0.12846</td>\n      <td>0.749926</td>\n      <td>-0.132286</td>\n      <td>-0.097601</td>\n      <td>0.653461</td>\n      <td>-0.260909</td>\n      <td>-0.369677</td>\n      <td>0.439143</td>\n      <td>-0.776308</td>\n      <td>0.600769</td>\n      <td>-0.445175</td>\n      <td>1.728853</td>\n      <td>-0.695598</td>\n      <td>0.886489</td>\n      <td>2.487035</td>\n      <td>-0.457625</td>\n      <td>0.631473</td>\n      <td>-0.550961</td>\n      <td>-0.463603</td>\n      <td>-0.264231</td>\n      <td>-0.109683</td>\n      <td>-0.153248</td>\n      <td>-0.258734</td>\n      <td>-0.377342</td>\n      <td>-0.391875</td>\n      <td>-0.393032</td>\n      <td>-0.411170</td>\n      <td>-0.416807</td>\n      <td>-0.154662</td>\n      <td>-0.057476</td>\n      <td>0.724583</td>\n      <td>-1.054889</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"from verstack import Stacker\n\nStacker_1 = Stacker(objective = 'binary', auto = True)\nstacked_train_1 = Stacker_1.fit_transform(X_train, y_train)","metadata":{"_kg_hide-output":true,"scrolled":true,"execution":{"iopub.status.busy":"2022-12-10T06:34:54.741722Z","iopub.execute_input":"2022-12-10T06:34:54.742449Z","iopub.status.idle":"2022-12-10T07:03:08.101190Z","shell.execute_reply.started":"2022-12-10T06:34:54.742415Z","shell.execute_reply":"2022-12-10T07:03:08.099114Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style type='text/css'>\n.datatable table.frame { margin-bottom: 0; }\n.datatable table.frame thead { border-bottom: none; }\n.datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n.datatable .bool    { background: #DDDD99; }\n.datatable .object  { background: #565656; }\n.datatable .int     { background: #5D9E5D; }\n.datatable .float   { background: #4040CC; }\n.datatable .str     { background: #CC4040; }\n.datatable .time    { background: #40CC40; }\n.datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n.datatable .frame tbody td { text-align: left; }\n.datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n.datatable th:nth-child(2) { padding-left: 12px; }\n.datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n.datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n.datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n.datatable .sp {  opacity: 0.25;}\n.datatable .footer { font-size: 9px; }\n.datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n.datatable .frame thead tr.colnames {  background-image: url('data:image/png;base64,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');  background-repeat: repeat-x;  background-size: 14px;  height: 28px;}\n</style>\n"},"metadata":{}},{"name":"stderr","text":"2022-12-10 06:34:56.587433: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-12-10 06:34:56.760437: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:\n2022-12-10 06:34:56.760491: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n2022-12-10 06:34:58.000713: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:\n2022-12-10 06:34:58.000910: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:\n2022-12-10 06:34:58.000926: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n","output_type":"stream"},{"name":"stdout","text":"\n * Initiating Stacker.fit_transform\n\n   - Training/predicting with layer_1 models\n     . Optimising model hyperparameters\n     .. fold 2 trained/predicted\n     .. fold 4 trained/predicted\n     . Optimising model hyperparameters\n     .. fold 2 trained/predicted\n     .. fold 4 trained/predicted\n     . Optimising model hyperparameters\n     .. fold 2 trained/predicted\n     .. fold 4 trained/predicted\n     . Optimising model hyperparameters\n     .. Model not in optimisation list <verstack.stacking.kerasModel.kerasModel object at 0x7f972ca39f10>\n","output_type":"stream"},{"name":"stderr","text":"2022-12-10 06:39:34.725891: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:\n2022-12-10 06:39:34.725995: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n2022-12-10 06:39:34.726036: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (c806563d0f82): /proc/driver/nvidia/version does not exist\n2022-12-10 06:39:34.726574: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/200\n211/211 [==============================] - 2s 3ms/step - loss: 0.2926 - val_loss: 0.1651\nEpoch 2/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1765 - val_loss: 0.1590\nEpoch 3/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1691 - val_loss: 0.1572\nEpoch 4/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.1625 - val_loss: 0.1576\nEpoch 5/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.1575 - val_loss: 0.1570\nEpoch 6/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.1533 - val_loss: 0.1586\nEpoch 7/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.1485 - val_loss: 0.1595\nEpoch 8/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.1424 - val_loss: 0.1608\nEpoch 9/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.1390 - val_loss: 0.1694\nEpoch 10/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.1345 - val_loss: 0.1633\nEpoch 11/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.1296 - val_loss: 0.1633\nEpoch 12/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.1261 - val_loss: 0.1692\nEpoch 13/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.1218 - val_loss: 0.1705\nEpoch 14/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.1184 - val_loss: 0.1700\nEpoch 15/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.1124 - val_loss: 0.1801\nEpoch 16/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.1093 - val_loss: 0.1813\nEpoch 17/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.1044 - val_loss: 0.1814\nEpoch 18/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0987 - val_loss: 0.2027\nEpoch 19/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0946 - val_loss: 0.1929\nEpoch 20/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0868 - val_loss: 0.2034\nEpoch 21/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0852 - val_loss: 0.2124\nEpoch 22/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0828 - val_loss: 0.2169\nEpoch 23/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0763 - val_loss: 0.2256\nEpoch 24/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0718 - val_loss: 0.2316\nEpoch 25/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0667 - val_loss: 0.2300\nEpoch 26/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0650 - val_loss: 0.2654\nEpoch 27/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0612 - val_loss: 0.2858\nEpoch 28/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0613 - val_loss: 0.2628\nEpoch 29/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0548 - val_loss: 0.2702\nEpoch 30/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0519 - val_loss: 0.2927\nEpoch 31/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0508 - val_loss: 0.2986\nEpoch 32/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0431 - val_loss: 0.3166\nEpoch 33/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0471 - val_loss: 0.3327\nEpoch 34/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0415 - val_loss: 0.3362\nEpoch 35/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0386 - val_loss: 0.3509\nEpoch 36/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0359 - val_loss: 0.3469\nEpoch 37/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0398 - val_loss: 0.3545\nEpoch 38/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0382 - val_loss: 0.3828\nEpoch 39/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0320 - val_loss: 0.3717\nEpoch 40/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0280 - val_loss: 0.4029\nEpoch 41/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0263 - val_loss: 0.4115\nEpoch 42/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0256 - val_loss: 0.4194\nEpoch 43/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0215 - val_loss: 0.4396\nEpoch 44/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0217 - val_loss: 0.4612\nEpoch 45/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0243 - val_loss: 0.4659\nEpoch 46/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0340 - val_loss: 0.4873\nEpoch 47/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0212 - val_loss: 0.4744\nEpoch 48/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0158 - val_loss: 0.4902\nEpoch 49/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0162 - val_loss: 0.4961\nEpoch 50/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0142 - val_loss: 0.5354\nEpoch 51/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0174 - val_loss: 0.5267\nEpoch 52/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0175 - val_loss: 0.5673\nEpoch 53/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0225 - val_loss: 0.5475\nEpoch 54/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0248 - val_loss: 0.5446\nEpoch 55/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0129 - val_loss: 0.5714\nEpoch 56/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0101 - val_loss: 0.5505\nEpoch 57/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0087 - val_loss: 0.5992\nEpoch 58/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0189 - val_loss: 0.6071\nEpoch 59/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0108 - val_loss: 0.5891\nEpoch 60/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0137 - val_loss: 0.6018\nEpoch 61/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0182 - val_loss: 0.5878\nEpoch 62/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0106 - val_loss: 0.5932\nEpoch 63/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0164 - val_loss: 0.6384\nEpoch 64/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0103 - val_loss: 0.6266\nEpoch 65/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0075 - val_loss: 0.6351\nEpoch 66/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0071 - val_loss: 0.6476\nEpoch 67/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0066 - val_loss: 0.6652\nEpoch 68/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0038 - val_loss: 0.6710\nEpoch 69/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0045 - val_loss: 0.6762\nEpoch 70/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0029 - val_loss: 0.6966\nEpoch 71/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0050 - val_loss: 0.7197\nEpoch 72/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0054 - val_loss: 0.7432\nEpoch 73/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0360 - val_loss: 0.6817\nEpoch 74/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0232 - val_loss: 0.6292\nEpoch 75/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0075 - val_loss: 0.6355\nEpoch 76/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0039 - val_loss: 0.6585\nEpoch 77/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0024 - val_loss: 0.6827\nEpoch 78/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0021 - val_loss: 0.6787\nEpoch 79/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0018 - val_loss: 0.6969\nEpoch 80/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0019 - val_loss: 0.6967\nEpoch 81/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0015 - val_loss: 0.7192\nEpoch 82/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0014 - val_loss: 0.7335\nEpoch 83/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0016 - val_loss: 0.7502\nEpoch 84/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0287 - val_loss: 0.7751\nEpoch 85/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0326 - val_loss: 0.6329\nEpoch 86/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0094 - val_loss: 0.6541\nEpoch 87/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0037 - val_loss: 0.6653\nEpoch 88/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0028 - val_loss: 0.6663\nEpoch 89/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0012 - val_loss: 0.6938\nEpoch 90/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0010 - val_loss: 0.7014\nEpoch 91/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0010 - val_loss: 0.7186\nEpoch 92/200\n211/211 [==============================] - 1s 2ms/step - loss: 8.5246e-04 - val_loss: 0.7395\nEpoch 93/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0014 - val_loss: 0.7512\nEpoch 94/200\n211/211 [==============================] - 1s 2ms/step - loss: 5.8318e-04 - val_loss: 0.7619\nEpoch 95/200\n211/211 [==============================] - 1s 3ms/step - loss: 4.7702e-04 - val_loss: 0.7774\nEpoch 96/200\n211/211 [==============================] - 1s 2ms/step - loss: 3.7717e-04 - val_loss: 0.7881\nEpoch 97/200\n211/211 [==============================] - 1s 2ms/step - loss: 3.3391e-04 - val_loss: 0.8019\nEpoch 98/200\n211/211 [==============================] - 1s 3ms/step - loss: 2.9907e-04 - val_loss: 0.8144\nEpoch 99/200\n211/211 [==============================] - 1s 3ms/step - loss: 3.1724e-04 - val_loss: 0.8327\nEpoch 100/200\n211/211 [==============================] - 1s 2ms/step - loss: 2.4382e-04 - val_loss: 0.8454\nEpoch 101/200\n211/211 [==============================] - 1s 3ms/step - loss: 1.9236e-04 - val_loss: 0.8585\nEpoch 102/200\n211/211 [==============================] - 1s 2ms/step - loss: 1.7206e-04 - val_loss: 0.8690\nEpoch 103/200\n211/211 [==============================] - 1s 2ms/step - loss: 1.5295e-04 - val_loss: 0.8831\nEpoch 104/200\n211/211 [==============================] - 1s 2ms/step - loss: 1.3313e-04 - val_loss: 0.8946\nEpoch 105/200\n211/211 [==============================] - 1s 3ms/step - loss: 1.1296e-04 - val_loss: 0.9071\nEpoch 106/200\n211/211 [==============================] - 1s 3ms/step - loss: 1.1958e-04 - val_loss: 0.9226\nEpoch 107/200\n211/211 [==============================] - 1s 2ms/step - loss: 1.0374e-04 - val_loss: 0.9393\nEpoch 108/200\n211/211 [==============================] - 1s 3ms/step - loss: 8.6120e-05 - val_loss: 0.9532\nEpoch 109/200\n211/211 [==============================] - 1s 2ms/step - loss: 7.6061e-05 - val_loss: 0.9620\nEpoch 110/200\n211/211 [==============================] - 1s 2ms/step - loss: 6.5840e-05 - val_loss: 0.9738\nEpoch 111/200\n211/211 [==============================] - 1s 2ms/step - loss: 6.1650e-05 - val_loss: 0.9886\nEpoch 112/200\n211/211 [==============================] - 1s 2ms/step - loss: 5.1743e-05 - val_loss: 0.9986\nEpoch 113/200\n211/211 [==============================] - 1s 3ms/step - loss: 4.4392e-05 - val_loss: 1.0146\nEpoch 114/200\n211/211 [==============================] - 1s 2ms/step - loss: 4.2160e-05 - val_loss: 1.0270\nEpoch 115/200\n211/211 [==============================] - 1s 3ms/step - loss: 3.2759e-05 - val_loss: 1.0355\nEpoch 116/200\n211/211 [==============================] - 1s 3ms/step - loss: 3.2354e-05 - val_loss: 1.0472\nEpoch 117/200\n211/211 [==============================] - 1s 2ms/step - loss: 2.9044e-05 - val_loss: 1.0633\nEpoch 118/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.1373 - val_loss: 0.6577\nEpoch 119/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0350 - val_loss: 0.6068\nEpoch 120/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0073 - val_loss: 0.6235\nEpoch 121/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0026 - val_loss: 0.6368\nEpoch 122/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0014 - val_loss: 0.6525\nEpoch 123/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0012 - val_loss: 0.6688\nEpoch 124/200\n211/211 [==============================] - 1s 2ms/step - loss: 9.0994e-04 - val_loss: 0.6832\nEpoch 125/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0011 - val_loss: 0.6966\nEpoch 126/200\n211/211 [==============================] - 1s 2ms/step - loss: 6.6508e-04 - val_loss: 0.7074\nEpoch 127/200\n211/211 [==============================] - 1s 3ms/step - loss: 5.4258e-04 - val_loss: 0.7174\nEpoch 128/200\n211/211 [==============================] - 1s 2ms/step - loss: 4.7311e-04 - val_loss: 0.7288\nEpoch 129/200\n211/211 [==============================] - 1s 2ms/step - loss: 4.1360e-04 - val_loss: 0.7397\nEpoch 130/200\n211/211 [==============================] - 1s 3ms/step - loss: 3.6924e-04 - val_loss: 0.7520\nEpoch 131/200\n211/211 [==============================] - 1s 2ms/step - loss: 3.2689e-04 - val_loss: 0.7623\nEpoch 132/200\n211/211 [==============================] - 1s 2ms/step - loss: 2.8718e-04 - val_loss: 0.7727\nEpoch 133/200\n211/211 [==============================] - 1s 2ms/step - loss: 2.4801e-04 - val_loss: 0.7846\nEpoch 134/200\n211/211 [==============================] - 1s 3ms/step - loss: 2.2042e-04 - val_loss: 0.7936\nEpoch 135/200\n211/211 [==============================] - 1s 3ms/step - loss: 1.9880e-04 - val_loss: 0.8076\nEpoch 136/200\n211/211 [==============================] - 1s 3ms/step - loss: 1.9019e-04 - val_loss: 0.8165\nEpoch 137/200\n211/211 [==============================] - 1s 3ms/step - loss: 1.6690e-04 - val_loss: 0.8303\nEpoch 138/200\n211/211 [==============================] - 1s 3ms/step - loss: 1.4001e-04 - val_loss: 0.8415\nEpoch 139/200\n211/211 [==============================] - 1s 3ms/step - loss: 1.2628e-04 - val_loss: 0.8512\nEpoch 140/200\n211/211 [==============================] - 1s 2ms/step - loss: 1.1484e-04 - val_loss: 0.8635\nEpoch 141/200\n211/211 [==============================] - 1s 3ms/step - loss: 9.9058e-05 - val_loss: 0.8756\nEpoch 142/200\n211/211 [==============================] - 1s 2ms/step - loss: 8.7612e-05 - val_loss: 0.8849\nEpoch 143/200\n211/211 [==============================] - 1s 3ms/step - loss: 7.8681e-05 - val_loss: 0.8946\nEpoch 144/200\n211/211 [==============================] - 1s 2ms/step - loss: 6.9165e-05 - val_loss: 0.9092\nEpoch 145/200\n211/211 [==============================] - 1s 2ms/step - loss: 6.0390e-05 - val_loss: 0.9177\nEpoch 146/200\n211/211 [==============================] - 1s 3ms/step - loss: 5.4539e-05 - val_loss: 0.9291\nEpoch 147/200\n211/211 [==============================] - 1s 2ms/step - loss: 4.9295e-05 - val_loss: 0.9435\nEpoch 148/200\n211/211 [==============================] - 1s 2ms/step - loss: 4.2326e-05 - val_loss: 0.9506\nEpoch 149/200\n211/211 [==============================] - 1s 2ms/step - loss: 3.6759e-05 - val_loss: 0.9638\nEpoch 150/200\n211/211 [==============================] - 1s 3ms/step - loss: 3.3976e-05 - val_loss: 0.9715\nEpoch 151/200\n211/211 [==============================] - 1s 2ms/step - loss: 3.0622e-05 - val_loss: 0.9857\nEpoch 152/200\n211/211 [==============================] - 1s 2ms/step - loss: 2.8318e-05 - val_loss: 0.9979\nEpoch 153/200\n211/211 [==============================] - 1s 3ms/step - loss: 2.4747e-05 - val_loss: 1.0066\nEpoch 154/200\n211/211 [==============================] - 1s 2ms/step - loss: 2.3333e-05 - val_loss: 1.0172\nEpoch 155/200\n211/211 [==============================] - 1s 2ms/step - loss: 1.8322e-05 - val_loss: 1.0289\nEpoch 156/200\n211/211 [==============================] - 1s 3ms/step - loss: 1.6628e-05 - val_loss: 1.0413\nEpoch 157/200\n211/211 [==============================] - 1s 3ms/step - loss: 1.6475e-05 - val_loss: 1.0555\nEpoch 158/200\n211/211 [==============================] - 1s 3ms/step - loss: 1.2354e-05 - val_loss: 1.0624\nEpoch 159/200\n211/211 [==============================] - 1s 3ms/step - loss: 1.1317e-05 - val_loss: 1.0733\nEpoch 160/200\n211/211 [==============================] - 1s 3ms/step - loss: 1.0297e-05 - val_loss: 1.0839\nEpoch 161/200\n211/211 [==============================] - 1s 2ms/step - loss: 8.5607e-06 - val_loss: 1.0941\nEpoch 162/200\n211/211 [==============================] - 1s 3ms/step - loss: 7.6183e-06 - val_loss: 1.1054\nEpoch 163/200\n211/211 [==============================] - 1s 3ms/step - loss: 7.4002e-06 - val_loss: 1.1180\nEpoch 164/200\n211/211 [==============================] - 1s 3ms/step - loss: 6.8333e-06 - val_loss: 1.1292\nEpoch 165/200\n211/211 [==============================] - 1s 3ms/step - loss: 5.3597e-06 - val_loss: 1.1394\nEpoch 166/200\n211/211 [==============================] - 1s 3ms/step - loss: 4.9304e-06 - val_loss: 1.1457\nEpoch 167/200\n211/211 [==============================] - 1s 2ms/step - loss: 4.5342e-06 - val_loss: 1.1585\nEpoch 168/200\n211/211 [==============================] - 1s 2ms/step - loss: 3.7590e-06 - val_loss: 1.1700\nEpoch 169/200\n211/211 [==============================] - 1s 2ms/step - loss: 3.5380e-06 - val_loss: 1.1838\nEpoch 170/200\n211/211 [==============================] - 1s 2ms/step - loss: 2.9880e-06 - val_loss: 1.1924\nEpoch 171/200\n211/211 [==============================] - 1s 2ms/step - loss: 2.6011e-06 - val_loss: 1.2034\nEpoch 172/200\n211/211 [==============================] - 1s 2ms/step - loss: 2.3468e-06 - val_loss: 1.2124\nEpoch 173/200\n211/211 [==============================] - 1s 3ms/step - loss: 2.2327e-06 - val_loss: 1.2192\nEpoch 174/200\n211/211 [==============================] - 1s 2ms/step - loss: 1.8163e-06 - val_loss: 1.2300\nEpoch 175/200\n211/211 [==============================] - 1s 3ms/step - loss: 1.7264e-06 - val_loss: 1.2412\nEpoch 176/200\n211/211 [==============================] - 1s 3ms/step - loss: 1.4975e-06 - val_loss: 1.2547\nEpoch 177/200\n211/211 [==============================] - 1s 2ms/step - loss: 1.3417e-06 - val_loss: 1.2605\nEpoch 178/200\n211/211 [==============================] - 1s 2ms/step - loss: 1.1437e-06 - val_loss: 1.2753\nEpoch 179/200\n211/211 [==============================] - 1s 2ms/step - loss: 9.9397e-07 - val_loss: 1.2833\nEpoch 180/200\n211/211 [==============================] - 1s 3ms/step - loss: 9.6124e-07 - val_loss: 1.2915\nEpoch 181/200\n211/211 [==============================] - 1s 3ms/step - loss: 8.1819e-07 - val_loss: 1.3028\nEpoch 182/200\n211/211 [==============================] - 1s 3ms/step - loss: 7.9456e-07 - val_loss: 1.3141\nEpoch 183/200\n211/211 [==============================] - 1s 3ms/step - loss: 6.9627e-07 - val_loss: 1.3243\nEpoch 184/200\n211/211 [==============================] - 1s 2ms/step - loss: 5.5843e-07 - val_loss: 1.3359\nEpoch 185/200\n211/211 [==============================] - 1s 2ms/step - loss: 5.0811e-07 - val_loss: 1.3453\nEpoch 186/200\n211/211 [==============================] - 1s 2ms/step - loss: 4.3365e-07 - val_loss: 1.3546\nEpoch 187/200\n211/211 [==============================] - 1s 3ms/step - loss: 3.8007e-07 - val_loss: 1.3630\nEpoch 188/200\n211/211 [==============================] - 1s 2ms/step - loss: 3.5541e-07 - val_loss: 1.3725\nEpoch 189/200\n211/211 [==============================] - 1s 2ms/step - loss: 3.1862e-07 - val_loss: 1.3808\nEpoch 190/200\n211/211 [==============================] - 1s 3ms/step - loss: 2.7893e-07 - val_loss: 1.3937\nEpoch 191/200\n211/211 [==============================] - 1s 3ms/step - loss: 2.5974e-07 - val_loss: 1.4022\nEpoch 192/200\n211/211 [==============================] - 1s 2ms/step - loss: 2.1358e-07 - val_loss: 1.4111\nEpoch 193/200\n211/211 [==============================] - 1s 2ms/step - loss: 1.9600e-07 - val_loss: 1.4200\nEpoch 194/200\n211/211 [==============================] - 1s 2ms/step - loss: 1.7966e-07 - val_loss: 1.4301\nEpoch 195/200\n211/211 [==============================] - 1s 3ms/step - loss: 1.7398e-07 - val_loss: 1.4385\nEpoch 196/200\n211/211 [==============================] - 1s 3ms/step - loss: 1.3979e-07 - val_loss: 1.4496\nEpoch 197/200\n211/211 [==============================] - 1s 3ms/step - loss: 1.3124e-07 - val_loss: 1.4588\nEpoch 198/200\n211/211 [==============================] - 1s 3ms/step - loss: 1.0882e-07 - val_loss: 1.4669\nEpoch 199/200\n211/211 [==============================] - 1s 2ms/step - loss: 9.7482e-08 - val_loss: 1.4756\nEpoch 200/200\n211/211 [==============================] - 1s 3ms/step - loss: 9.4950e-08 - val_loss: 1.4837\n88/88 [==============================] - 0s 1ms/step\nKeras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n...layers\n......dense\n.........vars\n............0\n............1\n......dense_1\n.........vars\n............0\n............1\n......dense_2\n.........vars\n............0\n............1\n......dense_3\n.........vars\n............0\n............1\n......normalization\n.........vars\n............0\n............1\n............2\n...metrics\n......mean\n.........vars\n............0\n............1\n...optimizer\n......vars\n.........0\n.........1\n.........10\n.........11\n.........12\n.........13\n.........14\n.........15\n.........16\n.........2\n.........3\n.........4\n.........5\n.........6\n.........7\n.........8\n.........9\n...vars\nKeras model archive saving:\nFile Name                                             Modified             Size\nconfig.json                                    2022-12-10 06:41:26         2825\nvariables.h5                                   2022-12-10 06:41:26        98832\nmetadata.json                                  2022-12-10 06:41:26           64\nKeras model archive loading:\nFile Name                                             Modified             Size\nconfig.json                                    2022-12-10 06:41:26         2825\nvariables.h5                                   2022-12-10 06:41:26        98832\nmetadata.json                                  2022-12-10 06:41:26           64\nEpoch 1/200\n211/211 [==============================] - 2s 3ms/step - loss: 0.2848 - val_loss: 0.1533\nEpoch 2/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.1788 - val_loss: 0.1449\nEpoch 3/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.1675 - val_loss: 0.1444\nEpoch 4/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.1620 - val_loss: 0.1452\nEpoch 5/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1548 - val_loss: 0.1428\nEpoch 6/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1495 - val_loss: 0.1461\nEpoch 7/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1459 - val_loss: 0.1429\nEpoch 8/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.1400 - val_loss: 0.1432\nEpoch 9/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1331 - val_loss: 0.1424\nEpoch 10/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.1261 - val_loss: 0.1460\nEpoch 11/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.1242 - val_loss: 0.1463\nEpoch 12/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.1190 - val_loss: 0.1543\nEpoch 13/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.1125 - val_loss: 0.1523\nEpoch 14/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.1092 - val_loss: 0.1592\nEpoch 15/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.1046 - val_loss: 0.1644\nEpoch 16/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0991 - val_loss: 0.1619\nEpoch 17/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0949 - val_loss: 0.1717\nEpoch 18/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0924 - val_loss: 0.1719\nEpoch 19/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0861 - val_loss: 0.1820\nEpoch 20/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0813 - val_loss: 0.1861\nEpoch 21/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0776 - val_loss: 0.2173\nEpoch 22/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0741 - val_loss: 0.2146\nEpoch 23/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0724 - val_loss: 0.2190\nEpoch 24/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0681 - val_loss: 0.2171\nEpoch 25/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0648 - val_loss: 0.2283\nEpoch 26/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0594 - val_loss: 0.2296\nEpoch 27/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0585 - val_loss: 0.2393\nEpoch 28/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0553 - val_loss: 0.2471\nEpoch 29/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0503 - val_loss: 0.2480\nEpoch 30/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0487 - val_loss: 0.2583\nEpoch 31/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0485 - val_loss: 0.2805\nEpoch 32/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0462 - val_loss: 0.2784\nEpoch 33/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0472 - val_loss: 0.2785\nEpoch 34/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0429 - val_loss: 0.2858\nEpoch 35/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0398 - val_loss: 0.2854\nEpoch 36/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0373 - val_loss: 0.3018\nEpoch 37/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0321 - val_loss: 0.3197\nEpoch 38/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0325 - val_loss: 0.3212\nEpoch 39/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0311 - val_loss: 0.3131\nEpoch 40/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0323 - val_loss: 0.3261\nEpoch 41/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0347 - val_loss: 0.3405\nEpoch 42/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0305 - val_loss: 0.3374\nEpoch 43/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0264 - val_loss: 0.3375\nEpoch 44/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0295 - val_loss: 0.4141\nEpoch 45/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0295 - val_loss: 0.3499\nEpoch 46/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0216 - val_loss: 0.3741\nEpoch 47/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0269 - val_loss: 0.3788\nEpoch 48/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0186 - val_loss: 0.4095\nEpoch 49/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0170 - val_loss: 0.3922\nEpoch 50/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0175 - val_loss: 0.4207\nEpoch 51/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0220 - val_loss: 0.4287\nEpoch 52/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0173 - val_loss: 0.4177\nEpoch 53/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0142 - val_loss: 0.4515\nEpoch 54/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0138 - val_loss: 0.4428\nEpoch 55/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0134 - val_loss: 0.4279\nEpoch 56/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0136 - val_loss: 0.4554\nEpoch 57/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0096 - val_loss: 0.4781\nEpoch 58/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0135 - val_loss: 0.4792\nEpoch 59/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0208 - val_loss: 0.4684\nEpoch 60/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0205 - val_loss: 0.5246\nEpoch 61/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0099 - val_loss: 0.4906\nEpoch 62/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0062 - val_loss: 0.4995\nEpoch 63/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0069 - val_loss: 0.5032\nEpoch 64/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0092 - val_loss: 0.5204\nEpoch 65/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0109 - val_loss: 0.5394\nEpoch 66/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0086 - val_loss: 0.5364\nEpoch 67/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0135 - val_loss: 0.6002\nEpoch 68/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0217 - val_loss: 0.5511\nEpoch 69/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0099 - val_loss: 0.5379\nEpoch 70/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0053 - val_loss: 0.5344\nEpoch 71/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0027 - val_loss: 0.5336\nEpoch 72/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0024 - val_loss: 0.5660\nEpoch 73/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0031 - val_loss: 0.5557\nEpoch 74/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0060 - val_loss: 0.5854\nEpoch 75/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0181 - val_loss: 0.5899\nEpoch 76/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0239 - val_loss: 0.6009\nEpoch 77/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0098 - val_loss: 0.6334\nEpoch 78/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0114 - val_loss: 0.5876\nEpoch 79/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0043 - val_loss: 0.5754\nEpoch 80/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0015 - val_loss: 0.6060\nEpoch 81/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0011 - val_loss: 0.5930\nEpoch 82/200\n211/211 [==============================] - 1s 2ms/step - loss: 9.2247e-04 - val_loss: 0.6071\nEpoch 83/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0053 - val_loss: 0.6339\nEpoch 84/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0438 - val_loss: 0.6012\nEpoch 85/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0228 - val_loss: 0.5338\nEpoch 86/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0054 - val_loss: 0.5512\nEpoch 87/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0019 - val_loss: 0.5619\nEpoch 88/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0012 - val_loss: 0.5663\nEpoch 89/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0010 - val_loss: 0.5670\nEpoch 90/200\n211/211 [==============================] - 1s 3ms/step - loss: 9.7348e-04 - val_loss: 0.5903\nEpoch 91/200\n211/211 [==============================] - 1s 2ms/step - loss: 8.0877e-04 - val_loss: 0.6005\nEpoch 92/200\n211/211 [==============================] - 1s 2ms/step - loss: 6.7685e-04 - val_loss: 0.6001\nEpoch 93/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0136 - val_loss: 0.5846\nEpoch 94/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0266 - val_loss: 0.5577\nEpoch 95/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0086 - val_loss: 0.5500\nEpoch 96/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0027 - val_loss: 0.5614\nEpoch 97/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0014 - val_loss: 0.5672\nEpoch 98/200\n211/211 [==============================] - 1s 2ms/step - loss: 9.0498e-04 - val_loss: 0.5700\nEpoch 99/200\n211/211 [==============================] - 1s 3ms/step - loss: 6.7461e-04 - val_loss: 0.5840\nEpoch 100/200\n211/211 [==============================] - 1s 2ms/step - loss: 6.0460e-04 - val_loss: 0.5940\nEpoch 101/200\n211/211 [==============================] - 1s 2ms/step - loss: 4.6633e-04 - val_loss: 0.5933\nEpoch 102/200\n211/211 [==============================] - 1s 3ms/step - loss: 4.0890e-04 - val_loss: 0.6046\nEpoch 103/200\n211/211 [==============================] - 1s 2ms/step - loss: 3.6609e-04 - val_loss: 0.6094\nEpoch 104/200\n211/211 [==============================] - 1s 3ms/step - loss: 3.3241e-04 - val_loss: 0.6234\nEpoch 105/200\n211/211 [==============================] - 1s 2ms/step - loss: 2.8643e-04 - val_loss: 0.6195\nEpoch 106/200\n211/211 [==============================] - 1s 2ms/step - loss: 2.6296e-04 - val_loss: 0.6329\nEpoch 107/200\n211/211 [==============================] - 1s 2ms/step - loss: 2.4645e-04 - val_loss: 0.6380\nEpoch 108/200\n211/211 [==============================] - 1s 3ms/step - loss: 2.1281e-04 - val_loss: 0.6673\nEpoch 109/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0723 - val_loss: 0.5783\nEpoch 110/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0324 - val_loss: 0.5148\nEpoch 111/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0061 - val_loss: 0.4916\nEpoch 112/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0049 - val_loss: 0.5139\nEpoch 113/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0015 - val_loss: 0.5233\nEpoch 114/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0012 - val_loss: 0.5324\nEpoch 115/200\n211/211 [==============================] - 1s 3ms/step - loss: 9.5081e-04 - val_loss: 0.5401\nEpoch 116/200\n211/211 [==============================] - 1s 2ms/step - loss: 8.1848e-04 - val_loss: 0.5475\nEpoch 117/200\n211/211 [==============================] - 1s 3ms/step - loss: 7.3860e-04 - val_loss: 0.5554\nEpoch 118/200\n211/211 [==============================] - 1s 3ms/step - loss: 6.6441e-04 - val_loss: 0.5644\nEpoch 119/200\n211/211 [==============================] - 1s 3ms/step - loss: 5.4547e-04 - val_loss: 0.5812\nEpoch 120/200\n211/211 [==============================] - 1s 2ms/step - loss: 4.6005e-04 - val_loss: 0.5855\nEpoch 121/200\n211/211 [==============================] - 1s 3ms/step - loss: 4.0598e-04 - val_loss: 0.5963\nEpoch 122/200\n211/211 [==============================] - 1s 3ms/step - loss: 3.7289e-04 - val_loss: 0.6040\nEpoch 123/200\n211/211 [==============================] - 1s 2ms/step - loss: 3.1417e-04 - val_loss: 0.6193\nEpoch 124/200\n211/211 [==============================] - 1s 2ms/step - loss: 2.8783e-04 - val_loss: 0.6247\nEpoch 125/200\n211/211 [==============================] - 1s 2ms/step - loss: 2.6022e-04 - val_loss: 0.6347\nEpoch 126/200\n211/211 [==============================] - 1s 3ms/step - loss: 2.0566e-04 - val_loss: 0.6482\nEpoch 127/200\n211/211 [==============================] - 1s 2ms/step - loss: 1.9764e-04 - val_loss: 0.6529\nEpoch 128/200\n211/211 [==============================] - 1s 3ms/step - loss: 1.6989e-04 - val_loss: 0.6676\nEpoch 129/200\n211/211 [==============================] - 1s 2ms/step - loss: 1.4868e-04 - val_loss: 0.6668\nEpoch 130/200\n211/211 [==============================] - 1s 2ms/step - loss: 1.3163e-04 - val_loss: 0.6801\nEpoch 131/200\n211/211 [==============================] - 1s 2ms/step - loss: 1.1264e-04 - val_loss: 0.6922\nEpoch 132/200\n211/211 [==============================] - 1s 3ms/step - loss: 1.0276e-04 - val_loss: 0.6967\nEpoch 133/200\n211/211 [==============================] - 1s 3ms/step - loss: 9.7421e-05 - val_loss: 0.7090\nEpoch 134/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0015 - val_loss: 0.8850\nEpoch 135/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0946 - val_loss: 0.5174\nEpoch 136/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0162 - val_loss: 0.5026\nEpoch 137/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0036 - val_loss: 0.5046\nEpoch 138/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0011 - val_loss: 0.5050\nEpoch 139/200\n211/211 [==============================] - 1s 3ms/step - loss: 8.5656e-04 - val_loss: 0.5174\nEpoch 140/200\n211/211 [==============================] - 1s 2ms/step - loss: 6.9832e-04 - val_loss: 0.5264\nEpoch 141/200\n211/211 [==============================] - 1s 3ms/step - loss: 6.0006e-04 - val_loss: 0.5365\nEpoch 142/200\n211/211 [==============================] - 1s 3ms/step - loss: 5.2075e-04 - val_loss: 0.5426\nEpoch 143/200\n211/211 [==============================] - 1s 3ms/step - loss: 4.4775e-04 - val_loss: 0.5547\nEpoch 144/200\n211/211 [==============================] - 1s 3ms/step - loss: 3.8894e-04 - val_loss: 0.5647\nEpoch 145/200\n211/211 [==============================] - 1s 2ms/step - loss: 3.4941e-04 - val_loss: 0.5677\nEpoch 146/200\n211/211 [==============================] - 1s 3ms/step - loss: 3.0711e-04 - val_loss: 0.5778\nEpoch 147/200\n211/211 [==============================] - 1s 2ms/step - loss: 2.7357e-04 - val_loss: 0.5887\nEpoch 148/200\n211/211 [==============================] - 1s 3ms/step - loss: 2.4107e-04 - val_loss: 0.5927\nEpoch 149/200\n211/211 [==============================] - 1s 2ms/step - loss: 2.1335e-04 - val_loss: 0.5994\nEpoch 150/200\n211/211 [==============================] - 1s 2ms/step - loss: 1.9584e-04 - val_loss: 0.6094\nEpoch 151/200\n211/211 [==============================] - 1s 2ms/step - loss: 1.6833e-04 - val_loss: 0.6183\nEpoch 152/200\n211/211 [==============================] - 1s 3ms/step - loss: 1.5188e-04 - val_loss: 0.6286\nEpoch 153/200\n211/211 [==============================] - 1s 2ms/step - loss: 1.2946e-04 - val_loss: 0.6364\nEpoch 154/200\n211/211 [==============================] - 1s 3ms/step - loss: 1.1879e-04 - val_loss: 0.6404\nEpoch 155/200\n211/211 [==============================] - 1s 2ms/step - loss: 1.0373e-04 - val_loss: 0.6570\nEpoch 156/200\n211/211 [==============================] - 1s 2ms/step - loss: 8.8966e-05 - val_loss: 0.6610\nEpoch 157/200\n211/211 [==============================] - 1s 3ms/step - loss: 7.9012e-05 - val_loss: 0.6742\nEpoch 158/200\n211/211 [==============================] - 1s 2ms/step - loss: 7.0855e-05 - val_loss: 0.6825\nEpoch 159/200\n211/211 [==============================] - 1s 2ms/step - loss: 6.7304e-05 - val_loss: 0.6887\nEpoch 160/200\n211/211 [==============================] - 1s 2ms/step - loss: 5.8825e-05 - val_loss: 0.6971\nEpoch 161/200\n211/211 [==============================] - 1s 3ms/step - loss: 5.0996e-05 - val_loss: 0.7080\nEpoch 162/200\n211/211 [==============================] - 1s 3ms/step - loss: 4.8463e-05 - val_loss: 0.7222\nEpoch 163/200\n211/211 [==============================] - 1s 2ms/step - loss: 4.2048e-05 - val_loss: 0.7232\nEpoch 164/200\n211/211 [==============================] - 1s 2ms/step - loss: 3.5336e-05 - val_loss: 0.7360\nEpoch 165/200\n211/211 [==============================] - 1s 2ms/step - loss: 3.2116e-05 - val_loss: 0.7432\nEpoch 166/200\n211/211 [==============================] - 1s 3ms/step - loss: 2.8757e-05 - val_loss: 0.7529\nEpoch 167/200\n211/211 [==============================] - 1s 3ms/step - loss: 2.4088e-05 - val_loss: 0.7595\nEpoch 168/200\n211/211 [==============================] - 1s 3ms/step - loss: 2.2610e-05 - val_loss: 0.7665\nEpoch 169/200\n211/211 [==============================] - 1s 3ms/step - loss: 2.0793e-05 - val_loss: 0.7744\nEpoch 170/200\n211/211 [==============================] - 1s 3ms/step - loss: 1.7369e-05 - val_loss: 0.7826\nEpoch 171/200\n211/211 [==============================] - 1s 2ms/step - loss: 1.5265e-05 - val_loss: 0.7961\nEpoch 172/200\n211/211 [==============================] - 1s 3ms/step - loss: 1.3814e-05 - val_loss: 0.7998\nEpoch 173/200\n211/211 [==============================] - 1s 3ms/step - loss: 1.2212e-05 - val_loss: 0.8035\nEpoch 174/200\n211/211 [==============================] - 1s 2ms/step - loss: 1.2075e-05 - val_loss: 0.8168\nEpoch 175/200\n211/211 [==============================] - 1s 3ms/step - loss: 9.6469e-06 - val_loss: 0.8296\nEpoch 176/200\n211/211 [==============================] - 1s 2ms/step - loss: 9.1870e-06 - val_loss: 0.8341\nEpoch 177/200\n211/211 [==============================] - 1s 3ms/step - loss: 9.5208e-05 - val_loss: 0.8230\nEpoch 178/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.1488 - val_loss: 0.6003\nEpoch 179/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0218 - val_loss: 0.5183\nEpoch 180/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0042 - val_loss: 0.5317\nEpoch 181/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0017 - val_loss: 0.5309\nEpoch 182/200\n211/211 [==============================] - 1s 2ms/step - loss: 9.7054e-04 - val_loss: 0.5479\nEpoch 183/200\n211/211 [==============================] - 1s 2ms/step - loss: 7.2673e-04 - val_loss: 0.5521\nEpoch 184/200\n211/211 [==============================] - 1s 3ms/step - loss: 5.8444e-04 - val_loss: 0.5547\nEpoch 185/200\n211/211 [==============================] - 1s 2ms/step - loss: 5.0427e-04 - val_loss: 0.5686\nEpoch 186/200\n211/211 [==============================] - 1s 3ms/step - loss: 4.2986e-04 - val_loss: 0.5734\nEpoch 187/200\n211/211 [==============================] - 1s 2ms/step - loss: 3.7440e-04 - val_loss: 0.5785\nEpoch 188/200\n211/211 [==============================] - 1s 3ms/step - loss: 3.3203e-04 - val_loss: 0.5880\nEpoch 189/200\n211/211 [==============================] - 1s 2ms/step - loss: 2.8444e-04 - val_loss: 0.5916\nEpoch 190/200\n211/211 [==============================] - 1s 2ms/step - loss: 2.5132e-04 - val_loss: 0.5948\nEpoch 191/200\n211/211 [==============================] - 0s 2ms/step - loss: 2.2936e-04 - val_loss: 0.6071\nEpoch 192/200\n211/211 [==============================] - 0s 2ms/step - loss: 1.9856e-04 - val_loss: 0.6134\nEpoch 193/200\n211/211 [==============================] - 1s 2ms/step - loss: 1.7702e-04 - val_loss: 0.6227\nEpoch 194/200\n211/211 [==============================] - 1s 2ms/step - loss: 1.5819e-04 - val_loss: 0.6278\nEpoch 195/200\n211/211 [==============================] - 1s 3ms/step - loss: 1.3981e-04 - val_loss: 0.6360\nEpoch 196/200\n211/211 [==============================] - 1s 3ms/step - loss: 1.2181e-04 - val_loss: 0.6422\nEpoch 197/200\n211/211 [==============================] - 1s 3ms/step - loss: 1.1509e-04 - val_loss: 0.6494\nEpoch 198/200\n211/211 [==============================] - 1s 2ms/step - loss: 9.8611e-05 - val_loss: 0.6550\nEpoch 199/200\n211/211 [==============================] - 1s 2ms/step - loss: 8.8469e-05 - val_loss: 0.6630\nEpoch 200/200\n211/211 [==============================] - 1s 2ms/step - loss: 8.0764e-05 - val_loss: 0.6716\n88/88 [==============================] - 0s 1ms/step\n     .. fold 2 trained/predicted\nKeras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n...layers\n......dense\n.........vars\n............0\n............1\n......dense_1\n.........vars\n............0\n............1\n......dense_2\n.........vars\n............0\n............1\n......dense_3\n.........vars\n............0\n............1\n......normalization\n.........vars\n............0\n............1\n............2\n...metrics\n......mean\n.........vars\n............0\n............1\n...optimizer\n......vars\n.........0\n.........1\n.........10\n.........11\n.........12\n.........13\n.........14\n.........15\n.........16\n.........2\n.........3\n.........4\n.........5\n.........6\n.........7\n.........8\n.........9\n...vars\nKeras model archive saving:\nFile Name                                             Modified             Size\nconfig.json                                    2022-12-10 06:43:16         2833\nvariables.h5                                   2022-12-10 06:43:16        98832\nmetadata.json                                  2022-12-10 06:43:16           64\nKeras model archive loading:\nFile Name                                             Modified             Size\nconfig.json                                    2022-12-10 06:43:16         2833\nvariables.h5                                   2022-12-10 06:43:16        98832\nmetadata.json                                  2022-12-10 06:43:16           64\nEpoch 1/200\n211/211 [==============================] - 2s 3ms/step - loss: 0.2919 - val_loss: 0.1638\nEpoch 2/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1728 - val_loss: 0.1632\nEpoch 3/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1656 - val_loss: 0.1568\nEpoch 4/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1582 - val_loss: 0.1596\nEpoch 5/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1539 - val_loss: 0.1546\nEpoch 6/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1489 - val_loss: 0.1539\nEpoch 7/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1448 - val_loss: 0.1580\nEpoch 8/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.1407 - val_loss: 0.1571\nEpoch 9/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1352 - val_loss: 0.1657\nEpoch 10/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.1308 - val_loss: 0.1618\nEpoch 11/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1278 - val_loss: 0.1585\nEpoch 12/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.1242 - val_loss: 0.1637\nEpoch 13/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.1199 - val_loss: 0.1652\nEpoch 14/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.1137 - val_loss: 0.1674\nEpoch 15/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.1095 - val_loss: 0.1761\nEpoch 16/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.1059 - val_loss: 0.1754\nEpoch 17/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.1016 - val_loss: 0.1830\nEpoch 18/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0973 - val_loss: 0.1909\nEpoch 19/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0922 - val_loss: 0.1884\nEpoch 20/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0904 - val_loss: 0.1939\nEpoch 21/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0865 - val_loss: 0.2072\nEpoch 22/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0790 - val_loss: 0.2129\nEpoch 23/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0758 - val_loss: 0.2252\nEpoch 24/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0724 - val_loss: 0.2215\nEpoch 25/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0691 - val_loss: 0.2343\nEpoch 26/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0676 - val_loss: 0.2560\nEpoch 27/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0637 - val_loss: 0.2641\nEpoch 28/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0548 - val_loss: 0.2586\nEpoch 29/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0521 - val_loss: 0.2674\nEpoch 30/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0505 - val_loss: 0.2847\nEpoch 31/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0488 - val_loss: 0.2936\nEpoch 32/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0464 - val_loss: 0.3010\nEpoch 33/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0411 - val_loss: 0.3093\nEpoch 34/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0365 - val_loss: 0.3395\nEpoch 35/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0383 - val_loss: 0.3493\nEpoch 36/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0392 - val_loss: 0.3444\nEpoch 37/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0355 - val_loss: 0.3596\nEpoch 38/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0326 - val_loss: 0.3602\nEpoch 39/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0288 - val_loss: 0.3965\nEpoch 40/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0266 - val_loss: 0.3805\nEpoch 41/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0251 - val_loss: 0.3894\nEpoch 42/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0237 - val_loss: 0.4158\nEpoch 43/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0234 - val_loss: 0.4223\nEpoch 44/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0211 - val_loss: 0.4293\nEpoch 45/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0239 - val_loss: 0.4796\nEpoch 46/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0222 - val_loss: 0.4815\nEpoch 47/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0206 - val_loss: 0.5604\nEpoch 48/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0284 - val_loss: 0.4468\nEpoch 49/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0179 - val_loss: 0.5158\nEpoch 50/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0231 - val_loss: 0.4855\nEpoch 51/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0157 - val_loss: 0.4726\nEpoch 52/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0105 - val_loss: 0.5057\nEpoch 53/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0102 - val_loss: 0.5131\nEpoch 54/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0110 - val_loss: 0.5563\nEpoch 55/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0132 - val_loss: 0.5456\nEpoch 56/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0183 - val_loss: 0.5798\nEpoch 57/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0369 - val_loss: 0.5677\nEpoch 58/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0240 - val_loss: 0.5361\nEpoch 59/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0112 - val_loss: 0.5493\nEpoch 60/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0083 - val_loss: 0.5560\nEpoch 61/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0070 - val_loss: 0.5674\nEpoch 62/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0055 - val_loss: 0.5712\nEpoch 63/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0057 - val_loss: 0.5984\nEpoch 64/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0062 - val_loss: 0.6124\nEpoch 65/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0071 - val_loss: 0.6078\nEpoch 66/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0183 - val_loss: 0.6154\nEpoch 67/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0217 - val_loss: 0.5801\nEpoch 68/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0111 - val_loss: 0.5626\nEpoch 69/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0056 - val_loss: 0.5844\nEpoch 70/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 0.5849\nEpoch 71/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.6184\nEpoch 72/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0110 - val_loss: 0.6078\nEpoch 73/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0063 - val_loss: 0.6210\nEpoch 74/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0149 - val_loss: 0.6171\nEpoch 75/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0084 - val_loss: 0.6864\nEpoch 76/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0104 - val_loss: 0.6518\nEpoch 77/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0349 - val_loss: 0.6421\nEpoch 78/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0136 - val_loss: 0.6182\nEpoch 79/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0075 - val_loss: 0.6106\nEpoch 80/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0047 - val_loss: 0.6166\nEpoch 81/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.6280\nEpoch 82/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0024 - val_loss: 0.6294\nEpoch 83/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0020 - val_loss: 0.6422\nEpoch 84/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0013 - val_loss: 0.6483\nEpoch 85/200\n211/211 [==============================] - 1s 3ms/step - loss: 8.6781e-04 - val_loss: 0.6601\nEpoch 86/200\n211/211 [==============================] - 0s 2ms/step - loss: 8.9803e-04 - val_loss: 0.6772\nEpoch 87/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0228 - val_loss: 0.6612\nEpoch 88/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0270 - val_loss: 0.6329\nEpoch 89/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0068 - val_loss: 0.6384\nEpoch 90/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0066 - val_loss: 0.6388\nEpoch 91/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0015 - val_loss: 0.6440\nEpoch 92/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.6491\nEpoch 93/200\n211/211 [==============================] - 1s 2ms/step - loss: 7.0519e-04 - val_loss: 0.6591\nEpoch 94/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.6822\nEpoch 95/200\n211/211 [==============================] - 0s 2ms/step - loss: 9.2248e-04 - val_loss: 0.6896\nEpoch 96/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0013 - val_loss: 0.7226\nEpoch 97/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0201 - val_loss: 0.6583\nEpoch 98/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0312 - val_loss: 0.6162\nEpoch 99/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0101 - val_loss: 0.5867\nEpoch 100/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.5971\nEpoch 101/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.6038\nEpoch 102/200\n211/211 [==============================] - 0s 2ms/step - loss: 8.7831e-04 - val_loss: 0.6138\nEpoch 103/200\n211/211 [==============================] - 0s 2ms/step - loss: 6.2369e-04 - val_loss: 0.6279\nEpoch 104/200\n211/211 [==============================] - 0s 2ms/step - loss: 5.2426e-04 - val_loss: 0.6373\nEpoch 105/200\n211/211 [==============================] - 0s 2ms/step - loss: 7.1467e-04 - val_loss: 0.6446\nEpoch 106/200\n211/211 [==============================] - 1s 3ms/step - loss: 5.2125e-04 - val_loss: 0.6581\nEpoch 107/200\n211/211 [==============================] - 0s 2ms/step - loss: 3.3792e-04 - val_loss: 0.6676\nEpoch 108/200\n211/211 [==============================] - 1s 3ms/step - loss: 2.7268e-04 - val_loss: 0.6766\nEpoch 109/200\n211/211 [==============================] - 0s 2ms/step - loss: 2.6170e-04 - val_loss: 0.6847\nEpoch 110/200\n211/211 [==============================] - 0s 2ms/step - loss: 2.0286e-04 - val_loss: 0.6951\nEpoch 111/200\n211/211 [==============================] - 1s 2ms/step - loss: 1.9256e-04 - val_loss: 0.7051\nEpoch 112/200\n211/211 [==============================] - 0s 2ms/step - loss: 1.7101e-04 - val_loss: 0.7153\nEpoch 113/200\n211/211 [==============================] - 1s 2ms/step - loss: 1.5678e-04 - val_loss: 0.7290\nEpoch 114/200\n211/211 [==============================] - 1s 3ms/step - loss: 1.4253e-04 - val_loss: 0.7370\nEpoch 115/200\n211/211 [==============================] - 1s 3ms/step - loss: 1.1671e-04 - val_loss: 0.7461\nEpoch 116/200\n211/211 [==============================] - 0s 2ms/step - loss: 9.9752e-05 - val_loss: 0.7539\nEpoch 117/200\n211/211 [==============================] - 1s 3ms/step - loss: 8.4308e-05 - val_loss: 0.7664\nEpoch 118/200\n211/211 [==============================] - 1s 3ms/step - loss: 7.9353e-05 - val_loss: 0.7717\nEpoch 119/200\n211/211 [==============================] - 0s 2ms/step - loss: 7.1525e-05 - val_loss: 0.7877\nEpoch 120/200\n211/211 [==============================] - 1s 2ms/step - loss: 6.3436e-05 - val_loss: 0.7931\nEpoch 121/200\n211/211 [==============================] - 1s 2ms/step - loss: 5.2530e-05 - val_loss: 0.8045\nEpoch 122/200\n211/211 [==============================] - 1s 3ms/step - loss: 4.9783e-05 - val_loss: 0.8136\nEpoch 123/200\n211/211 [==============================] - 0s 2ms/step - loss: 4.3879e-05 - val_loss: 0.8273\nEpoch 124/200\n211/211 [==============================] - 0s 2ms/step - loss: 4.0988e-05 - val_loss: 0.8363\nEpoch 125/200\n211/211 [==============================] - 0s 2ms/step - loss: 3.7817e-05 - val_loss: 0.8483\nEpoch 126/200\n211/211 [==============================] - 1s 2ms/step - loss: 3.3598e-05 - val_loss: 0.8595\nEpoch 127/200\n211/211 [==============================] - 1s 2ms/step - loss: 3.0207e-05 - val_loss: 0.8671\nEpoch 128/200\n211/211 [==============================] - 1s 3ms/step - loss: 2.3544e-05 - val_loss: 0.8705\nEpoch 129/200\n211/211 [==============================] - 1s 2ms/step - loss: 2.1995e-05 - val_loss: 0.8845\nEpoch 130/200\n211/211 [==============================] - 0s 2ms/step - loss: 1.7468e-05 - val_loss: 0.8959\nEpoch 131/200\n211/211 [==============================] - 0s 2ms/step - loss: 1.6693e-05 - val_loss: 0.9035\nEpoch 132/200\n211/211 [==============================] - 0s 2ms/step - loss: 1.3893e-05 - val_loss: 0.9129\nEpoch 133/200\n211/211 [==============================] - 0s 2ms/step - loss: 1.2048e-05 - val_loss: 0.9249\nEpoch 134/200\n211/211 [==============================] - 0s 2ms/step - loss: 1.1317e-05 - val_loss: 0.9326\nEpoch 135/200\n211/211 [==============================] - 0s 2ms/step - loss: 1.0915e-05 - val_loss: 0.9392\nEpoch 136/200\n211/211 [==============================] - 1s 3ms/step - loss: 8.5624e-06 - val_loss: 0.9533\nEpoch 137/200\n211/211 [==============================] - 1s 2ms/step - loss: 7.8514e-06 - val_loss: 0.9595\nEpoch 138/200\n211/211 [==============================] - 1s 3ms/step - loss: 6.6934e-06 - val_loss: 0.9699\nEpoch 139/200\n211/211 [==============================] - 1s 2ms/step - loss: 5.7788e-06 - val_loss: 0.9767\nEpoch 140/200\n211/211 [==============================] - 1s 3ms/step - loss: 5.6303e-06 - val_loss: 0.9853\nEpoch 141/200\n211/211 [==============================] - 0s 2ms/step - loss: 4.8533e-06 - val_loss: 0.9954\nEpoch 142/200\n211/211 [==============================] - 1s 2ms/step - loss: 5.2507e-06 - val_loss: 1.0068\nEpoch 143/200\n211/211 [==============================] - 1s 2ms/step - loss: 3.7794e-06 - val_loss: 1.0135\nEpoch 144/200\n211/211 [==============================] - 1s 3ms/step - loss: 3.9929e-06 - val_loss: 1.0217\nEpoch 145/200\n211/211 [==============================] - 1s 2ms/step - loss: 3.5529e-06 - val_loss: 1.0342\nEpoch 146/200\n211/211 [==============================] - 0s 2ms/step - loss: 2.8648e-06 - val_loss: 1.0436\nEpoch 147/200\n211/211 [==============================] - 0s 2ms/step - loss: 2.2089e-06 - val_loss: 1.0551\nEpoch 148/200\n211/211 [==============================] - 1s 3ms/step - loss: 2.1434e-06 - val_loss: 1.0650\nEpoch 149/200\n211/211 [==============================] - 0s 2ms/step - loss: 1.7236e-06 - val_loss: 1.0717\nEpoch 150/200\n211/211 [==============================] - 0s 2ms/step - loss: 1.7414e-06 - val_loss: 1.0796\nEpoch 151/200\n211/211 [==============================] - 0s 2ms/step - loss: 1.5342e-06 - val_loss: 1.0911\nEpoch 152/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0895 - val_loss: 1.0877\nEpoch 153/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.1050 - val_loss: 0.6295\nEpoch 154/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0070 - val_loss: 0.6237\nEpoch 155/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0032 - val_loss: 0.6345\nEpoch 156/200\n211/211 [==============================] - 1s 2ms/step - loss: 8.8281e-04 - val_loss: 0.6419\nEpoch 157/200\n211/211 [==============================] - 1s 2ms/step - loss: 6.4886e-04 - val_loss: 0.6504\nEpoch 158/200\n211/211 [==============================] - 1s 2ms/step - loss: 5.7664e-04 - val_loss: 0.6565\nEpoch 159/200\n211/211 [==============================] - 1s 2ms/step - loss: 4.5228e-04 - val_loss: 0.6619\nEpoch 160/200\n211/211 [==============================] - 1s 3ms/step - loss: 4.1397e-04 - val_loss: 0.6686\nEpoch 161/200\n211/211 [==============================] - 0s 2ms/step - loss: 3.5395e-04 - val_loss: 0.6763\nEpoch 162/200\n211/211 [==============================] - 0s 2ms/step - loss: 3.1827e-04 - val_loss: 0.6830\nEpoch 163/200\n211/211 [==============================] - 0s 2ms/step - loss: 2.7505e-04 - val_loss: 0.6892\nEpoch 164/200\n211/211 [==============================] - 0s 2ms/step - loss: 2.4088e-04 - val_loss: 0.6969\nEpoch 165/200\n211/211 [==============================] - 0s 2ms/step - loss: 2.3413e-04 - val_loss: 0.7030\nEpoch 166/200\n211/211 [==============================] - 0s 2ms/step - loss: 2.0582e-04 - val_loss: 0.7103\nEpoch 167/200\n211/211 [==============================] - 0s 2ms/step - loss: 1.6827e-04 - val_loss: 0.7173\nEpoch 168/200\n211/211 [==============================] - 1s 3ms/step - loss: 1.5371e-04 - val_loss: 0.7229\nEpoch 169/200\n211/211 [==============================] - 1s 3ms/step - loss: 1.4250e-04 - val_loss: 0.7308\nEpoch 170/200\n211/211 [==============================] - 0s 2ms/step - loss: 1.2404e-04 - val_loss: 0.7386\nEpoch 171/200\n211/211 [==============================] - 0s 2ms/step - loss: 1.0967e-04 - val_loss: 0.7456\nEpoch 172/200\n211/211 [==============================] - 0s 2ms/step - loss: 9.5854e-05 - val_loss: 0.7513\nEpoch 173/200\n211/211 [==============================] - 0s 2ms/step - loss: 9.0444e-05 - val_loss: 0.7593\nEpoch 174/200\n211/211 [==============================] - 0s 2ms/step - loss: 7.5632e-05 - val_loss: 0.7675\nEpoch 175/200\n211/211 [==============================] - 1s 3ms/step - loss: 6.6368e-05 - val_loss: 0.7737\nEpoch 176/200\n211/211 [==============================] - 0s 2ms/step - loss: 6.2297e-05 - val_loss: 0.7822\nEpoch 177/200\n211/211 [==============================] - 1s 3ms/step - loss: 5.1331e-05 - val_loss: 0.7894\nEpoch 178/200\n211/211 [==============================] - 0s 2ms/step - loss: 5.3321e-05 - val_loss: 0.7973\nEpoch 179/200\n211/211 [==============================] - 1s 3ms/step - loss: 4.1949e-05 - val_loss: 0.8040\nEpoch 180/200\n211/211 [==============================] - 0s 2ms/step - loss: 3.9350e-05 - val_loss: 0.8138\nEpoch 181/200\n211/211 [==============================] - 1s 2ms/step - loss: 3.3569e-05 - val_loss: 0.8206\nEpoch 182/200\n211/211 [==============================] - 1s 3ms/step - loss: 2.9817e-05 - val_loss: 0.8281\nEpoch 183/200\n211/211 [==============================] - 0s 2ms/step - loss: 2.6603e-05 - val_loss: 0.8358\nEpoch 184/200\n211/211 [==============================] - 0s 2ms/step - loss: 2.3741e-05 - val_loss: 0.8445\nEpoch 185/200\n211/211 [==============================] - 0s 2ms/step - loss: 2.1961e-05 - val_loss: 0.8515\nEpoch 186/200\n211/211 [==============================] - 0s 2ms/step - loss: 1.9295e-05 - val_loss: 0.8601\nEpoch 187/200\n211/211 [==============================] - 0s 2ms/step - loss: 1.7056e-05 - val_loss: 0.8669\nEpoch 188/200\n211/211 [==============================] - 1s 2ms/step - loss: 1.5052e-05 - val_loss: 0.8745\nEpoch 189/200\n211/211 [==============================] - 0s 2ms/step - loss: 1.3743e-05 - val_loss: 0.8826\nEpoch 190/200\n211/211 [==============================] - 0s 2ms/step - loss: 1.1905e-05 - val_loss: 0.8913\nEpoch 191/200\n211/211 [==============================] - 0s 2ms/step - loss: 1.1769e-05 - val_loss: 0.9004\nEpoch 192/200\n211/211 [==============================] - 1s 3ms/step - loss: 9.5616e-06 - val_loss: 0.9085\nEpoch 193/200\n211/211 [==============================] - 0s 2ms/step - loss: 8.7727e-06 - val_loss: 0.9157\nEpoch 194/200\n211/211 [==============================] - 0s 2ms/step - loss: 7.6593e-06 - val_loss: 0.9216\nEpoch 195/200\n211/211 [==============================] - 0s 2ms/step - loss: 6.8874e-06 - val_loss: 0.9313\nEpoch 196/200\n211/211 [==============================] - 1s 3ms/step - loss: 6.1234e-06 - val_loss: 0.9396\nEpoch 197/200\n211/211 [==============================] - 1s 2ms/step - loss: 5.2655e-06 - val_loss: 0.9473\nEpoch 198/200\n211/211 [==============================] - 1s 3ms/step - loss: 4.6670e-06 - val_loss: 0.9543\nEpoch 199/200\n211/211 [==============================] - 1s 2ms/step - loss: 4.2617e-06 - val_loss: 0.9621\nEpoch 200/200\n211/211 [==============================] - 1s 2ms/step - loss: 3.8681e-06 - val_loss: 0.9723\n88/88 [==============================] - 0s 1ms/step\nKeras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n...layers\n......dense\n.........vars\n............0\n............1\n......dense_1\n.........vars\n............0\n............1\n......dense_2\n.........vars\n............0\n............1\n......dense_3\n.........vars\n............0\n............1\n......normalization\n.........vars\n............0\n............1\n............2\n...metrics\n......mean\n.........vars\n............0\n............1\n...optimizer\n......vars\n.........0\n.........1\n.........10\n.........11\n.........12\n.........13\n.........14\n.........15\n.........16\n.........2\n.........3\n.........4\n.........5\n.........6\n.........7\n.........8\n.........9\n...vars\nKeras model archive saving:\nFile Name                                             Modified             Size\nconfig.json                                    2022-12-10 06:45:02         2835\nvariables.h5                                   2022-12-10 06:45:02        98832\nmetadata.json                                  2022-12-10 06:45:02           64\nKeras model archive loading:\nFile Name                                             Modified             Size\nconfig.json                                    2022-12-10 06:45:02         2835\nvariables.h5                                   2022-12-10 06:45:02        98832\nmetadata.json                                  2022-12-10 06:45:02           64\nEpoch 1/200\n211/211 [==============================] - 2s 4ms/step - loss: 0.2801 - val_loss: 0.1667\nEpoch 2/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.1802 - val_loss: 0.1662\nEpoch 3/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1717 - val_loss: 0.1591\nEpoch 4/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.1644 - val_loss: 0.1604\nEpoch 5/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1590 - val_loss: 0.1572\nEpoch 6/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.1545 - val_loss: 0.1579\nEpoch 7/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.1491 - val_loss: 0.1556\nEpoch 8/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1438 - val_loss: 0.1592\nEpoch 9/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1399 - val_loss: 0.1558\nEpoch 10/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1340 - val_loss: 0.1653\nEpoch 11/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.1316 - val_loss: 0.1599\nEpoch 12/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.1247 - val_loss: 0.1761\nEpoch 13/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1204 - val_loss: 0.1705\nEpoch 14/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1159 - val_loss: 0.1721\nEpoch 15/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1094 - val_loss: 0.1755\nEpoch 16/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1076 - val_loss: 0.1823\nEpoch 17/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1009 - val_loss: 0.1898\nEpoch 18/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0974 - val_loss: 0.1922\nEpoch 19/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0912 - val_loss: 0.1943\nEpoch 20/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0884 - val_loss: 0.1994\nEpoch 21/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0857 - val_loss: 0.1872\nEpoch 22/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0810 - val_loss: 0.2008\nEpoch 23/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0773 - val_loss: 0.2095\nEpoch 24/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0741 - val_loss: 0.2125\nEpoch 25/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0679 - val_loss: 0.2184\nEpoch 26/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0662 - val_loss: 0.2190\nEpoch 27/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0625 - val_loss: 0.2356\nEpoch 28/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0603 - val_loss: 0.2339\nEpoch 29/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0556 - val_loss: 0.2523\nEpoch 30/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0540 - val_loss: 0.2461\nEpoch 31/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0499 - val_loss: 0.2596\nEpoch 32/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0480 - val_loss: 0.2757\nEpoch 33/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0455 - val_loss: 0.2757\nEpoch 34/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0456 - val_loss: 0.2793\nEpoch 35/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0405 - val_loss: 0.2942\nEpoch 36/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0432 - val_loss: 0.2910\nEpoch 37/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0372 - val_loss: 0.2990\nEpoch 38/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0358 - val_loss: 0.3082\nEpoch 39/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0320 - val_loss: 0.3157\nEpoch 40/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0311 - val_loss: 0.3275\nEpoch 41/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0305 - val_loss: 0.3319\nEpoch 42/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0277 - val_loss: 0.3661\nEpoch 43/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0264 - val_loss: 0.3651\nEpoch 44/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0255 - val_loss: 0.3683\nEpoch 45/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0254 - val_loss: 0.3837\nEpoch 46/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0262 - val_loss: 0.4091\nEpoch 47/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0227 - val_loss: 0.4115\nEpoch 48/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0194 - val_loss: 0.4181\nEpoch 49/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0195 - val_loss: 0.4087\nEpoch 50/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0174 - val_loss: 0.4295\nEpoch 51/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0179 - val_loss: 0.4479\nEpoch 52/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0161 - val_loss: 0.4542\nEpoch 53/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0130 - val_loss: 0.4651\nEpoch 54/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0116 - val_loss: 0.4800\nEpoch 55/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0186 - val_loss: 0.5283\nEpoch 56/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0309 - val_loss: 0.4958\nEpoch 57/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0187 - val_loss: 0.5134\nEpoch 58/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0121 - val_loss: 0.5146\nEpoch 59/200\n211/211 [==============================] - 1s 4ms/step - loss: 0.0080 - val_loss: 0.5219\nEpoch 60/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0102 - val_loss: 0.5361\nEpoch 61/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0088 - val_loss: 0.5346\nEpoch 62/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0085 - val_loss: 0.5595\nEpoch 63/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0079 - val_loss: 0.5679\nEpoch 64/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0075 - val_loss: 0.5759\nEpoch 65/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0125 - val_loss: 0.5852\nEpoch 66/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0098 - val_loss: 0.5753\nEpoch 67/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0054 - val_loss: 0.6168\nEpoch 68/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0095 - val_loss: 0.5992\nEpoch 69/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0235 - val_loss: 0.6202\nEpoch 70/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.6027\nEpoch 71/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0154 - val_loss: 0.6074\nEpoch 72/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0074 - val_loss: 0.5723\nEpoch 73/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0026 - val_loss: 0.5931\nEpoch 74/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0028 - val_loss: 0.5999\nEpoch 75/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0027 - val_loss: 0.6213\nEpoch 76/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0026 - val_loss: 0.6339\nEpoch 77/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0071 - val_loss: 0.6309\nEpoch 78/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0169 - val_loss: 0.6343\nEpoch 79/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0228 - val_loss: 0.6077\nEpoch 80/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0043 - val_loss: 0.6112\nEpoch 81/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.6262\nEpoch 82/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0013 - val_loss: 0.6346\nEpoch 83/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0011 - val_loss: 0.6422\nEpoch 84/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.6515\nEpoch 85/200\n211/211 [==============================] - 0s 2ms/step - loss: 7.6192e-04 - val_loss: 0.6580\nEpoch 86/200\n211/211 [==============================] - 0s 2ms/step - loss: 5.9301e-04 - val_loss: 0.6708\nEpoch 87/200\n211/211 [==============================] - 0s 2ms/step - loss: 5.1391e-04 - val_loss: 0.6795\nEpoch 88/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0379 - val_loss: 0.6047\nEpoch 89/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0364 - val_loss: 0.5792\nEpoch 90/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0104 - val_loss: 0.6040\nEpoch 91/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0038 - val_loss: 0.5840\nEpoch 92/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.5988\nEpoch 93/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0012 - val_loss: 0.6106\nEpoch 94/200\n211/211 [==============================] - 0s 2ms/step - loss: 9.6877e-04 - val_loss: 0.6229\nEpoch 95/200\n211/211 [==============================] - 0s 2ms/step - loss: 9.0277e-04 - val_loss: 0.6351\nEpoch 96/200\n211/211 [==============================] - 0s 2ms/step - loss: 7.3471e-04 - val_loss: 0.6465\nEpoch 97/200\n211/211 [==============================] - 1s 3ms/step - loss: 6.5488e-04 - val_loss: 0.6572\nEpoch 98/200\n211/211 [==============================] - 0s 2ms/step - loss: 5.4725e-04 - val_loss: 0.6645\nEpoch 99/200\n211/211 [==============================] - 1s 2ms/step - loss: 5.0592e-04 - val_loss: 0.6834\nEpoch 100/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0164 - val_loss: 0.7038\nEpoch 101/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0439 - val_loss: 0.5848\nEpoch 102/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0099 - val_loss: 0.5768\nEpoch 103/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0024 - val_loss: 0.6000\nEpoch 104/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0013 - val_loss: 0.6120\nEpoch 105/200\n211/211 [==============================] - 1s 2ms/step - loss: 9.2350e-04 - val_loss: 0.6263\nEpoch 106/200\n211/211 [==============================] - 1s 2ms/step - loss: 8.1183e-04 - val_loss: 0.6389\nEpoch 107/200\n211/211 [==============================] - 1s 3ms/step - loss: 5.7802e-04 - val_loss: 0.6448\nEpoch 108/200\n211/211 [==============================] - 1s 2ms/step - loss: 5.7827e-04 - val_loss: 0.6544\nEpoch 109/200\n211/211 [==============================] - 1s 2ms/step - loss: 5.6100e-04 - val_loss: 0.6690\nEpoch 110/200\n211/211 [==============================] - 1s 2ms/step - loss: 4.0503e-04 - val_loss: 0.6787\nEpoch 111/200\n211/211 [==============================] - 0s 2ms/step - loss: 3.6385e-04 - val_loss: 0.6910\nEpoch 112/200\n211/211 [==============================] - 0s 2ms/step - loss: 3.0881e-04 - val_loss: 0.7006\nEpoch 113/200\n211/211 [==============================] - 1s 2ms/step - loss: 2.7292e-04 - val_loss: 0.7105\nEpoch 114/200\n211/211 [==============================] - 1s 2ms/step - loss: 2.4781e-04 - val_loss: 0.7218\nEpoch 115/200\n211/211 [==============================] - 1s 3ms/step - loss: 2.2648e-04 - val_loss: 0.7334\nEpoch 116/200\n211/211 [==============================] - 0s 2ms/step - loss: 2.0384e-04 - val_loss: 0.7455\nEpoch 117/200\n211/211 [==============================] - 0s 2ms/step - loss: 2.0745e-04 - val_loss: 0.7587\nEpoch 118/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0389 - val_loss: 0.7433\nEpoch 119/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0412 - val_loss: 0.5696\nEpoch 120/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0095 - val_loss: 0.5593\nEpoch 121/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0028 - val_loss: 0.5725\nEpoch 122/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0013 - val_loss: 0.5861\nEpoch 123/200\n211/211 [==============================] - 0s 2ms/step - loss: 8.2185e-04 - val_loss: 0.5927\nEpoch 124/200\n211/211 [==============================] - 0s 2ms/step - loss: 6.8551e-04 - val_loss: 0.6011\nEpoch 125/200\n211/211 [==============================] - 0s 2ms/step - loss: 5.7643e-04 - val_loss: 0.6106\nEpoch 126/200\n211/211 [==============================] - 1s 2ms/step - loss: 4.9544e-04 - val_loss: 0.6208\nEpoch 127/200\n211/211 [==============================] - 0s 2ms/step - loss: 4.3551e-04 - val_loss: 0.6315\nEpoch 128/200\n211/211 [==============================] - 0s 2ms/step - loss: 3.7713e-04 - val_loss: 0.6367\nEpoch 129/200\n211/211 [==============================] - 0s 2ms/step - loss: 3.3959e-04 - val_loss: 0.6480\nEpoch 130/200\n211/211 [==============================] - 0s 2ms/step - loss: 3.0988e-04 - val_loss: 0.6569\nEpoch 131/200\n211/211 [==============================] - 1s 2ms/step - loss: 2.6300e-04 - val_loss: 0.6670\nEpoch 132/200\n211/211 [==============================] - 0s 2ms/step - loss: 2.2644e-04 - val_loss: 0.6777\nEpoch 133/200\n211/211 [==============================] - 0s 2ms/step - loss: 2.0394e-04 - val_loss: 0.6856\nEpoch 134/200\n211/211 [==============================] - 1s 3ms/step - loss: 1.8530e-04 - val_loss: 0.6932\nEpoch 135/200\n211/211 [==============================] - 0s 2ms/step - loss: 1.6195e-04 - val_loss: 0.7027\nEpoch 136/200\n211/211 [==============================] - 1s 2ms/step - loss: 1.3864e-04 - val_loss: 0.7127\nEpoch 137/200\n211/211 [==============================] - 0s 2ms/step - loss: 1.2518e-04 - val_loss: 0.7231\nEpoch 138/200\n211/211 [==============================] - 1s 3ms/step - loss: 1.1102e-04 - val_loss: 0.7339\nEpoch 139/200\n211/211 [==============================] - 1s 3ms/step - loss: 1.0439e-04 - val_loss: 0.7401\nEpoch 140/200\n211/211 [==============================] - 1s 3ms/step - loss: 8.8494e-05 - val_loss: 0.7510\nEpoch 141/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0541 - val_loss: 0.7107\nEpoch 142/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0375 - val_loss: 0.5172\nEpoch 143/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0056 - val_loss: 0.5321\nEpoch 144/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.5433\nEpoch 145/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0012 - val_loss: 0.5572\nEpoch 146/200\n211/211 [==============================] - 0s 2ms/step - loss: 8.3535e-04 - val_loss: 0.5675\nEpoch 147/200\n211/211 [==============================] - 0s 2ms/step - loss: 6.6117e-04 - val_loss: 0.5796\nEpoch 148/200\n211/211 [==============================] - 0s 2ms/step - loss: 5.5993e-04 - val_loss: 0.5885\nEpoch 149/200\n211/211 [==============================] - 1s 2ms/step - loss: 4.7888e-04 - val_loss: 0.5994\nEpoch 150/200\n211/211 [==============================] - 1s 3ms/step - loss: 4.1623e-04 - val_loss: 0.6086\nEpoch 151/200\n211/211 [==============================] - 0s 2ms/step - loss: 3.6577e-04 - val_loss: 0.6181\nEpoch 152/200\n211/211 [==============================] - 0s 2ms/step - loss: 3.1838e-04 - val_loss: 0.6280\nEpoch 153/200\n211/211 [==============================] - 1s 2ms/step - loss: 2.8159e-04 - val_loss: 0.6372\nEpoch 154/200\n211/211 [==============================] - 1s 3ms/step - loss: 2.4939e-04 - val_loss: 0.6465\nEpoch 155/200\n211/211 [==============================] - 1s 3ms/step - loss: 2.1819e-04 - val_loss: 0.6573\nEpoch 156/200\n211/211 [==============================] - 1s 2ms/step - loss: 1.8924e-04 - val_loss: 0.6657\nEpoch 157/200\n211/211 [==============================] - 0s 2ms/step - loss: 1.6808e-04 - val_loss: 0.6749\nEpoch 158/200\n211/211 [==============================] - 0s 2ms/step - loss: 1.4883e-04 - val_loss: 0.6846\nEpoch 159/200\n211/211 [==============================] - 1s 2ms/step - loss: 1.3341e-04 - val_loss: 0.6933\nEpoch 160/200\n211/211 [==============================] - 0s 2ms/step - loss: 1.1864e-04 - val_loss: 0.7033\nEpoch 161/200\n211/211 [==============================] - 1s 2ms/step - loss: 1.0578e-04 - val_loss: 0.7133\nEpoch 162/200\n211/211 [==============================] - 1s 2ms/step - loss: 9.2689e-05 - val_loss: 0.7223\nEpoch 163/200\n211/211 [==============================] - 1s 2ms/step - loss: 7.7659e-05 - val_loss: 0.7306\nEpoch 164/200\n211/211 [==============================] - 1s 3ms/step - loss: 7.0727e-05 - val_loss: 0.7404\nEpoch 165/200\n211/211 [==============================] - 1s 2ms/step - loss: 6.4416e-05 - val_loss: 0.7486\nEpoch 166/200\n211/211 [==============================] - 1s 2ms/step - loss: 5.6039e-05 - val_loss: 0.7599\nEpoch 167/200\n211/211 [==============================] - 1s 2ms/step - loss: 4.7346e-05 - val_loss: 0.7686\nEpoch 168/200\n211/211 [==============================] - 1s 3ms/step - loss: 4.2153e-05 - val_loss: 0.7785\nEpoch 169/200\n211/211 [==============================] - 0s 2ms/step - loss: 3.8070e-05 - val_loss: 0.7850\nEpoch 170/200\n211/211 [==============================] - 0s 2ms/step - loss: 3.5604e-05 - val_loss: 0.7989\nEpoch 171/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0622 - val_loss: 0.6653\nEpoch 172/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0583 - val_loss: 0.5012\nEpoch 173/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0113 - val_loss: 0.5099\nEpoch 174/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0038 - val_loss: 0.5310\nEpoch 175/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0025 - val_loss: 0.5484\nEpoch 176/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0017 - val_loss: 0.5639\nEpoch 177/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0014 - val_loss: 0.5771\nEpoch 178/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0011 - val_loss: 0.5902\nEpoch 179/200\n211/211 [==============================] - 1s 3ms/step - loss: 8.2717e-04 - val_loss: 0.6042\nEpoch 180/200\n211/211 [==============================] - 1s 3ms/step - loss: 7.2396e-04 - val_loss: 0.6183\nEpoch 181/200\n211/211 [==============================] - 1s 3ms/step - loss: 6.3811e-04 - val_loss: 0.6304\nEpoch 182/200\n211/211 [==============================] - 1s 2ms/step - loss: 5.9298e-04 - val_loss: 0.6416\nEpoch 183/200\n211/211 [==============================] - 1s 2ms/step - loss: 5.2421e-04 - val_loss: 0.6541\nEpoch 184/200\n211/211 [==============================] - 1s 2ms/step - loss: 3.7688e-04 - val_loss: 0.6645\nEpoch 185/200\n211/211 [==============================] - 1s 3ms/step - loss: 3.2052e-04 - val_loss: 0.6754\nEpoch 186/200\n211/211 [==============================] - 1s 3ms/step - loss: 2.7070e-04 - val_loss: 0.6839\nEpoch 187/200\n211/211 [==============================] - 0s 2ms/step - loss: 2.2170e-04 - val_loss: 0.6940\nEpoch 188/200\n211/211 [==============================] - 1s 2ms/step - loss: 1.9935e-04 - val_loss: 0.7048\nEpoch 189/200\n211/211 [==============================] - 0s 2ms/step - loss: 1.6844e-04 - val_loss: 0.7134\nEpoch 190/200\n211/211 [==============================] - 0s 2ms/step - loss: 1.4099e-04 - val_loss: 0.7244\nEpoch 191/200\n211/211 [==============================] - 0s 2ms/step - loss: 1.2443e-04 - val_loss: 0.7324\nEpoch 192/200\n211/211 [==============================] - 0s 2ms/step - loss: 1.1385e-04 - val_loss: 0.7433\nEpoch 193/200\n211/211 [==============================] - 1s 3ms/step - loss: 1.0027e-04 - val_loss: 0.7516\nEpoch 194/200\n211/211 [==============================] - 0s 2ms/step - loss: 9.6094e-05 - val_loss: 0.7637\nEpoch 195/200\n211/211 [==============================] - 0s 2ms/step - loss: 8.0571e-05 - val_loss: 0.7720\nEpoch 196/200\n211/211 [==============================] - 1s 2ms/step - loss: 6.7866e-05 - val_loss: 0.7813\nEpoch 197/200\n211/211 [==============================] - 1s 3ms/step - loss: 5.9879e-05 - val_loss: 0.7916\nEpoch 198/200\n211/211 [==============================] - 0s 2ms/step - loss: 5.3333e-05 - val_loss: 0.8011\nEpoch 199/200\n211/211 [==============================] - 0s 2ms/step - loss: 4.7434e-05 - val_loss: 0.8099\nEpoch 200/200\n211/211 [==============================] - 1s 2ms/step - loss: 4.3856e-05 - val_loss: 0.8195\n88/88 [==============================] - 0s 1ms/step\n     .. fold 4 trained/predicted\n     . Optimising model hyperparameters\n     .. Model not in optimisation list <verstack.stacking.kerasModel.kerasModel object at 0x7f972b8cd590>\nEpoch 1/200\n211/211 [==============================] - 2s 5ms/step - loss: 0.2981 - val_loss: 0.1594\nEpoch 2/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.1805 - val_loss: 0.1529\nEpoch 3/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.1739 - val_loss: 0.1439\nEpoch 4/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.1652 - val_loss: 0.1520\nEpoch 5/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.1597 - val_loss: 0.1401\nEpoch 6/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1558 - val_loss: 0.1413\nEpoch 7/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1505 - val_loss: 0.1438\nEpoch 8/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1465 - val_loss: 0.1423\nEpoch 9/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1425 - val_loss: 0.1408\nEpoch 10/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1372 - val_loss: 0.1410\nEpoch 11/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1349 - val_loss: 0.1412\nEpoch 12/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.1318 - val_loss: 0.1464\nEpoch 13/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1273 - val_loss: 0.1446\nEpoch 14/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1249 - val_loss: 0.1450\nEpoch 15/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.1184 - val_loss: 0.1509\nEpoch 16/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1161 - val_loss: 0.1481\nEpoch 17/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.1116 - val_loss: 0.1571\nEpoch 18/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.1088 - val_loss: 0.1532\nEpoch 19/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1043 - val_loss: 0.1534\nEpoch 20/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1000 - val_loss: 0.1611\nEpoch 21/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0973 - val_loss: 0.1573\nEpoch 22/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0941 - val_loss: 0.1649\nEpoch 23/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0900 - val_loss: 0.1667\nEpoch 24/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0863 - val_loss: 0.1770\nEpoch 25/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0846 - val_loss: 0.1758\nEpoch 26/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0802 - val_loss: 0.1751\nEpoch 27/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0771 - val_loss: 0.1787\nEpoch 28/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0745 - val_loss: 0.1823\nEpoch 29/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0707 - val_loss: 0.1901\nEpoch 30/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0687 - val_loss: 0.1820\nEpoch 31/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0652 - val_loss: 0.1919\nEpoch 32/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0633 - val_loss: 0.1927\nEpoch 33/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0616 - val_loss: 0.2098\nEpoch 34/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0586 - val_loss: 0.2027\nEpoch 35/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0553 - val_loss: 0.2016\nEpoch 36/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0538 - val_loss: 0.2111\nEpoch 37/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0511 - val_loss: 0.2092\nEpoch 38/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0497 - val_loss: 0.2212\nEpoch 39/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0465 - val_loss: 0.2162\nEpoch 40/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0444 - val_loss: 0.2167\nEpoch 41/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0425 - val_loss: 0.2192\nEpoch 42/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0410 - val_loss: 0.2274\nEpoch 43/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0388 - val_loss: 0.2305\nEpoch 44/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0410 - val_loss: 0.2385\nEpoch 45/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0366 - val_loss: 0.2453\nEpoch 46/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0342 - val_loss: 0.2503\nEpoch 47/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0359 - val_loss: 0.2446\nEpoch 48/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0319 - val_loss: 0.2456\nEpoch 49/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0297 - val_loss: 0.2542\nEpoch 50/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0269 - val_loss: 0.2482\nEpoch 51/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0270 - val_loss: 0.2632\nEpoch 52/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0248 - val_loss: 0.2649\nEpoch 53/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0231 - val_loss: 0.2729\nEpoch 54/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0214 - val_loss: 0.2750\nEpoch 55/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0235 - val_loss: 0.2760\nEpoch 56/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0211 - val_loss: 0.2722\nEpoch 57/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0189 - val_loss: 0.2720\nEpoch 58/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0171 - val_loss: 0.2811\nEpoch 59/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0198 - val_loss: 0.2796\nEpoch 60/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0175 - val_loss: 0.2873\nEpoch 61/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0154 - val_loss: 0.2987\nEpoch 62/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0146 - val_loss: 0.3077\nEpoch 63/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0148 - val_loss: 0.3016\nEpoch 64/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0141 - val_loss: 0.3073\nEpoch 65/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0114 - val_loss: 0.3134\nEpoch 66/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0110 - val_loss: 0.3231\nEpoch 67/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0105 - val_loss: 0.3116\nEpoch 68/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0102 - val_loss: 0.3287\nEpoch 69/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0095 - val_loss: 0.3264\nEpoch 70/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0112 - val_loss: 0.3353\nEpoch 71/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0167 - val_loss: 0.3186\nEpoch 72/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0090 - val_loss: 0.3335\nEpoch 73/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0066 - val_loss: 0.3289\nEpoch 74/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0065 - val_loss: 0.3374\nEpoch 75/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0059 - val_loss: 0.3407\nEpoch 76/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.3496\nEpoch 77/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0083 - val_loss: 0.3589\nEpoch 78/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0179 - val_loss: 0.3734\nEpoch 79/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0107 - val_loss: 0.3493\nEpoch 80/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0075 - val_loss: 0.3670\nEpoch 81/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0043 - val_loss: 0.3574\nEpoch 82/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0043 - val_loss: 0.3595\nEpoch 83/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0033 - val_loss: 0.3624\nEpoch 84/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0029 - val_loss: 0.3624\nEpoch 85/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0029 - val_loss: 0.3727\nEpoch 86/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0167 - val_loss: 0.3709\nEpoch 87/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0086 - val_loss: 0.3788\nEpoch 88/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0059 - val_loss: 0.3706\nEpoch 89/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0030 - val_loss: 0.3786\nEpoch 90/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0022 - val_loss: 0.3782\nEpoch 91/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0020 - val_loss: 0.3818\nEpoch 92/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0019 - val_loss: 0.3850\nEpoch 93/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0019 - val_loss: 0.3930\nEpoch 94/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0017 - val_loss: 0.3952\nEpoch 95/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0015 - val_loss: 0.4018\nEpoch 96/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0014 - val_loss: 0.4031\nEpoch 97/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0013 - val_loss: 0.4064\nEpoch 98/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0011 - val_loss: 0.4102\nEpoch 99/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0011 - val_loss: 0.4269\nEpoch 100/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0012 - val_loss: 0.4276\nEpoch 101/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0012 - val_loss: 0.4200\nEpoch 102/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0208 - val_loss: 0.4281\nEpoch 103/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0225 - val_loss: 0.3765\nEpoch 104/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0064 - val_loss: 0.3777\nEpoch 105/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0019 - val_loss: 0.3906\nEpoch 106/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0013 - val_loss: 0.3960\nEpoch 107/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0011 - val_loss: 0.3999\nEpoch 108/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0010 - val_loss: 0.3987\nEpoch 109/200\n211/211 [==============================] - 1s 2ms/step - loss: 9.5562e-04 - val_loss: 0.4071\nEpoch 110/200\n211/211 [==============================] - 1s 2ms/step - loss: 8.7037e-04 - val_loss: 0.4073\nEpoch 111/200\n211/211 [==============================] - 1s 2ms/step - loss: 7.8844e-04 - val_loss: 0.4076\nEpoch 112/200\n211/211 [==============================] - 1s 2ms/step - loss: 7.5788e-04 - val_loss: 0.4174\nEpoch 113/200\n211/211 [==============================] - 1s 2ms/step - loss: 7.0295e-04 - val_loss: 0.4149\nEpoch 114/200\n211/211 [==============================] - 0s 2ms/step - loss: 6.5501e-04 - val_loss: 0.4181\nEpoch 115/200\n211/211 [==============================] - 0s 2ms/step - loss: 6.2883e-04 - val_loss: 0.4234\nEpoch 116/200\n211/211 [==============================] - 1s 3ms/step - loss: 5.5997e-04 - val_loss: 0.4284\nEpoch 117/200\n211/211 [==============================] - 1s 2ms/step - loss: 5.5646e-04 - val_loss: 0.4300\nEpoch 118/200\n211/211 [==============================] - 1s 3ms/step - loss: 4.8111e-04 - val_loss: 0.4355\nEpoch 119/200\n211/211 [==============================] - 1s 2ms/step - loss: 4.5451e-04 - val_loss: 0.4372\nEpoch 120/200\n211/211 [==============================] - 1s 2ms/step - loss: 4.3931e-04 - val_loss: 0.4424\nEpoch 121/200\n211/211 [==============================] - 0s 2ms/step - loss: 4.0534e-04 - val_loss: 0.4432\nEpoch 122/200\n211/211 [==============================] - 1s 2ms/step - loss: 3.6864e-04 - val_loss: 0.4578\nEpoch 123/200\n211/211 [==============================] - 1s 2ms/step - loss: 3.4592e-04 - val_loss: 0.4511\nEpoch 124/200\n211/211 [==============================] - 1s 2ms/step - loss: 3.1081e-04 - val_loss: 0.4592\nEpoch 125/200\n211/211 [==============================] - 1s 2ms/step - loss: 2.9489e-04 - val_loss: 0.4600\nEpoch 126/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0263 - val_loss: 0.5286\nEpoch 127/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0363 - val_loss: 0.4671\nEpoch 128/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0086 - val_loss: 0.4047\nEpoch 129/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0021 - val_loss: 0.4324\nEpoch 130/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0024 - val_loss: 0.4251\nEpoch 131/200\n211/211 [==============================] - 1s 3ms/step - loss: 8.4479e-04 - val_loss: 0.4281\nEpoch 132/200\n211/211 [==============================] - 1s 3ms/step - loss: 6.8560e-04 - val_loss: 0.4331\nEpoch 133/200\n211/211 [==============================] - 1s 3ms/step - loss: 5.9677e-04 - val_loss: 0.4378\nEpoch 134/200\n211/211 [==============================] - 1s 2ms/step - loss: 5.4645e-04 - val_loss: 0.4385\nEpoch 135/200\n211/211 [==============================] - 1s 3ms/step - loss: 4.9933e-04 - val_loss: 0.4420\nEpoch 136/200\n211/211 [==============================] - 1s 2ms/step - loss: 4.6492e-04 - val_loss: 0.4472\nEpoch 137/200\n211/211 [==============================] - 1s 2ms/step - loss: 4.4012e-04 - val_loss: 0.4496\nEpoch 138/200\n211/211 [==============================] - 1s 2ms/step - loss: 3.9221e-04 - val_loss: 0.4513\nEpoch 139/200\n211/211 [==============================] - 1s 3ms/step - loss: 3.7403e-04 - val_loss: 0.4540\nEpoch 140/200\n211/211 [==============================] - 1s 2ms/step - loss: 3.5201e-04 - val_loss: 0.4576\nEpoch 141/200\n211/211 [==============================] - 1s 3ms/step - loss: 3.2356e-04 - val_loss: 0.4619\nEpoch 142/200\n211/211 [==============================] - 0s 2ms/step - loss: 2.9727e-04 - val_loss: 0.4630\nEpoch 143/200\n211/211 [==============================] - 0s 2ms/step - loss: 2.8358e-04 - val_loss: 0.4675\nEpoch 144/200\n211/211 [==============================] - 1s 2ms/step - loss: 2.6909e-04 - val_loss: 0.4700\nEpoch 145/200\n211/211 [==============================] - 0s 2ms/step - loss: 2.5064e-04 - val_loss: 0.4763\nEpoch 146/200\n211/211 [==============================] - 1s 3ms/step - loss: 2.3897e-04 - val_loss: 0.4778\nEpoch 147/200\n211/211 [==============================] - 1s 2ms/step - loss: 2.2204e-04 - val_loss: 0.4794\nEpoch 148/200\n211/211 [==============================] - 1s 3ms/step - loss: 2.0544e-04 - val_loss: 0.4828\nEpoch 149/200\n211/211 [==============================] - 0s 2ms/step - loss: 1.9496e-04 - val_loss: 0.4903\nEpoch 150/200\n211/211 [==============================] - 0s 2ms/step - loss: 1.8437e-04 - val_loss: 0.4912\nEpoch 151/200\n211/211 [==============================] - 1s 3ms/step - loss: 1.6883e-04 - val_loss: 0.4955\nEpoch 152/200\n211/211 [==============================] - 1s 2ms/step - loss: 1.6207e-04 - val_loss: 0.5008\nEpoch 153/200\n211/211 [==============================] - 1s 3ms/step - loss: 1.5173e-04 - val_loss: 0.4973\nEpoch 154/200\n211/211 [==============================] - 1s 2ms/step - loss: 1.4377e-04 - val_loss: 0.5047\nEpoch 155/200\n211/211 [==============================] - 1s 3ms/step - loss: 1.2799e-04 - val_loss: 0.5051\nEpoch 156/200\n211/211 [==============================] - 1s 3ms/step - loss: 1.2164e-04 - val_loss: 0.5153\nEpoch 157/200\n211/211 [==============================] - 1s 3ms/step - loss: 1.1658e-04 - val_loss: 0.5159\nEpoch 158/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0394 - val_loss: 0.4587\nEpoch 159/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0216 - val_loss: 0.4627\nEpoch 160/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0035 - val_loss: 0.4869\nEpoch 161/200\n211/211 [==============================] - 1s 2ms/step - loss: 9.8929e-04 - val_loss: 0.4826\nEpoch 162/200\n211/211 [==============================] - 0s 2ms/step - loss: 7.3331e-04 - val_loss: 0.4821\nEpoch 163/200\n211/211 [==============================] - 0s 2ms/step - loss: 6.1184e-04 - val_loss: 0.4831\nEpoch 164/200\n211/211 [==============================] - 0s 2ms/step - loss: 5.3623e-04 - val_loss: 0.4817\nEpoch 165/200\n211/211 [==============================] - 0s 2ms/step - loss: 4.8342e-04 - val_loss: 0.4851\nEpoch 166/200\n211/211 [==============================] - 1s 3ms/step - loss: 4.3247e-04 - val_loss: 0.4843\nEpoch 167/200\n211/211 [==============================] - 1s 2ms/step - loss: 3.8050e-04 - val_loss: 0.4861\nEpoch 168/200\n211/211 [==============================] - 0s 2ms/step - loss: 3.4754e-04 - val_loss: 0.4863\nEpoch 169/200\n211/211 [==============================] - 1s 2ms/step - loss: 3.0544e-04 - val_loss: 0.4874\nEpoch 170/200\n211/211 [==============================] - 0s 2ms/step - loss: 2.7685e-04 - val_loss: 0.4891\nEpoch 171/200\n211/211 [==============================] - 0s 2ms/step - loss: 2.4993e-04 - val_loss: 0.4916\nEpoch 172/200\n211/211 [==============================] - 1s 3ms/step - loss: 2.3011e-04 - val_loss: 0.4936\nEpoch 173/200\n211/211 [==============================] - 1s 2ms/step - loss: 2.1006e-04 - val_loss: 0.4940\nEpoch 174/200\n211/211 [==============================] - 0s 2ms/step - loss: 1.9584e-04 - val_loss: 0.4960\nEpoch 175/200\n211/211 [==============================] - 0s 2ms/step - loss: 1.7753e-04 - val_loss: 0.4989\nEpoch 176/200\n211/211 [==============================] - 1s 3ms/step - loss: 1.6433e-04 - val_loss: 0.5002\nEpoch 177/200\n211/211 [==============================] - 0s 2ms/step - loss: 1.5484e-04 - val_loss: 0.5016\nEpoch 178/200\n211/211 [==============================] - 0s 2ms/step - loss: 1.4369e-04 - val_loss: 0.5032\nEpoch 179/200\n211/211 [==============================] - 0s 2ms/step - loss: 1.3331e-04 - val_loss: 0.5073\nEpoch 180/200\n211/211 [==============================] - 1s 3ms/step - loss: 1.2484e-04 - val_loss: 0.5111\nEpoch 181/200\n211/211 [==============================] - 0s 2ms/step - loss: 1.1884e-04 - val_loss: 0.5117\nEpoch 182/200\n211/211 [==============================] - 1s 3ms/step - loss: 1.0946e-04 - val_loss: 0.5136\nEpoch 183/200\n211/211 [==============================] - 0s 2ms/step - loss: 1.0306e-04 - val_loss: 0.5152\nEpoch 184/200\n211/211 [==============================] - 1s 2ms/step - loss: 9.4622e-05 - val_loss: 0.5191\nEpoch 185/200\n211/211 [==============================] - 1s 2ms/step - loss: 8.9371e-05 - val_loss: 0.5213\nEpoch 186/200\n211/211 [==============================] - 0s 2ms/step - loss: 8.2461e-05 - val_loss: 0.5233\nEpoch 187/200\n211/211 [==============================] - 1s 3ms/step - loss: 7.5861e-05 - val_loss: 0.5277\nEpoch 188/200\n211/211 [==============================] - 0s 2ms/step - loss: 7.3355e-05 - val_loss: 0.5280\nEpoch 189/200\n211/211 [==============================] - 0s 2ms/step - loss: 6.6227e-05 - val_loss: 0.5339\nEpoch 190/200\n211/211 [==============================] - 1s 3ms/step - loss: 6.2678e-05 - val_loss: 0.5339\nEpoch 191/200\n211/211 [==============================] - 0s 2ms/step - loss: 5.8582e-05 - val_loss: 0.5399\nEpoch 192/200\n211/211 [==============================] - 0s 2ms/step - loss: 5.4810e-05 - val_loss: 0.5421\nEpoch 193/200\n211/211 [==============================] - 0s 2ms/step - loss: 5.1308e-05 - val_loss: 0.5494\nEpoch 194/200\n211/211 [==============================] - 0s 2ms/step - loss: 4.6795e-05 - val_loss: 0.5481\nEpoch 195/200\n211/211 [==============================] - 0s 2ms/step - loss: 4.2376e-05 - val_loss: 0.5535\nEpoch 196/200\n211/211 [==============================] - 0s 2ms/step - loss: 3.9998e-05 - val_loss: 0.5621\nEpoch 197/200\n211/211 [==============================] - 0s 2ms/step - loss: 3.6096e-05 - val_loss: 0.5589\nEpoch 198/200\n211/211 [==============================] - 0s 2ms/step - loss: 3.3187e-05 - val_loss: 0.5597\nEpoch 199/200\n211/211 [==============================] - 1s 2ms/step - loss: 3.1276e-05 - val_loss: 0.5688\nEpoch 200/200\n211/211 [==============================] - 1s 3ms/step - loss: 2.8346e-05 - val_loss: 0.5750\n88/88 [==============================] - 0s 1ms/step\nKeras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n...layers\n......dense\n.........vars\n............0\n............1\n......dense_1\n.........vars\n............0\n............1\n......dense_2\n.........vars\n............0\n............1\n......normalization\n.........vars\n............0\n............1\n............2\n...metrics\n......mean\n.........vars\n............0\n............1\n...optimizer\n......vars\n.........0\n.........1\n.........10\n.........11\n.........12\n.........2\n.........3\n.........4\n.........5\n.........6\n.........7\n.........8\n.........9\n...vars\nKeras model archive saving:\nFile Name                                             Modified             Size\nconfig.json                                    2022-12-10 06:48:38         2413\nvariables.h5                                   2022-12-10 06:48:38       135656\nmetadata.json                                  2022-12-10 06:48:38           64\nKeras model archive loading:\nFile Name                                             Modified             Size\nconfig.json                                    2022-12-10 06:48:38         2413\nvariables.h5                                   2022-12-10 06:48:38       135656\nmetadata.json                                  2022-12-10 06:48:38           64\nEpoch 1/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.2967 - val_loss: 0.1675\nEpoch 2/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1748 - val_loss: 0.1578\nEpoch 3/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1671 - val_loss: 0.1520\nEpoch 4/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.1597 - val_loss: 0.1562\nEpoch 5/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1543 - val_loss: 0.1526\nEpoch 6/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.1501 - val_loss: 0.1518\nEpoch 7/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1457 - val_loss: 0.1549\nEpoch 8/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.1429 - val_loss: 0.1515\nEpoch 9/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1385 - val_loss: 0.1531\nEpoch 10/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1375 - val_loss: 0.1537\nEpoch 11/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1321 - val_loss: 0.1506\nEpoch 12/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1287 - val_loss: 0.1537\nEpoch 13/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.1251 - val_loss: 0.1540\nEpoch 14/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.1209 - val_loss: 0.1555\nEpoch 15/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1181 - val_loss: 0.1548\nEpoch 16/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1146 - val_loss: 0.1583\nEpoch 17/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1108 - val_loss: 0.1580\nEpoch 18/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1082 - val_loss: 0.1603\nEpoch 19/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1040 - val_loss: 0.1608\nEpoch 20/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1002 - val_loss: 0.1607\nEpoch 21/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0981 - val_loss: 0.1694\nEpoch 22/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0934 - val_loss: 0.1730\nEpoch 23/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0898 - val_loss: 0.1727\nEpoch 24/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0866 - val_loss: 0.1727\nEpoch 25/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0840 - val_loss: 0.1729\nEpoch 26/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0800 - val_loss: 0.1775\nEpoch 27/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0778 - val_loss: 0.1840\nEpoch 28/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0743 - val_loss: 0.1807\nEpoch 29/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0694 - val_loss: 0.1889\nEpoch 30/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0682 - val_loss: 0.1866\nEpoch 31/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0649 - val_loss: 0.1899\nEpoch 32/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0627 - val_loss: 0.1944\nEpoch 33/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0600 - val_loss: 0.1981\nEpoch 34/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0577 - val_loss: 0.2078\nEpoch 35/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0556 - val_loss: 0.2080\nEpoch 36/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0545 - val_loss: 0.2066\nEpoch 37/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0521 - val_loss: 0.2134\nEpoch 38/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0498 - val_loss: 0.2132\nEpoch 39/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0461 - val_loss: 0.2191\nEpoch 40/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0444 - val_loss: 0.2219\nEpoch 41/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0429 - val_loss: 0.2237\nEpoch 42/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0420 - val_loss: 0.2252\nEpoch 43/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0385 - val_loss: 0.2280\nEpoch 44/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0363 - val_loss: 0.2372\nEpoch 45/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0371 - val_loss: 0.2355\nEpoch 46/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0354 - val_loss: 0.2420\nEpoch 47/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0320 - val_loss: 0.2410\nEpoch 48/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0302 - val_loss: 0.2437\nEpoch 49/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0292 - val_loss: 0.2510\nEpoch 50/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0286 - val_loss: 0.2505\nEpoch 51/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0264 - val_loss: 0.2525\nEpoch 52/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0240 - val_loss: 0.2591\nEpoch 53/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0239 - val_loss: 0.2644\nEpoch 54/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0217 - val_loss: 0.2648\nEpoch 55/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0264 - val_loss: 0.2880\nEpoch 56/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0207 - val_loss: 0.2814\nEpoch 57/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0203 - val_loss: 0.2818\nEpoch 58/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0191 - val_loss: 0.2809\nEpoch 59/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0176 - val_loss: 0.2945\nEpoch 60/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0169 - val_loss: 0.2831\nEpoch 61/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0152 - val_loss: 0.2868\nEpoch 62/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0144 - val_loss: 0.2945\nEpoch 63/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0161 - val_loss: 0.3051\nEpoch 64/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0160 - val_loss: 0.3005\nEpoch 65/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0120 - val_loss: 0.3058\nEpoch 66/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0113 - val_loss: 0.3086\nEpoch 67/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0109 - val_loss: 0.3071\nEpoch 68/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0127 - val_loss: 0.3044\nEpoch 69/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0153 - val_loss: 0.3128\nEpoch 70/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0099 - val_loss: 0.3091\nEpoch 71/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0084 - val_loss: 0.3181\nEpoch 72/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0097 - val_loss: 0.3204\nEpoch 73/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0076 - val_loss: 0.3222\nEpoch 74/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0072 - val_loss: 0.3315\nEpoch 75/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0139 - val_loss: 0.3387\nEpoch 76/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0106 - val_loss: 0.3337\nEpoch 77/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0080 - val_loss: 0.3336\nEpoch 78/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0058 - val_loss: 0.3354\nEpoch 79/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0057 - val_loss: 0.3367\nEpoch 80/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0061 - val_loss: 0.3505\nEpoch 81/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0060 - val_loss: 0.3570\nEpoch 82/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0054 - val_loss: 0.3721\nEpoch 83/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0113 - val_loss: 0.3773\nEpoch 84/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0119 - val_loss: 0.3589\nEpoch 85/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0048 - val_loss: 0.3580\nEpoch 86/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.3641\nEpoch 87/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 0.3667\nEpoch 88/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0033 - val_loss: 0.3734\nEpoch 89/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0036 - val_loss: 0.3745\nEpoch 90/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0033 - val_loss: 0.3877\nEpoch 91/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0055 - val_loss: 0.3711\nEpoch 92/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0049 - val_loss: 0.3922\nEpoch 93/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0056 - val_loss: 0.3758\nEpoch 94/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0025 - val_loss: 0.3830\nEpoch 95/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0026 - val_loss: 0.3864\nEpoch 96/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0026 - val_loss: 0.3778\nEpoch 97/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0035 - val_loss: 0.4089\nEpoch 98/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0208 - val_loss: 0.4158\nEpoch 99/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0142 - val_loss: 0.4013\nEpoch 100/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0050 - val_loss: 0.3859\nEpoch 101/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0022 - val_loss: 0.3860\nEpoch 102/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0019 - val_loss: 0.3902\nEpoch 103/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0019 - val_loss: 0.3896\nEpoch 104/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.3948\nEpoch 105/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.3984\nEpoch 106/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.4039\nEpoch 107/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.4088\nEpoch 108/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.4138\nEpoch 109/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.4162\nEpoch 110/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0015 - val_loss: 0.4252\nEpoch 111/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0025 - val_loss: 0.4144\nEpoch 112/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.4200\nEpoch 113/200\n211/211 [==============================] - 0s 2ms/step - loss: 9.5707e-04 - val_loss: 0.4241\nEpoch 114/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.4239\nEpoch 115/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0045 - val_loss: 0.4225\nEpoch 116/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0252 - val_loss: 0.4030\nEpoch 117/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0053 - val_loss: 0.4057\nEpoch 118/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0016 - val_loss: 0.4050\nEpoch 119/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0010 - val_loss: 0.4067\nEpoch 120/200\n211/211 [==============================] - 1s 2ms/step - loss: 8.6895e-04 - val_loss: 0.4108\nEpoch 121/200\n211/211 [==============================] - 0s 2ms/step - loss: 7.7354e-04 - val_loss: 0.4124\nEpoch 122/200\n211/211 [==============================] - 0s 2ms/step - loss: 6.6978e-04 - val_loss: 0.4162\nEpoch 123/200\n211/211 [==============================] - 1s 2ms/step - loss: 6.2471e-04 - val_loss: 0.4167\nEpoch 124/200\n211/211 [==============================] - 0s 2ms/step - loss: 5.7294e-04 - val_loss: 0.4207\nEpoch 125/200\n211/211 [==============================] - 0s 2ms/step - loss: 5.1842e-04 - val_loss: 0.4263\nEpoch 126/200\n211/211 [==============================] - 0s 2ms/step - loss: 4.9121e-04 - val_loss: 0.4301\nEpoch 127/200\n211/211 [==============================] - 0s 2ms/step - loss: 4.9698e-04 - val_loss: 0.4314\nEpoch 128/200\n211/211 [==============================] - 0s 2ms/step - loss: 4.2070e-04 - val_loss: 0.4351\nEpoch 129/200\n211/211 [==============================] - 0s 2ms/step - loss: 4.2143e-04 - val_loss: 0.4397\nEpoch 130/200\n211/211 [==============================] - 0s 2ms/step - loss: 4.3020e-04 - val_loss: 0.4457\nEpoch 131/200\n211/211 [==============================] - 0s 2ms/step - loss: 3.4361e-04 - val_loss: 0.4478\nEpoch 132/200\n211/211 [==============================] - 0s 2ms/step - loss: 3.3069e-04 - val_loss: 0.4497\nEpoch 133/200\n211/211 [==============================] - 1s 3ms/step - loss: 3.4274e-04 - val_loss: 0.4568\nEpoch 134/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0055 - val_loss: 0.5006\nEpoch 135/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0401 - val_loss: 0.4364\nEpoch 136/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0070 - val_loss: 0.4267\nEpoch 137/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0026 - val_loss: 0.4303\nEpoch 138/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0013 - val_loss: 0.4327\nEpoch 139/200\n211/211 [==============================] - 1s 3ms/step - loss: 7.8552e-04 - val_loss: 0.4338\nEpoch 140/200\n211/211 [==============================] - 0s 2ms/step - loss: 6.5059e-04 - val_loss: 0.4372\nEpoch 141/200\n211/211 [==============================] - 0s 2ms/step - loss: 5.7896e-04 - val_loss: 0.4394\nEpoch 142/200\n211/211 [==============================] - 0s 2ms/step - loss: 5.2045e-04 - val_loss: 0.4423\nEpoch 143/200\n211/211 [==============================] - 1s 3ms/step - loss: 4.6402e-04 - val_loss: 0.4454\nEpoch 144/200\n211/211 [==============================] - 0s 2ms/step - loss: 4.2013e-04 - val_loss: 0.4482\nEpoch 145/200\n211/211 [==============================] - 0s 2ms/step - loss: 3.9136e-04 - val_loss: 0.4501\nEpoch 146/200\n211/211 [==============================] - 0s 2ms/step - loss: 3.6460e-04 - val_loss: 0.4536\nEpoch 147/200\n211/211 [==============================] - 1s 3ms/step - loss: 3.3701e-04 - val_loss: 0.4561\nEpoch 148/200\n211/211 [==============================] - 1s 3ms/step - loss: 3.1353e-04 - val_loss: 0.4600\nEpoch 149/200\n211/211 [==============================] - 0s 2ms/step - loss: 2.9309e-04 - val_loss: 0.4629\nEpoch 150/200\n211/211 [==============================] - 0s 2ms/step - loss: 2.8137e-04 - val_loss: 0.4658\nEpoch 151/200\n211/211 [==============================] - 0s 2ms/step - loss: 2.6195e-04 - val_loss: 0.4671\nEpoch 152/200\n211/211 [==============================] - 1s 3ms/step - loss: 2.3949e-04 - val_loss: 0.4725\nEpoch 153/200\n211/211 [==============================] - 0s 2ms/step - loss: 2.2065e-04 - val_loss: 0.4789\nEpoch 154/200\n211/211 [==============================] - 1s 3ms/step - loss: 2.1392e-04 - val_loss: 0.4785\nEpoch 155/200\n211/211 [==============================] - 0s 2ms/step - loss: 2.0551e-04 - val_loss: 0.4834\nEpoch 156/200\n211/211 [==============================] - 0s 2ms/step - loss: 1.7931e-04 - val_loss: 0.4859\nEpoch 157/200\n211/211 [==============================] - 0s 2ms/step - loss: 1.8011e-04 - val_loss: 0.4895\nEpoch 158/200\n211/211 [==============================] - 0s 2ms/step - loss: 1.5806e-04 - val_loss: 0.4929\nEpoch 159/200\n211/211 [==============================] - 1s 2ms/step - loss: 1.5092e-04 - val_loss: 0.5031\nEpoch 160/200\n211/211 [==============================] - 1s 3ms/step - loss: 1.6059e-04 - val_loss: 0.5052\nEpoch 161/200\n211/211 [==============================] - 1s 2ms/step - loss: 1.4672e-04 - val_loss: 0.5059\nEpoch 162/200\n211/211 [==============================] - 1s 2ms/step - loss: 1.4993e-04 - val_loss: 0.5131\nEpoch 163/200\n211/211 [==============================] - 1s 3ms/step - loss: 1.3824e-04 - val_loss: 0.5256\nEpoch 164/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0364 - val_loss: 0.4872\nEpoch 165/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0237 - val_loss: 0.4775\nEpoch 166/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0028 - val_loss: 0.4884\nEpoch 167/200\n211/211 [==============================] - 0s 2ms/step - loss: 9.2899e-04 - val_loss: 0.4892\nEpoch 168/200\n211/211 [==============================] - 0s 2ms/step - loss: 6.1096e-04 - val_loss: 0.4943\nEpoch 169/200\n211/211 [==============================] - 0s 2ms/step - loss: 4.4419e-04 - val_loss: 0.4956\nEpoch 170/200\n211/211 [==============================] - 0s 2ms/step - loss: 3.7564e-04 - val_loss: 0.4971\nEpoch 171/200\n211/211 [==============================] - 1s 2ms/step - loss: 3.2766e-04 - val_loss: 0.4991\nEpoch 172/200\n211/211 [==============================] - 1s 3ms/step - loss: 2.9208e-04 - val_loss: 0.5012\nEpoch 173/200\n211/211 [==============================] - 0s 2ms/step - loss: 2.6681e-04 - val_loss: 0.5037\nEpoch 174/200\n211/211 [==============================] - 0s 2ms/step - loss: 2.4263e-04 - val_loss: 0.5057\nEpoch 175/200\n211/211 [==============================] - 0s 2ms/step - loss: 2.2479e-04 - val_loss: 0.5074\nEpoch 176/200\n211/211 [==============================] - 0s 2ms/step - loss: 2.0806e-04 - val_loss: 0.5098\nEpoch 177/200\n211/211 [==============================] - 1s 3ms/step - loss: 1.9176e-04 - val_loss: 0.5118\nEpoch 178/200\n211/211 [==============================] - 0s 2ms/step - loss: 1.8294e-04 - val_loss: 0.5162\nEpoch 179/200\n211/211 [==============================] - 0s 2ms/step - loss: 1.6543e-04 - val_loss: 0.5177\nEpoch 180/200\n211/211 [==============================] - 0s 2ms/step - loss: 1.5688e-04 - val_loss: 0.5200\nEpoch 181/200\n211/211 [==============================] - 1s 2ms/step - loss: 1.4797e-04 - val_loss: 0.5229\nEpoch 182/200\n211/211 [==============================] - 0s 2ms/step - loss: 1.3708e-04 - val_loss: 0.5262\nEpoch 183/200\n211/211 [==============================] - 0s 2ms/step - loss: 1.2974e-04 - val_loss: 0.5296\nEpoch 184/200\n211/211 [==============================] - 0s 2ms/step - loss: 1.2297e-04 - val_loss: 0.5329\nEpoch 185/200\n211/211 [==============================] - 0s 2ms/step - loss: 1.1505e-04 - val_loss: 0.5340\nEpoch 186/200\n211/211 [==============================] - 0s 2ms/step - loss: 1.0748e-04 - val_loss: 0.5377\nEpoch 187/200\n211/211 [==============================] - 0s 2ms/step - loss: 1.0207e-04 - val_loss: 0.5396\nEpoch 188/200\n211/211 [==============================] - 0s 2ms/step - loss: 9.4980e-05 - val_loss: 0.5453\nEpoch 189/200\n211/211 [==============================] - 0s 2ms/step - loss: 9.0585e-05 - val_loss: 0.5477\nEpoch 190/200\n211/211 [==============================] - 0s 2ms/step - loss: 8.3725e-05 - val_loss: 0.5512\nEpoch 191/200\n211/211 [==============================] - 0s 2ms/step - loss: 7.9301e-05 - val_loss: 0.5559\nEpoch 192/200\n211/211 [==============================] - 0s 2ms/step - loss: 7.6358e-05 - val_loss: 0.5568\nEpoch 193/200\n211/211 [==============================] - 0s 2ms/step - loss: 6.9858e-05 - val_loss: 0.5630\nEpoch 194/200\n211/211 [==============================] - 1s 3ms/step - loss: 6.6331e-05 - val_loss: 0.5690\nEpoch 195/200\n211/211 [==============================] - 1s 3ms/step - loss: 6.0678e-05 - val_loss: 0.5709\nEpoch 196/200\n211/211 [==============================] - 0s 2ms/step - loss: 5.7781e-05 - val_loss: 0.5746\nEpoch 197/200\n211/211 [==============================] - 1s 3ms/step - loss: 5.5234e-05 - val_loss: 0.5795\nEpoch 198/200\n211/211 [==============================] - 1s 3ms/step - loss: 4.9727e-05 - val_loss: 0.5841\nEpoch 199/200\n211/211 [==============================] - 1s 3ms/step - loss: 5.0800e-05 - val_loss: 0.5831\nEpoch 200/200\n211/211 [==============================] - 1s 3ms/step - loss: 9.9227e-05 - val_loss: 0.6757\n88/88 [==============================] - 0s 1ms/step\n     .. fold 2 trained/predicted\nKeras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n...layers\n......dense\n.........vars\n............0\n............1\n......dense_1\n.........vars\n............0\n............1\n......dense_2\n.........vars\n............0\n............1\n......normalization\n.........vars\n............0\n............1\n............2\n...metrics\n......mean\n.........vars\n............0\n............1\n...optimizer\n......vars\n.........0\n.........1\n.........10\n.........11\n.........12\n.........2\n.........3\n.........4\n.........5\n.........6\n.........7\n.........8\n.........9\n...vars\nKeras model archive saving:\nFile Name                                             Modified             Size\nconfig.json                                    2022-12-10 06:50:21         2413\nvariables.h5                                   2022-12-10 06:50:22       135656\nmetadata.json                                  2022-12-10 06:50:21           64\nKeras model archive loading:\nFile Name                                             Modified             Size\nconfig.json                                    2022-12-10 06:50:20         2413\nvariables.h5                                   2022-12-10 06:50:22       135656\nmetadata.json                                  2022-12-10 06:50:20           64\nEpoch 1/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.2941 - val_loss: 0.1704\nEpoch 2/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.1819 - val_loss: 0.1626\nEpoch 3/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1711 - val_loss: 0.1604\nEpoch 4/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1652 - val_loss: 0.1596\nEpoch 5/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.1597 - val_loss: 0.1582\nEpoch 6/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1559 - val_loss: 0.1606\nEpoch 7/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1529 - val_loss: 0.1563\nEpoch 8/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1478 - val_loss: 0.1560\nEpoch 9/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1452 - val_loss: 0.1576\nEpoch 10/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1394 - val_loss: 0.1600\nEpoch 11/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1376 - val_loss: 0.1586\nEpoch 12/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1350 - val_loss: 0.1624\nEpoch 13/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1313 - val_loss: 0.1610\nEpoch 14/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1268 - val_loss: 0.1590\nEpoch 15/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1219 - val_loss: 0.1622\nEpoch 16/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1200 - val_loss: 0.1647\nEpoch 17/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.1179 - val_loss: 0.1648\nEpoch 18/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1123 - val_loss: 0.1714\nEpoch 19/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1101 - val_loss: 0.1746\nEpoch 20/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.1072 - val_loss: 0.1687\nEpoch 21/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1025 - val_loss: 0.1721\nEpoch 22/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0997 - val_loss: 0.1716\nEpoch 23/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0943 - val_loss: 0.1796\nEpoch 24/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0940 - val_loss: 0.1780\nEpoch 25/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0897 - val_loss: 0.1806\nEpoch 26/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0861 - val_loss: 0.1830\nEpoch 27/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0832 - val_loss: 0.1855\nEpoch 28/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0802 - val_loss: 0.1877\nEpoch 29/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0774 - val_loss: 0.1968\nEpoch 30/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0746 - val_loss: 0.1975\nEpoch 31/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0725 - val_loss: 0.1998\nEpoch 32/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0672 - val_loss: 0.1969\nEpoch 33/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0661 - val_loss: 0.2001\nEpoch 34/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0630 - val_loss: 0.2074\nEpoch 35/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0614 - val_loss: 0.2065\nEpoch 36/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0578 - val_loss: 0.2152\nEpoch 37/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0555 - val_loss: 0.2135\nEpoch 38/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0515 - val_loss: 0.2132\nEpoch 39/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0492 - val_loss: 0.2240\nEpoch 40/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0473 - val_loss: 0.2235\nEpoch 41/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0452 - val_loss: 0.2273\nEpoch 42/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0423 - val_loss: 0.2254\nEpoch 43/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0412 - val_loss: 0.2390\nEpoch 44/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0405 - val_loss: 0.2356\nEpoch 45/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0373 - val_loss: 0.2361\nEpoch 46/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0356 - val_loss: 0.2413\nEpoch 47/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0356 - val_loss: 0.2421\nEpoch 48/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.2500\nEpoch 49/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0315 - val_loss: 0.2542\nEpoch 50/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0292 - val_loss: 0.2617\nEpoch 51/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0271 - val_loss: 0.2615\nEpoch 52/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0255 - val_loss: 0.2693\nEpoch 53/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0261 - val_loss: 0.2731\nEpoch 54/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0223 - val_loss: 0.2727\nEpoch 55/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0224 - val_loss: 0.2731\nEpoch 56/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0200 - val_loss: 0.2877\nEpoch 57/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0209 - val_loss: 0.2853\nEpoch 58/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0191 - val_loss: 0.2976\nEpoch 59/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0175 - val_loss: 0.2974\nEpoch 60/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0170 - val_loss: 0.3036\nEpoch 61/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0162 - val_loss: 0.2917\nEpoch 62/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0158 - val_loss: 0.2929\nEpoch 63/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0161 - val_loss: 0.3245\nEpoch 64/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0154 - val_loss: 0.3095\nEpoch 65/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0123 - val_loss: 0.3209\nEpoch 66/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0109 - val_loss: 0.3340\nEpoch 67/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0138 - val_loss: 0.3273\nEpoch 68/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0103 - val_loss: 0.3357\nEpoch 69/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0084 - val_loss: 0.3314\nEpoch 70/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0089 - val_loss: 0.3412\nEpoch 71/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0087 - val_loss: 0.3492\nEpoch 72/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0083 - val_loss: 0.3535\nEpoch 73/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0094 - val_loss: 0.3505\nEpoch 74/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0166 - val_loss: 0.3799\nEpoch 75/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0154 - val_loss: 0.3641\nEpoch 76/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0119 - val_loss: 0.3539\nEpoch 77/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0062 - val_loss: 0.3606\nEpoch 78/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0059 - val_loss: 0.3670\nEpoch 79/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0051 - val_loss: 0.3697\nEpoch 80/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0049 - val_loss: 0.3728\nEpoch 81/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0048 - val_loss: 0.3810\nEpoch 82/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0043 - val_loss: 0.3852\nEpoch 83/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0044 - val_loss: 0.3865\nEpoch 84/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0044 - val_loss: 0.3974\nEpoch 85/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0048 - val_loss: 0.4056\nEpoch 86/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0042 - val_loss: 0.4092\nEpoch 87/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0032 - val_loss: 0.4081\nEpoch 88/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0032 - val_loss: 0.4141\nEpoch 89/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0037 - val_loss: 0.4188\nEpoch 90/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0206 - val_loss: 0.4104\nEpoch 91/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0135 - val_loss: 0.4323\nEpoch 92/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0049 - val_loss: 0.4144\nEpoch 93/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0027 - val_loss: 0.4244\nEpoch 94/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0024 - val_loss: 0.4260\nEpoch 95/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0024 - val_loss: 0.4334\nEpoch 96/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0020 - val_loss: 0.4335\nEpoch 97/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0018 - val_loss: 0.4374\nEpoch 98/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.4446\nEpoch 99/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0017 - val_loss: 0.4467\nEpoch 100/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0016 - val_loss: 0.4559\nEpoch 101/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0015 - val_loss: 0.4509\nEpoch 102/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0031 - val_loss: 0.4694\nEpoch 103/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0253 - val_loss: 0.4457\nEpoch 104/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0080 - val_loss: 0.4403\nEpoch 105/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0021 - val_loss: 0.4424\nEpoch 106/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0016 - val_loss: 0.4469\nEpoch 107/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.4488\nEpoch 108/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.4510\nEpoch 109/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.4587\nEpoch 110/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.4596\nEpoch 111/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0011 - val_loss: 0.4603\nEpoch 112/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0036 - val_loss: 0.4525\nEpoch 113/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0067 - val_loss: 0.4473\nEpoch 114/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0041 - val_loss: 0.4690\nEpoch 115/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.4647\nEpoch 116/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0011 - val_loss: 0.4658\nEpoch 117/200\n211/211 [==============================] - 1s 3ms/step - loss: 8.9411e-04 - val_loss: 0.4707\nEpoch 118/200\n211/211 [==============================] - 1s 3ms/step - loss: 8.1779e-04 - val_loss: 0.4721\nEpoch 119/200\n211/211 [==============================] - 0s 2ms/step - loss: 7.4388e-04 - val_loss: 0.4748\nEpoch 120/200\n211/211 [==============================] - 0s 2ms/step - loss: 7.2451e-04 - val_loss: 0.4813\nEpoch 121/200\n211/211 [==============================] - 1s 3ms/step - loss: 6.8456e-04 - val_loss: 0.4803\nEpoch 122/200\n211/211 [==============================] - 0s 2ms/step - loss: 6.4978e-04 - val_loss: 0.4842\nEpoch 123/200\n211/211 [==============================] - 1s 2ms/step - loss: 6.0222e-04 - val_loss: 0.4857\nEpoch 124/200\n211/211 [==============================] - 1s 3ms/step - loss: 5.6282e-04 - val_loss: 0.4973\nEpoch 125/200\n211/211 [==============================] - 1s 2ms/step - loss: 6.0383e-04 - val_loss: 0.4975\nEpoch 126/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0516 - val_loss: 0.4461\nEpoch 127/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0129 - val_loss: 0.4758\nEpoch 128/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0037 - val_loss: 0.4715\nEpoch 129/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0014 - val_loss: 0.4742\nEpoch 130/200\n211/211 [==============================] - 0s 2ms/step - loss: 9.4336e-04 - val_loss: 0.4782\nEpoch 131/200\n211/211 [==============================] - 0s 2ms/step - loss: 8.2611e-04 - val_loss: 0.4818\nEpoch 132/200\n211/211 [==============================] - 1s 2ms/step - loss: 7.3604e-04 - val_loss: 0.4845\nEpoch 133/200\n211/211 [==============================] - 1s 2ms/step - loss: 6.8318e-04 - val_loss: 0.4854\nEpoch 134/200\n211/211 [==============================] - 1s 2ms/step - loss: 6.2945e-04 - val_loss: 0.4877\nEpoch 135/200\n211/211 [==============================] - 0s 2ms/step - loss: 5.8343e-04 - val_loss: 0.4930\nEpoch 136/200\n211/211 [==============================] - 0s 2ms/step - loss: 5.3845e-04 - val_loss: 0.4933\nEpoch 137/200\n211/211 [==============================] - 0s 2ms/step - loss: 5.1803e-04 - val_loss: 0.4949\nEpoch 138/200\n211/211 [==============================] - 0s 2ms/step - loss: 4.8079e-04 - val_loss: 0.5000\nEpoch 139/200\n211/211 [==============================] - 1s 2ms/step - loss: 4.5869e-04 - val_loss: 0.5042\nEpoch 140/200\n211/211 [==============================] - 0s 2ms/step - loss: 4.3627e-04 - val_loss: 0.5028\nEpoch 141/200\n211/211 [==============================] - 0s 2ms/step - loss: 4.0149e-04 - val_loss: 0.5058\nEpoch 142/200\n211/211 [==============================] - 0s 2ms/step - loss: 4.0380e-04 - val_loss: 0.5114\nEpoch 143/200\n211/211 [==============================] - 0s 2ms/step - loss: 3.6389e-04 - val_loss: 0.5156\nEpoch 144/200\n211/211 [==============================] - 1s 3ms/step - loss: 3.5546e-04 - val_loss: 0.5155\nEpoch 145/200\n211/211 [==============================] - 0s 2ms/step - loss: 3.2114e-04 - val_loss: 0.5207\nEpoch 146/200\n211/211 [==============================] - 0s 2ms/step - loss: 3.2943e-04 - val_loss: 0.5303\nEpoch 147/200\n211/211 [==============================] - 1s 3ms/step - loss: 3.0394e-04 - val_loss: 0.5309\nEpoch 148/200\n211/211 [==============================] - 1s 3ms/step - loss: 2.8639e-04 - val_loss: 0.5321\nEpoch 149/200\n211/211 [==============================] - 0s 2ms/step - loss: 2.6665e-04 - val_loss: 0.5341\nEpoch 150/200\n211/211 [==============================] - 0s 2ms/step - loss: 2.8754e-04 - val_loss: 0.5326\nEpoch 151/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0255 - val_loss: 0.5652\nEpoch 152/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0249 - val_loss: 0.4903\nEpoch 153/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0020 - val_loss: 0.4983\nEpoch 154/200\n211/211 [==============================] - 1s 2ms/step - loss: 9.1631e-04 - val_loss: 0.5008\nEpoch 155/200\n211/211 [==============================] - 1s 3ms/step - loss: 6.6581e-04 - val_loss: 0.5054\nEpoch 156/200\n211/211 [==============================] - 1s 2ms/step - loss: 5.5705e-04 - val_loss: 0.5087\nEpoch 157/200\n211/211 [==============================] - 1s 2ms/step - loss: 4.9933e-04 - val_loss: 0.5099\nEpoch 158/200\n211/211 [==============================] - 1s 2ms/step - loss: 4.5175e-04 - val_loss: 0.5158\nEpoch 159/200\n211/211 [==============================] - 1s 3ms/step - loss: 4.1951e-04 - val_loss: 0.5163\nEpoch 160/200\n211/211 [==============================] - 0s 2ms/step - loss: 3.9097e-04 - val_loss: 0.5185\nEpoch 161/200\n211/211 [==============================] - 1s 2ms/step - loss: 3.5901e-04 - val_loss: 0.5216\nEpoch 162/200\n211/211 [==============================] - 1s 2ms/step - loss: 3.4045e-04 - val_loss: 0.5235\nEpoch 163/200\n211/211 [==============================] - 0s 2ms/step - loss: 3.2255e-04 - val_loss: 0.5269\nEpoch 164/200\n211/211 [==============================] - 1s 2ms/step - loss: 3.0384e-04 - val_loss: 0.5278\nEpoch 165/200\n211/211 [==============================] - 1s 2ms/step - loss: 2.8696e-04 - val_loss: 0.5324\nEpoch 166/200\n211/211 [==============================] - 1s 3ms/step - loss: 2.6743e-04 - val_loss: 0.5341\nEpoch 167/200\n211/211 [==============================] - 1s 3ms/step - loss: 2.5429e-04 - val_loss: 0.5388\nEpoch 168/200\n211/211 [==============================] - 1s 2ms/step - loss: 2.4882e-04 - val_loss: 0.5411\nEpoch 169/200\n211/211 [==============================] - 1s 3ms/step - loss: 2.2562e-04 - val_loss: 0.5447\nEpoch 170/200\n211/211 [==============================] - 0s 2ms/step - loss: 2.1248e-04 - val_loss: 0.5442\nEpoch 171/200\n211/211 [==============================] - 1s 2ms/step - loss: 2.0394e-04 - val_loss: 0.5494\nEpoch 172/200\n211/211 [==============================] - 1s 3ms/step - loss: 1.8963e-04 - val_loss: 0.5495\nEpoch 173/200\n211/211 [==============================] - 1s 2ms/step - loss: 1.8245e-04 - val_loss: 0.5542\nEpoch 174/200\n211/211 [==============================] - 1s 2ms/step - loss: 1.7151e-04 - val_loss: 0.5560\nEpoch 175/200\n211/211 [==============================] - 1s 3ms/step - loss: 1.6254e-04 - val_loss: 0.5610\nEpoch 176/200\n211/211 [==============================] - 1s 2ms/step - loss: 1.5656e-04 - val_loss: 0.5648\nEpoch 177/200\n211/211 [==============================] - 1s 3ms/step - loss: 1.4379e-04 - val_loss: 0.5688\nEpoch 178/200\n211/211 [==============================] - 1s 3ms/step - loss: 1.3012e-04 - val_loss: 0.5697\nEpoch 179/200\n211/211 [==============================] - 1s 3ms/step - loss: 1.2737e-04 - val_loss: 0.5742\nEpoch 180/200\n211/211 [==============================] - 0s 2ms/step - loss: 1.2028e-04 - val_loss: 0.5786\nEpoch 181/200\n211/211 [==============================] - 1s 3ms/step - loss: 1.1192e-04 - val_loss: 0.5809\nEpoch 182/200\n211/211 [==============================] - 1s 3ms/step - loss: 1.0409e-04 - val_loss: 0.5864\nEpoch 183/200\n211/211 [==============================] - 0s 2ms/step - loss: 1.0118e-04 - val_loss: 0.5918\nEpoch 184/200\n211/211 [==============================] - 1s 2ms/step - loss: 9.4505e-05 - val_loss: 0.5863\nEpoch 185/200\n211/211 [==============================] - 0s 2ms/step - loss: 8.9816e-05 - val_loss: 0.6011\nEpoch 186/200\n211/211 [==============================] - 1s 3ms/step - loss: 9.0446e-05 - val_loss: 0.6011\nEpoch 187/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0535 - val_loss: 0.5315\nEpoch 188/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0160 - val_loss: 0.5209\nEpoch 189/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0077 - val_loss: 0.5731\nEpoch 190/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0032 - val_loss: 0.5686\nEpoch 191/200\n211/211 [==============================] - 1s 2ms/step - loss: 8.5902e-04 - val_loss: 0.5731\nEpoch 192/200\n211/211 [==============================] - 1s 3ms/step - loss: 4.7050e-04 - val_loss: 0.5761\nEpoch 193/200\n211/211 [==============================] - 1s 2ms/step - loss: 3.8977e-04 - val_loss: 0.5774\nEpoch 194/200\n211/211 [==============================] - 1s 3ms/step - loss: 3.2898e-04 - val_loss: 0.5783\nEpoch 195/200\n211/211 [==============================] - 1s 3ms/step - loss: 2.8983e-04 - val_loss: 0.5780\nEpoch 196/200\n211/211 [==============================] - 1s 3ms/step - loss: 2.5666e-04 - val_loss: 0.5789\nEpoch 197/200\n211/211 [==============================] - 1s 2ms/step - loss: 2.2933e-04 - val_loss: 0.5798\nEpoch 198/200\n211/211 [==============================] - 1s 2ms/step - loss: 2.1275e-04 - val_loss: 0.5817\nEpoch 199/200\n211/211 [==============================] - 1s 2ms/step - loss: 1.9419e-04 - val_loss: 0.5830\nEpoch 200/200\n211/211 [==============================] - 1s 3ms/step - loss: 1.7931e-04 - val_loss: 0.5829\n88/88 [==============================] - 0s 1ms/step\nKeras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n...layers\n......dense\n.........vars\n............0\n............1\n......dense_1\n.........vars\n............0\n............1\n......dense_2\n.........vars\n............0\n............1\n......normalization\n.........vars\n............0\n............1\n............2\n...metrics\n......mean\n.........vars\n............0\n............1\n...optimizer\n......vars\n.........0\n.........1\n.........10\n.........11\n.........12\n.........2\n.........3\n.........4\n.........5\n.........6\n.........7\n.........8\n.........9\n...vars\nKeras model archive saving:\nFile Name                                             Modified             Size\nconfig.json                                    2022-12-10 06:52:08         2413\nvariables.h5                                   2022-12-10 06:52:08       135656\nmetadata.json                                  2022-12-10 06:52:08           64\nKeras model archive loading:\nFile Name                                             Modified             Size\nconfig.json                                    2022-12-10 06:52:08         2413\nvariables.h5                                   2022-12-10 06:52:08       135656\nmetadata.json                                  2022-12-10 06:52:08           64\nEpoch 1/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.3276 - val_loss: 0.1661\nEpoch 2/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1827 - val_loss: 0.1586\nEpoch 3/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1745 - val_loss: 0.1553\nEpoch 4/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1699 - val_loss: 0.1508\nEpoch 5/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.1641 - val_loss: 0.1524\nEpoch 6/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1587 - val_loss: 0.1563\nEpoch 7/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1548 - val_loss: 0.1507\nEpoch 8/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.1507 - val_loss: 0.1532\nEpoch 9/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.1476 - val_loss: 0.1514\nEpoch 10/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1414 - val_loss: 0.1526\nEpoch 11/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.1387 - val_loss: 0.1564\nEpoch 12/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.1353 - val_loss: 0.1537\nEpoch 13/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.1321 - val_loss: 0.1567\nEpoch 14/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1269 - val_loss: 0.1569\nEpoch 15/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.1249 - val_loss: 0.1664\nEpoch 16/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1211 - val_loss: 0.1614\nEpoch 17/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1175 - val_loss: 0.1595\nEpoch 18/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.1120 - val_loss: 0.1616\nEpoch 19/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1112 - val_loss: 0.1671\nEpoch 20/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.1051 - val_loss: 0.1690\nEpoch 21/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.1023 - val_loss: 0.1700\nEpoch 22/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.1002 - val_loss: 0.1725\nEpoch 23/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0950 - val_loss: 0.1742\nEpoch 24/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0919 - val_loss: 0.1832\nEpoch 25/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0887 - val_loss: 0.1821\nEpoch 26/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0852 - val_loss: 0.1821\nEpoch 27/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0819 - val_loss: 0.1790\nEpoch 28/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0786 - val_loss: 0.1898\nEpoch 29/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0762 - val_loss: 0.1961\nEpoch 30/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0723 - val_loss: 0.1893\nEpoch 31/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0695 - val_loss: 0.1983\nEpoch 32/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0673 - val_loss: 0.2062\nEpoch 33/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0651 - val_loss: 0.2022\nEpoch 34/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0623 - val_loss: 0.2085\nEpoch 35/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0589 - val_loss: 0.2133\nEpoch 36/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0563 - val_loss: 0.2187\nEpoch 37/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0543 - val_loss: 0.2186\nEpoch 38/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0525 - val_loss: 0.2270\nEpoch 39/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0504 - val_loss: 0.2261\nEpoch 40/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0470 - val_loss: 0.2357\nEpoch 41/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0446 - val_loss: 0.2380\nEpoch 42/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0428 - val_loss: 0.2398\nEpoch 43/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0413 - val_loss: 0.2417\nEpoch 44/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0392 - val_loss: 0.2449\nEpoch 45/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0362 - val_loss: 0.2541\nEpoch 46/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0354 - val_loss: 0.2745\nEpoch 47/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0336 - val_loss: 0.2530\nEpoch 48/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0313 - val_loss: 0.2684\nEpoch 49/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0287 - val_loss: 0.2696\nEpoch 50/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0307 - val_loss: 0.2687\nEpoch 51/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0266 - val_loss: 0.2748\nEpoch 52/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0243 - val_loss: 0.2805\nEpoch 53/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0222 - val_loss: 0.2918\nEpoch 54/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0239 - val_loss: 0.2902\nEpoch 55/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0217 - val_loss: 0.3005\nEpoch 56/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0210 - val_loss: 0.3159\nEpoch 57/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0186 - val_loss: 0.3203\nEpoch 58/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0169 - val_loss: 0.3218\nEpoch 59/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0169 - val_loss: 0.3238\nEpoch 60/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0150 - val_loss: 0.3249\nEpoch 61/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0136 - val_loss: 0.3384\nEpoch 62/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0210 - val_loss: 0.3788\nEpoch 63/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0211 - val_loss: 0.3478\nEpoch 64/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0122 - val_loss: 0.3486\nEpoch 65/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0111 - val_loss: 0.3508\nEpoch 66/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0092 - val_loss: 0.3503\nEpoch 67/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0085 - val_loss: 0.3592\nEpoch 68/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0086 - val_loss: 0.3600\nEpoch 69/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0077 - val_loss: 0.3585\nEpoch 70/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0116 - val_loss: 0.4019\nEpoch 71/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0121 - val_loss: 0.3750\nEpoch 72/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0072 - val_loss: 0.3952\nEpoch 73/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0083 - val_loss: 0.3960\nEpoch 74/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0074 - val_loss: 0.3919\nEpoch 75/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0061 - val_loss: 0.3899\nEpoch 76/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0047 - val_loss: 0.3880\nEpoch 77/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0051 - val_loss: 0.4032\nEpoch 78/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0040 - val_loss: 0.4056\nEpoch 79/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0048 - val_loss: 0.4019\nEpoch 80/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0051 - val_loss: 0.3979\nEpoch 81/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0029 - val_loss: 0.4027\nEpoch 82/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.4391\nEpoch 83/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0145 - val_loss: 0.4057\nEpoch 84/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0086 - val_loss: 0.4131\nEpoch 85/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0102 - val_loss: 0.4348\nEpoch 86/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0084 - val_loss: 0.4104\nEpoch 87/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0024 - val_loss: 0.4187\nEpoch 88/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0017 - val_loss: 0.4235\nEpoch 89/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0016 - val_loss: 0.4237\nEpoch 90/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0014 - val_loss: 0.4275\nEpoch 91/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0013 - val_loss: 0.4332\nEpoch 92/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0013 - val_loss: 0.4374\nEpoch 93/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0012 - val_loss: 0.4399\nEpoch 94/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0011 - val_loss: 0.4429\nEpoch 95/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0010 - val_loss: 0.4536\nEpoch 96/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0015 - val_loss: 0.5048\nEpoch 97/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0207 - val_loss: 0.4774\nEpoch 98/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0084 - val_loss: 0.4700\nEpoch 99/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0062 - val_loss: 0.4614\nEpoch 100/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0015 - val_loss: 0.4595\nEpoch 101/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0011 - val_loss: 0.4617\nEpoch 102/200\n211/211 [==============================] - 1s 2ms/step - loss: 9.0632e-04 - val_loss: 0.4650\nEpoch 103/200\n211/211 [==============================] - 1s 2ms/step - loss: 8.3941e-04 - val_loss: 0.4668\nEpoch 104/200\n211/211 [==============================] - 1s 2ms/step - loss: 7.6183e-04 - val_loss: 0.4719\nEpoch 105/200\n211/211 [==============================] - 1s 2ms/step - loss: 7.0480e-04 - val_loss: 0.4733\nEpoch 106/200\n211/211 [==============================] - 1s 2ms/step - loss: 6.5221e-04 - val_loss: 0.4771\nEpoch 107/200\n211/211 [==============================] - 1s 2ms/step - loss: 6.4286e-04 - val_loss: 0.4822\nEpoch 108/200\n211/211 [==============================] - 1s 3ms/step - loss: 5.6489e-04 - val_loss: 0.4834\nEpoch 109/200\n211/211 [==============================] - 1s 2ms/step - loss: 5.4325e-04 - val_loss: 0.4845\nEpoch 110/200\n211/211 [==============================] - 1s 3ms/step - loss: 5.4806e-04 - val_loss: 0.4883\nEpoch 111/200\n211/211 [==============================] - 1s 2ms/step - loss: 4.8614e-04 - val_loss: 0.4947\nEpoch 112/200\n211/211 [==============================] - 1s 2ms/step - loss: 4.4046e-04 - val_loss: 0.4962\nEpoch 113/200\n211/211 [==============================] - 1s 2ms/step - loss: 4.2386e-04 - val_loss: 0.5020\nEpoch 114/200\n211/211 [==============================] - 1s 3ms/step - loss: 3.8742e-04 - val_loss: 0.5079\nEpoch 115/200\n211/211 [==============================] - 1s 3ms/step - loss: 3.7218e-04 - val_loss: 0.5143\nEpoch 116/200\n211/211 [==============================] - 1s 3ms/step - loss: 3.3841e-04 - val_loss: 0.5142\nEpoch 117/200\n211/211 [==============================] - 0s 2ms/step - loss: 3.3983e-04 - val_loss: 0.5180\nEpoch 118/200\n211/211 [==============================] - 1s 2ms/step - loss: 3.3128e-04 - val_loss: 0.5188\nEpoch 119/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0579 - val_loss: 0.5141\nEpoch 120/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0113 - val_loss: 0.4923\nEpoch 121/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0050 - val_loss: 0.4821\nEpoch 122/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.4839\nEpoch 123/200\n211/211 [==============================] - 1s 2ms/step - loss: 8.7357e-04 - val_loss: 0.4880\nEpoch 124/200\n211/211 [==============================] - 1s 3ms/step - loss: 6.8914e-04 - val_loss: 0.4919\nEpoch 125/200\n211/211 [==============================] - 1s 3ms/step - loss: 6.1930e-04 - val_loss: 0.4961\nEpoch 126/200\n211/211 [==============================] - 1s 2ms/step - loss: 5.5638e-04 - val_loss: 0.4975\nEpoch 127/200\n211/211 [==============================] - 1s 2ms/step - loss: 5.1092e-04 - val_loss: 0.5010\nEpoch 128/200\n211/211 [==============================] - 1s 2ms/step - loss: 4.6733e-04 - val_loss: 0.5031\nEpoch 129/200\n211/211 [==============================] - 1s 3ms/step - loss: 4.1936e-04 - val_loss: 0.5060\nEpoch 130/200\n211/211 [==============================] - 1s 2ms/step - loss: 4.0086e-04 - val_loss: 0.5091\nEpoch 131/200\n211/211 [==============================] - 1s 2ms/step - loss: 3.7050e-04 - val_loss: 0.5106\nEpoch 132/200\n211/211 [==============================] - 1s 2ms/step - loss: 3.4209e-04 - val_loss: 0.5138\nEpoch 133/200\n211/211 [==============================] - 1s 2ms/step - loss: 3.2254e-04 - val_loss: 0.5169\nEpoch 134/200\n211/211 [==============================] - 0s 2ms/step - loss: 2.9769e-04 - val_loss: 0.5198\nEpoch 135/200\n211/211 [==============================] - 1s 3ms/step - loss: 2.7472e-04 - val_loss: 0.5214\nEpoch 136/200\n211/211 [==============================] - 1s 2ms/step - loss: 2.5841e-04 - val_loss: 0.5248\nEpoch 137/200\n211/211 [==============================] - 1s 2ms/step - loss: 2.3664e-04 - val_loss: 0.5287\nEpoch 138/200\n211/211 [==============================] - 0s 2ms/step - loss: 2.1460e-04 - val_loss: 0.5318\nEpoch 139/200\n211/211 [==============================] - 1s 3ms/step - loss: 2.1371e-04 - val_loss: 0.5341\nEpoch 140/200\n211/211 [==============================] - 1s 2ms/step - loss: 1.9274e-04 - val_loss: 0.5376\nEpoch 141/200\n211/211 [==============================] - 1s 3ms/step - loss: 1.8376e-04 - val_loss: 0.5440\nEpoch 142/200\n211/211 [==============================] - 1s 2ms/step - loss: 1.7039e-04 - val_loss: 0.5448\nEpoch 143/200\n211/211 [==============================] - 1s 2ms/step - loss: 1.5576e-04 - val_loss: 0.5459\nEpoch 144/200\n211/211 [==============================] - 1s 3ms/step - loss: 1.4597e-04 - val_loss: 0.5512\nEpoch 145/200\n211/211 [==============================] - 1s 2ms/step - loss: 1.2509e-04 - val_loss: 0.5546\nEpoch 146/200\n211/211 [==============================] - 1s 2ms/step - loss: 1.1567e-04 - val_loss: 0.5587\nEpoch 147/200\n211/211 [==============================] - 0s 2ms/step - loss: 1.1210e-04 - val_loss: 0.5631\nEpoch 148/200\n211/211 [==============================] - 1s 2ms/step - loss: 1.0349e-04 - val_loss: 0.5668\nEpoch 149/200\n211/211 [==============================] - 1s 2ms/step - loss: 1.2706e-04 - val_loss: 0.5777\nEpoch 150/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0481 - val_loss: 0.5502\nEpoch 151/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0279 - val_loss: 0.5080\nEpoch 152/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0035 - val_loss: 0.5147\nEpoch 153/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0010 - val_loss: 0.5095\nEpoch 154/200\n211/211 [==============================] - 0s 2ms/step - loss: 5.4619e-04 - val_loss: 0.5113\nEpoch 155/200\n211/211 [==============================] - 1s 2ms/step - loss: 4.5174e-04 - val_loss: 0.5160\nEpoch 156/200\n211/211 [==============================] - 1s 2ms/step - loss: 3.8510e-04 - val_loss: 0.5188\nEpoch 157/200\n211/211 [==============================] - 1s 2ms/step - loss: 3.3755e-04 - val_loss: 0.5203\nEpoch 158/200\n211/211 [==============================] - 1s 2ms/step - loss: 2.9929e-04 - val_loss: 0.5224\nEpoch 159/200\n211/211 [==============================] - 1s 2ms/step - loss: 2.7314e-04 - val_loss: 0.5245\nEpoch 160/200\n211/211 [==============================] - 1s 3ms/step - loss: 2.4786e-04 - val_loss: 0.5271\nEpoch 161/200\n211/211 [==============================] - 1s 2ms/step - loss: 2.2578e-04 - val_loss: 0.5297\nEpoch 162/200\n211/211 [==============================] - 1s 2ms/step - loss: 2.0749e-04 - val_loss: 0.5313\nEpoch 163/200\n211/211 [==============================] - 1s 2ms/step - loss: 1.9239e-04 - val_loss: 0.5333\nEpoch 164/200\n211/211 [==============================] - 1s 2ms/step - loss: 1.8049e-04 - val_loss: 0.5360\nEpoch 165/200\n211/211 [==============================] - 1s 2ms/step - loss: 1.6664e-04 - val_loss: 0.5386\nEpoch 166/200\n211/211 [==============================] - 1s 2ms/step - loss: 1.5557e-04 - val_loss: 0.5418\nEpoch 167/200\n211/211 [==============================] - 1s 2ms/step - loss: 1.4307e-04 - val_loss: 0.5434\nEpoch 168/200\n211/211 [==============================] - 1s 2ms/step - loss: 1.3401e-04 - val_loss: 0.5460\nEpoch 169/200\n211/211 [==============================] - 1s 2ms/step - loss: 1.2544e-04 - val_loss: 0.5488\nEpoch 170/200\n211/211 [==============================] - 1s 2ms/step - loss: 1.1789e-04 - val_loss: 0.5494\nEpoch 171/200\n211/211 [==============================] - 1s 3ms/step - loss: 1.0919e-04 - val_loss: 0.5525\nEpoch 172/200\n211/211 [==============================] - 1s 2ms/step - loss: 1.0079e-04 - val_loss: 0.5558\nEpoch 173/200\n211/211 [==============================] - 1s 2ms/step - loss: 9.4939e-05 - val_loss: 0.5582\nEpoch 174/200\n211/211 [==============================] - 1s 2ms/step - loss: 8.8827e-05 - val_loss: 0.5596\nEpoch 175/200\n211/211 [==============================] - 1s 2ms/step - loss: 8.1890e-05 - val_loss: 0.5643\nEpoch 176/200\n211/211 [==============================] - 0s 2ms/step - loss: 7.6896e-05 - val_loss: 0.5657\nEpoch 177/200\n211/211 [==============================] - 1s 2ms/step - loss: 7.0122e-05 - val_loss: 0.5665\nEpoch 178/200\n211/211 [==============================] - 1s 2ms/step - loss: 6.7062e-05 - val_loss: 0.5728\nEpoch 179/200\n211/211 [==============================] - 1s 2ms/step - loss: 6.3194e-05 - val_loss: 0.5744\nEpoch 180/200\n211/211 [==============================] - 1s 2ms/step - loss: 5.7270e-05 - val_loss: 0.5762\nEpoch 181/200\n211/211 [==============================] - 1s 2ms/step - loss: 5.4532e-05 - val_loss: 0.5804\nEpoch 182/200\n211/211 [==============================] - 1s 3ms/step - loss: 5.0306e-05 - val_loss: 0.5848\nEpoch 183/200\n211/211 [==============================] - 1s 2ms/step - loss: 4.7183e-05 - val_loss: 0.5868\nEpoch 184/200\n211/211 [==============================] - 1s 2ms/step - loss: 4.3554e-05 - val_loss: 0.5904\nEpoch 185/200\n211/211 [==============================] - 1s 2ms/step - loss: 3.9802e-05 - val_loss: 0.5952\nEpoch 186/200\n211/211 [==============================] - 1s 2ms/step - loss: 3.8342e-05 - val_loss: 0.5996\nEpoch 187/200\n211/211 [==============================] - 1s 2ms/step - loss: 3.4075e-05 - val_loss: 0.6043\nEpoch 188/200\n211/211 [==============================] - 1s 2ms/step - loss: 3.3210e-05 - val_loss: 0.6065\nEpoch 189/200\n211/211 [==============================] - 1s 3ms/step - loss: 3.1941e-05 - val_loss: 0.6122\nEpoch 190/200\n211/211 [==============================] - 1s 2ms/step - loss: 2.7711e-05 - val_loss: 0.6161\nEpoch 191/200\n211/211 [==============================] - 1s 2ms/step - loss: 2.7709e-05 - val_loss: 0.6182\nEpoch 192/200\n211/211 [==============================] - 1s 3ms/step - loss: 2.2732e-05 - val_loss: 0.6233\nEpoch 193/200\n211/211 [==============================] - 1s 2ms/step - loss: 2.1005e-05 - val_loss: 0.6310\nEpoch 194/200\n211/211 [==============================] - 0s 2ms/step - loss: 1.9208e-05 - val_loss: 0.6325\nEpoch 195/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0207 - val_loss: 0.6476\nEpoch 196/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0354 - val_loss: 0.5594\nEpoch 197/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0030 - val_loss: 0.5671\nEpoch 198/200\n211/211 [==============================] - 1s 2ms/step - loss: 6.5907e-04 - val_loss: 0.5652\nEpoch 199/200\n211/211 [==============================] - 1s 2ms/step - loss: 3.8768e-04 - val_loss: 0.5677\nEpoch 200/200\n211/211 [==============================] - 1s 2ms/step - loss: 3.0863e-04 - val_loss: 0.5698\n88/88 [==============================] - 0s 1ms/step\n     .. fold 4 trained/predicted\n     . Optimising model hyperparameters\n     .. Model not in optimisation list <verstack.stacking.kerasModel.kerasModel object at 0x7f96e8528b50>\nEpoch 1/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.4058 - val_loss: 0.2101\nEpoch 2/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1969 - val_loss: 0.1650\nEpoch 3/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1800 - val_loss: 0.1608\nEpoch 4/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.1757 - val_loss: 0.1550\nEpoch 5/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.1732 - val_loss: 0.1555\nEpoch 6/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1706 - val_loss: 0.1576\nEpoch 7/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1676 - val_loss: 0.1544\nEpoch 8/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.1641 - val_loss: 0.1547\nEpoch 9/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.1623 - val_loss: 0.1539\nEpoch 10/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.1589 - val_loss: 0.1560\nEpoch 11/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.1565 - val_loss: 0.1533\nEpoch 12/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1542 - val_loss: 0.1532\nEpoch 13/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.1517 - val_loss: 0.1536\nEpoch 14/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1494 - val_loss: 0.1508\nEpoch 15/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1463 - val_loss: 0.1517\nEpoch 16/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1428 - val_loss: 0.1510\nEpoch 17/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1404 - val_loss: 0.1524\nEpoch 18/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1382 - val_loss: 0.1520\nEpoch 19/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1364 - val_loss: 0.1533\nEpoch 20/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1334 - val_loss: 0.1530\nEpoch 21/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1309 - val_loss: 0.1552\nEpoch 22/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1280 - val_loss: 0.1567\nEpoch 23/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1245 - val_loss: 0.1544\nEpoch 24/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.1225 - val_loss: 0.1565\nEpoch 25/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1199 - val_loss: 0.1558\nEpoch 26/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.1177 - val_loss: 0.1588\nEpoch 27/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1150 - val_loss: 0.1612\nEpoch 28/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1128 - val_loss: 0.1595\nEpoch 29/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.1108 - val_loss: 0.1597\nEpoch 30/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.1085 - val_loss: 0.1607\nEpoch 31/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1060 - val_loss: 0.1629\nEpoch 32/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1036 - val_loss: 0.1672\nEpoch 33/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1003 - val_loss: 0.1636\nEpoch 34/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0999 - val_loss: 0.1665\nEpoch 35/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0965 - val_loss: 0.1701\nEpoch 36/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0941 - val_loss: 0.1691\nEpoch 37/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0925 - val_loss: 0.1670\nEpoch 38/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0908 - val_loss: 0.1705\nEpoch 39/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0890 - val_loss: 0.1725\nEpoch 40/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0867 - val_loss: 0.1724\nEpoch 41/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0846 - val_loss: 0.1755\nEpoch 42/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0824 - val_loss: 0.1785\nEpoch 43/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0806 - val_loss: 0.1757\nEpoch 44/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0791 - val_loss: 0.1788\nEpoch 45/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0762 - val_loss: 0.1825\nEpoch 46/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0750 - val_loss: 0.1847\nEpoch 47/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0736 - val_loss: 0.1827\nEpoch 48/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0712 - val_loss: 0.1828\nEpoch 49/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0702 - val_loss: 0.1868\nEpoch 50/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0679 - val_loss: 0.1902\nEpoch 51/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0668 - val_loss: 0.1894\nEpoch 52/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0649 - val_loss: 0.1929\nEpoch 53/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0629 - val_loss: 0.1917\nEpoch 54/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0619 - val_loss: 0.1971\nEpoch 55/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0600 - val_loss: 0.1976\nEpoch 56/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0587 - val_loss: 0.1957\nEpoch 57/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0576 - val_loss: 0.1974\nEpoch 58/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0556 - val_loss: 0.2024\nEpoch 59/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0541 - val_loss: 0.2030\nEpoch 60/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0527 - val_loss: 0.2043\nEpoch 61/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0506 - val_loss: 0.2098\nEpoch 62/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0506 - val_loss: 0.2095\nEpoch 63/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0489 - val_loss: 0.2126\nEpoch 64/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0486 - val_loss: 0.2178\nEpoch 65/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0467 - val_loss: 0.2151\nEpoch 66/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0452 - val_loss: 0.2179\nEpoch 67/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0446 - val_loss: 0.2182\nEpoch 68/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0425 - val_loss: 0.2226\nEpoch 69/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0424 - val_loss: 0.2292\nEpoch 70/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0409 - val_loss: 0.2246\nEpoch 71/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0398 - val_loss: 0.2290\nEpoch 72/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0387 - val_loss: 0.2298\nEpoch 73/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0376 - val_loss: 0.2374\nEpoch 74/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0369 - val_loss: 0.2330\nEpoch 75/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0351 - val_loss: 0.2373\nEpoch 76/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0340 - val_loss: 0.2441\nEpoch 77/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0334 - val_loss: 0.2396\nEpoch 78/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0330 - val_loss: 0.2421\nEpoch 79/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0314 - val_loss: 0.2428\nEpoch 80/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0308 - val_loss: 0.2473\nEpoch 81/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0299 - val_loss: 0.2454\nEpoch 82/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0289 - val_loss: 0.2510\nEpoch 83/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0284 - val_loss: 0.2521\nEpoch 84/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0271 - val_loss: 0.2630\nEpoch 85/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0269 - val_loss: 0.2606\nEpoch 86/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0258 - val_loss: 0.2596\nEpoch 87/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0247 - val_loss: 0.2593\nEpoch 88/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0241 - val_loss: 0.2654\nEpoch 89/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0239 - val_loss: 0.2654\nEpoch 90/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0230 - val_loss: 0.2646\nEpoch 91/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0222 - val_loss: 0.2678\nEpoch 92/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0218 - val_loss: 0.2726\nEpoch 93/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0207 - val_loss: 0.2717\nEpoch 94/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0203 - val_loss: 0.2763\nEpoch 95/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0200 - val_loss: 0.2782\nEpoch 96/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0193 - val_loss: 0.2836\nEpoch 97/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0190 - val_loss: 0.2863\nEpoch 98/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0177 - val_loss: 0.2830\nEpoch 99/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0179 - val_loss: 0.2864\nEpoch 100/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0171 - val_loss: 0.2879\nEpoch 101/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0165 - val_loss: 0.2910\nEpoch 102/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0155 - val_loss: 0.2952\nEpoch 103/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0155 - val_loss: 0.2961\nEpoch 104/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0147 - val_loss: 0.2940\nEpoch 105/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0143 - val_loss: 0.2979\nEpoch 106/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0136 - val_loss: 0.3035\nEpoch 107/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0140 - val_loss: 0.3084\nEpoch 108/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0129 - val_loss: 0.3029\nEpoch 109/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0122 - val_loss: 0.3081\nEpoch 110/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0125 - val_loss: 0.3086\nEpoch 111/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0122 - val_loss: 0.3105\nEpoch 112/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0114 - val_loss: 0.3105\nEpoch 113/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0109 - val_loss: 0.3136\nEpoch 114/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0111 - val_loss: 0.3169\nEpoch 115/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0102 - val_loss: 0.3213\nEpoch 116/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0098 - val_loss: 0.3212\nEpoch 117/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0101 - val_loss: 0.3238\nEpoch 118/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0095 - val_loss: 0.3264\nEpoch 119/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0088 - val_loss: 0.3298\nEpoch 120/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0089 - val_loss: 0.3279\nEpoch 121/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0084 - val_loss: 0.3313\nEpoch 122/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0080 - val_loss: 0.3337\nEpoch 123/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0082 - val_loss: 0.3341\nEpoch 124/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0079 - val_loss: 0.3382\nEpoch 125/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0074 - val_loss: 0.3376\nEpoch 126/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0072 - val_loss: 0.3426\nEpoch 127/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0069 - val_loss: 0.3450\nEpoch 128/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0068 - val_loss: 0.3470\nEpoch 129/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0064 - val_loss: 0.3476\nEpoch 130/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0061 - val_loss: 0.3552\nEpoch 131/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0059 - val_loss: 0.3555\nEpoch 132/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0058 - val_loss: 0.3613\nEpoch 133/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0055 - val_loss: 0.3576\nEpoch 134/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0055 - val_loss: 0.3596\nEpoch 135/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0052 - val_loss: 0.3610\nEpoch 136/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0051 - val_loss: 0.3641\nEpoch 137/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0048 - val_loss: 0.3653\nEpoch 138/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0046 - val_loss: 0.3679\nEpoch 139/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0047 - val_loss: 0.3725\nEpoch 140/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0045 - val_loss: 0.3729\nEpoch 141/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0041 - val_loss: 0.3755\nEpoch 142/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0043 - val_loss: 0.3788\nEpoch 143/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0038 - val_loss: 0.3777\nEpoch 144/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0037 - val_loss: 0.3836\nEpoch 145/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0034 - val_loss: 0.3878\nEpoch 146/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0034 - val_loss: 0.3888\nEpoch 147/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0036 - val_loss: 0.3895\nEpoch 148/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0034 - val_loss: 0.3922\nEpoch 149/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0031 - val_loss: 0.4021\nEpoch 150/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0029 - val_loss: 0.3958\nEpoch 151/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0033 - val_loss: 0.4053\nEpoch 152/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0029 - val_loss: 0.4009\nEpoch 153/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0026 - val_loss: 0.4071\nEpoch 154/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0030 - val_loss: 0.4102\nEpoch 155/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0024 - val_loss: 0.4137\nEpoch 156/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0025 - val_loss: 0.4133\nEpoch 157/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0023 - val_loss: 0.4154\nEpoch 158/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0021 - val_loss: 0.4180\nEpoch 159/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0021 - val_loss: 0.4195\nEpoch 160/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0021 - val_loss: 0.4214\nEpoch 161/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 0.4227\nEpoch 162/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.4260\nEpoch 163/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.4280\nEpoch 164/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.4310\nEpoch 165/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.4366\nEpoch 166/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.4387\nEpoch 167/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.4415\nEpoch 168/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0015 - val_loss: 0.4382\nEpoch 169/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.4420\nEpoch 170/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.4458\nEpoch 171/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.4478\nEpoch 172/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.4495\nEpoch 173/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0016 - val_loss: 0.4589\nEpoch 174/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0023 - val_loss: 0.4502\nEpoch 175/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0012 - val_loss: 0.4555\nEpoch 176/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0010 - val_loss: 0.4558\nEpoch 177/200\n211/211 [==============================] - 0s 2ms/step - loss: 9.7609e-04 - val_loss: 0.4570\nEpoch 178/200\n211/211 [==============================] - 0s 2ms/step - loss: 9.5263e-04 - val_loss: 0.4606\nEpoch 179/200\n211/211 [==============================] - 0s 2ms/step - loss: 9.3809e-04 - val_loss: 0.4643\nEpoch 180/200\n211/211 [==============================] - 1s 2ms/step - loss: 9.4673e-04 - val_loss: 0.4660\nEpoch 181/200\n211/211 [==============================] - 0s 2ms/step - loss: 8.3787e-04 - val_loss: 0.4691\nEpoch 182/200\n211/211 [==============================] - 0s 2ms/step - loss: 8.7259e-04 - val_loss: 0.4697\nEpoch 183/200\n211/211 [==============================] - 0s 2ms/step - loss: 8.0606e-04 - val_loss: 0.4730\nEpoch 184/200\n211/211 [==============================] - 1s 3ms/step - loss: 8.0972e-04 - val_loss: 0.4730\nEpoch 185/200\n211/211 [==============================] - 0s 2ms/step - loss: 8.7347e-04 - val_loss: 0.4845\nEpoch 186/200\n211/211 [==============================] - 0s 2ms/step - loss: 9.0484e-04 - val_loss: 0.4788\nEpoch 187/200\n211/211 [==============================] - 0s 2ms/step - loss: 7.1948e-04 - val_loss: 0.4816\nEpoch 188/200\n211/211 [==============================] - 0s 2ms/step - loss: 6.6203e-04 - val_loss: 0.4843\nEpoch 189/200\n211/211 [==============================] - 0s 2ms/step - loss: 6.6646e-04 - val_loss: 0.4927\nEpoch 190/200\n211/211 [==============================] - 1s 3ms/step - loss: 7.0506e-04 - val_loss: 0.4902\nEpoch 191/200\n211/211 [==============================] - 0s 2ms/step - loss: 7.2360e-04 - val_loss: 0.4943\nEpoch 192/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0023 - val_loss: 0.4941\nEpoch 193/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0016 - val_loss: 0.5033\nEpoch 194/200\n211/211 [==============================] - 1s 3ms/step - loss: 5.6849e-04 - val_loss: 0.5005\nEpoch 195/200\n211/211 [==============================] - 0s 2ms/step - loss: 5.0956e-04 - val_loss: 0.5006\nEpoch 196/200\n211/211 [==============================] - 1s 3ms/step - loss: 4.9126e-04 - val_loss: 0.5000\nEpoch 197/200\n211/211 [==============================] - 1s 3ms/step - loss: 4.6940e-04 - val_loss: 0.5012\nEpoch 198/200\n211/211 [==============================] - 0s 2ms/step - loss: 4.7051e-04 - val_loss: 0.5029\nEpoch 199/200\n211/211 [==============================] - 0s 2ms/step - loss: 4.5046e-04 - val_loss: 0.5039\nEpoch 200/200\n211/211 [==============================] - 0s 2ms/step - loss: 4.3879e-04 - val_loss: 0.5061\n88/88 [==============================] - 0s 1ms/step\nKeras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n...layers\n......dense\n.........vars\n............0\n............1\n......dense_1\n.........vars\n............0\n............1\n......normalization\n.........vars\n............0\n............1\n............2\n...metrics\n......mean\n.........vars\n............0\n............1\n...optimizer\n......vars\n.........0\n.........1\n.........2\n.........3\n.........4\n.........5\n.........6\n.........7\n.........8\n...vars\nKeras model archive saving:\nFile Name                                             Modified             Size\nconfig.json                                    2022-12-10 06:55:41         1987\nvariables.h5                                   2022-12-10 06:55:41        66096\nmetadata.json                                  2022-12-10 06:55:41           64\nKeras model archive loading:\nFile Name                                             Modified             Size\nconfig.json                                    2022-12-10 06:55:40         1987\nvariables.h5                                   2022-12-10 06:55:40        66096\nmetadata.json                                  2022-12-10 06:55:40           64\nEpoch 1/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.3972 - val_loss: 0.2090\nEpoch 2/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.1925 - val_loss: 0.1659\nEpoch 3/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.1762 - val_loss: 0.1616\nEpoch 4/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1716 - val_loss: 0.1608\nEpoch 5/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1674 - val_loss: 0.1600\nEpoch 6/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1648 - val_loss: 0.1560\nEpoch 7/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.1619 - val_loss: 0.1568\nEpoch 8/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.1587 - val_loss: 0.1563\nEpoch 9/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1565 - val_loss: 0.1516\nEpoch 10/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1544 - val_loss: 0.1529\nEpoch 11/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1516 - val_loss: 0.1546\nEpoch 12/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1494 - val_loss: 0.1563\nEpoch 13/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1464 - val_loss: 0.1540\nEpoch 14/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.1436 - val_loss: 0.1515\nEpoch 15/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1421 - val_loss: 0.1532\nEpoch 16/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1389 - val_loss: 0.1537\nEpoch 17/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1361 - val_loss: 0.1537\nEpoch 18/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1339 - val_loss: 0.1496\nEpoch 19/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1308 - val_loss: 0.1552\nEpoch 20/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.1285 - val_loss: 0.1541\nEpoch 21/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1261 - val_loss: 0.1530\nEpoch 22/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.1244 - val_loss: 0.1520\nEpoch 23/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.1209 - val_loss: 0.1551\nEpoch 24/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1189 - val_loss: 0.1536\nEpoch 25/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1173 - val_loss: 0.1560\nEpoch 26/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1143 - val_loss: 0.1583\nEpoch 27/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1123 - val_loss: 0.1561\nEpoch 28/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.1105 - val_loss: 0.1552\nEpoch 29/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.1082 - val_loss: 0.1575\nEpoch 30/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.1062 - val_loss: 0.1605\nEpoch 31/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.1031 - val_loss: 0.1601\nEpoch 32/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1022 - val_loss: 0.1587\nEpoch 33/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0990 - val_loss: 0.1573\nEpoch 34/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0972 - val_loss: 0.1624\nEpoch 35/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0949 - val_loss: 0.1620\nEpoch 36/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0924 - val_loss: 0.1640\nEpoch 37/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0904 - val_loss: 0.1638\nEpoch 38/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0880 - val_loss: 0.1638\nEpoch 39/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0864 - val_loss: 0.1651\nEpoch 40/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0831 - val_loss: 0.1720\nEpoch 41/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0823 - val_loss: 0.1668\nEpoch 42/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0809 - val_loss: 0.1685\nEpoch 43/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0787 - val_loss: 0.1652\nEpoch 44/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0775 - val_loss: 0.1695\nEpoch 45/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0750 - val_loss: 0.1706\nEpoch 46/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0734 - val_loss: 0.1713\nEpoch 47/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0714 - val_loss: 0.1745\nEpoch 48/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0694 - val_loss: 0.1736\nEpoch 49/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0678 - val_loss: 0.1755\nEpoch 50/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0659 - val_loss: 0.1762\nEpoch 51/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0642 - val_loss: 0.1785\nEpoch 52/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0631 - val_loss: 0.1797\nEpoch 53/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0612 - val_loss: 0.1801\nEpoch 54/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0601 - val_loss: 0.1805\nEpoch 55/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0577 - val_loss: 0.1826\nEpoch 56/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0559 - val_loss: 0.1911\nEpoch 57/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0538 - val_loss: 0.1877\nEpoch 58/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0529 - val_loss: 0.1943\nEpoch 59/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0519 - val_loss: 0.1985\nEpoch 60/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0507 - val_loss: 0.1942\nEpoch 61/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0493 - val_loss: 0.1932\nEpoch 62/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0477 - val_loss: 0.1987\nEpoch 63/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0461 - val_loss: 0.1984\nEpoch 64/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0451 - val_loss: 0.2036\nEpoch 65/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0432 - val_loss: 0.2006\nEpoch 66/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0417 - val_loss: 0.2036\nEpoch 67/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0406 - val_loss: 0.2050\nEpoch 68/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0399 - val_loss: 0.2071\nEpoch 69/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0379 - val_loss: 0.2126\nEpoch 70/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0372 - val_loss: 0.2148\nEpoch 71/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0361 - val_loss: 0.2111\nEpoch 72/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0354 - val_loss: 0.2113\nEpoch 73/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0340 - val_loss: 0.2151\nEpoch 74/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0327 - val_loss: 0.2159\nEpoch 75/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0317 - val_loss: 0.2187\nEpoch 76/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0311 - val_loss: 0.2188\nEpoch 77/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0302 - val_loss: 0.2177\nEpoch 78/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0291 - val_loss: 0.2228\nEpoch 79/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0285 - val_loss: 0.2253\nEpoch 80/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0277 - val_loss: 0.2316\nEpoch 81/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0266 - val_loss: 0.2280\nEpoch 82/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0255 - val_loss: 0.2331\nEpoch 83/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0249 - val_loss: 0.2345\nEpoch 84/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0239 - val_loss: 0.2373\nEpoch 85/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0234 - val_loss: 0.2331\nEpoch 86/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0227 - val_loss: 0.2369\nEpoch 87/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0220 - val_loss: 0.2392\nEpoch 88/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0218 - val_loss: 0.2342\nEpoch 89/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0205 - val_loss: 0.2443\nEpoch 90/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0195 - val_loss: 0.2386\nEpoch 91/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0194 - val_loss: 0.2444\nEpoch 92/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0182 - val_loss: 0.2478\nEpoch 93/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0181 - val_loss: 0.2492\nEpoch 94/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0179 - val_loss: 0.2523\nEpoch 95/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0164 - val_loss: 0.2561\nEpoch 96/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0164 - val_loss: 0.2548\nEpoch 97/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0156 - val_loss: 0.2580\nEpoch 98/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0148 - val_loss: 0.2522\nEpoch 99/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0144 - val_loss: 0.2589\nEpoch 100/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0142 - val_loss: 0.2607\nEpoch 101/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0136 - val_loss: 0.2601\nEpoch 102/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0129 - val_loss: 0.2654\nEpoch 103/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0125 - val_loss: 0.2634\nEpoch 104/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0126 - val_loss: 0.2676\nEpoch 105/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0118 - val_loss: 0.2745\nEpoch 106/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0115 - val_loss: 0.2720\nEpoch 107/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0113 - val_loss: 0.2725\nEpoch 108/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0111 - val_loss: 0.2757\nEpoch 109/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0102 - val_loss: 0.2835\nEpoch 110/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0098 - val_loss: 0.2846\nEpoch 111/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0095 - val_loss: 0.2791\nEpoch 112/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0093 - val_loss: 0.2838\nEpoch 113/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0089 - val_loss: 0.2905\nEpoch 114/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0082 - val_loss: 0.2897\nEpoch 115/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0081 - val_loss: 0.2905\nEpoch 116/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0078 - val_loss: 0.2951\nEpoch 117/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0076 - val_loss: 0.2953\nEpoch 118/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0077 - val_loss: 0.2993\nEpoch 119/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0069 - val_loss: 0.3019\nEpoch 120/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0071 - val_loss: 0.3016\nEpoch 121/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0067 - val_loss: 0.3052\nEpoch 122/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0063 - val_loss: 0.3106\nEpoch 123/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0062 - val_loss: 0.3077\nEpoch 124/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0057 - val_loss: 0.3101\nEpoch 125/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0059 - val_loss: 0.3095\nEpoch 126/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0054 - val_loss: 0.3169\nEpoch 127/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0050 - val_loss: 0.3164\nEpoch 128/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0051 - val_loss: 0.3168\nEpoch 129/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0047 - val_loss: 0.3220\nEpoch 130/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0045 - val_loss: 0.3240\nEpoch 131/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0045 - val_loss: 0.3224\nEpoch 132/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0043 - val_loss: 0.3275\nEpoch 133/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0042 - val_loss: 0.3278\nEpoch 134/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0041 - val_loss: 0.3310\nEpoch 135/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0038 - val_loss: 0.3438\nEpoch 136/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0042 - val_loss: 0.3395\nEpoch 137/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 0.3409\nEpoch 138/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0034 - val_loss: 0.3420\nEpoch 139/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0033 - val_loss: 0.3418\nEpoch 140/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0032 - val_loss: 0.3437\nEpoch 141/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0030 - val_loss: 0.3476\nEpoch 142/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0029 - val_loss: 0.3530\nEpoch 143/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0029 - val_loss: 0.3534\nEpoch 144/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0027 - val_loss: 0.3552\nEpoch 145/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0027 - val_loss: 0.3558\nEpoch 146/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0026 - val_loss: 0.3608\nEpoch 147/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0024 - val_loss: 0.3571\nEpoch 148/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0023 - val_loss: 0.3634\nEpoch 149/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0022 - val_loss: 0.3624\nEpoch 150/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0021 - val_loss: 0.3652\nEpoch 151/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 0.3699\nEpoch 152/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0020 - val_loss: 0.3703\nEpoch 153/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.3747\nEpoch 154/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.3766\nEpoch 155/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0018 - val_loss: 0.3777\nEpoch 156/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.3821\nEpoch 157/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0016 - val_loss: 0.3844\nEpoch 158/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.3854\nEpoch 159/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.3854\nEpoch 160/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0015 - val_loss: 0.3860\nEpoch 161/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.3935\nEpoch 162/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.3916\nEpoch 163/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.3951\nEpoch 164/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0011 - val_loss: 0.3968\nEpoch 165/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.3965\nEpoch 166/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.4032\nEpoch 167/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.4104\nEpoch 168/200\n211/211 [==============================] - 0s 2ms/step - loss: 9.9772e-04 - val_loss: 0.4097\nEpoch 169/200\n211/211 [==============================] - 0s 2ms/step - loss: 9.1732e-04 - val_loss: 0.4067\nEpoch 170/200\n211/211 [==============================] - 0s 2ms/step - loss: 9.8397e-04 - val_loss: 0.4069\nEpoch 171/200\n211/211 [==============================] - 0s 2ms/step - loss: 8.5713e-04 - val_loss: 0.4115\nEpoch 172/200\n211/211 [==============================] - 0s 2ms/step - loss: 8.3467e-04 - val_loss: 0.4133\nEpoch 173/200\n211/211 [==============================] - 0s 2ms/step - loss: 8.6280e-04 - val_loss: 0.4229\nEpoch 174/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.4324\nEpoch 175/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.4307\nEpoch 176/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0031 - val_loss: 0.4240\nEpoch 177/200\n211/211 [==============================] - 0s 2ms/step - loss: 7.0333e-04 - val_loss: 0.4212\nEpoch 178/200\n211/211 [==============================] - 1s 3ms/step - loss: 6.3657e-04 - val_loss: 0.4210\nEpoch 179/200\n211/211 [==============================] - 1s 3ms/step - loss: 6.0917e-04 - val_loss: 0.4224\nEpoch 180/200\n211/211 [==============================] - 0s 2ms/step - loss: 5.9862e-04 - val_loss: 0.4236\nEpoch 181/200\n211/211 [==============================] - 0s 2ms/step - loss: 5.8998e-04 - val_loss: 0.4266\nEpoch 182/200\n211/211 [==============================] - 0s 2ms/step - loss: 5.5065e-04 - val_loss: 0.4273\nEpoch 183/200\n211/211 [==============================] - 0s 2ms/step - loss: 5.6641e-04 - val_loss: 0.4269\nEpoch 184/200\n211/211 [==============================] - 0s 2ms/step - loss: 5.2751e-04 - val_loss: 0.4315\nEpoch 185/200\n211/211 [==============================] - 0s 2ms/step - loss: 5.3687e-04 - val_loss: 0.4366\nEpoch 186/200\n211/211 [==============================] - 0s 2ms/step - loss: 5.0621e-04 - val_loss: 0.4379\nEpoch 187/200\n211/211 [==============================] - 0s 2ms/step - loss: 5.6877e-04 - val_loss: 0.4384\nEpoch 188/200\n211/211 [==============================] - 0s 2ms/step - loss: 4.7921e-04 - val_loss: 0.4349\nEpoch 189/200\n211/211 [==============================] - 0s 2ms/step - loss: 4.7681e-04 - val_loss: 0.4419\nEpoch 190/200\n211/211 [==============================] - 0s 2ms/step - loss: 4.3154e-04 - val_loss: 0.4463\nEpoch 191/200\n211/211 [==============================] - 1s 3ms/step - loss: 4.3880e-04 - val_loss: 0.4447\nEpoch 192/200\n211/211 [==============================] - 0s 2ms/step - loss: 4.3639e-04 - val_loss: 0.4484\nEpoch 193/200\n211/211 [==============================] - 0s 2ms/step - loss: 4.1412e-04 - val_loss: 0.4501\nEpoch 194/200\n211/211 [==============================] - 0s 2ms/step - loss: 3.9427e-04 - val_loss: 0.4514\nEpoch 195/200\n211/211 [==============================] - 0s 2ms/step - loss: 4.0256e-04 - val_loss: 0.4559\nEpoch 196/200\n211/211 [==============================] - 0s 2ms/step - loss: 3.7112e-04 - val_loss: 0.4543\nEpoch 197/200\n211/211 [==============================] - 1s 2ms/step - loss: 3.5692e-04 - val_loss: 0.4561\nEpoch 198/200\n211/211 [==============================] - 0s 2ms/step - loss: 7.1063e-04 - val_loss: 0.4596\nEpoch 199/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0029 - val_loss: 0.4610\nEpoch 200/200\n211/211 [==============================] - 1s 3ms/step - loss: 4.7099e-04 - val_loss: 0.4597\n88/88 [==============================] - 0s 1ms/step\n     .. fold 2 trained/predicted\nKeras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n...layers\n......dense\n.........vars\n............0\n............1\n......dense_1\n.........vars\n............0\n............1\n......normalization\n.........vars\n............0\n............1\n............2\n...metrics\n......mean\n.........vars\n............0\n............1\n...optimizer\n......vars\n.........0\n.........1\n.........2\n.........3\n.........4\n.........5\n.........6\n.........7\n.........8\n...vars\nKeras model archive saving:\nFile Name                                             Modified             Size\nconfig.json                                    2022-12-10 06:57:22         1987\nvariables.h5                                   2022-12-10 06:57:22        66096\nmetadata.json                                  2022-12-10 06:57:22           64\nKeras model archive loading:\nFile Name                                             Modified             Size\nconfig.json                                    2022-12-10 06:57:22         1987\nvariables.h5                                   2022-12-10 06:57:22        66096\nmetadata.json                                  2022-12-10 06:57:22           64\nEpoch 1/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.4411 - val_loss: 0.2238\nEpoch 2/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.1938 - val_loss: 0.1585\nEpoch 3/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1713 - val_loss: 0.1474\nEpoch 4/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.1670 - val_loss: 0.1490\nEpoch 5/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1632 - val_loss: 0.1488\nEpoch 6/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.1605 - val_loss: 0.1508\nEpoch 7/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1578 - val_loss: 0.1485\nEpoch 8/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.1555 - val_loss: 0.1488\nEpoch 9/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1532 - val_loss: 0.1501\nEpoch 10/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.1507 - val_loss: 0.1496\nEpoch 11/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1478 - val_loss: 0.1472\nEpoch 12/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1458 - val_loss: 0.1478\nEpoch 13/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1434 - val_loss: 0.1490\nEpoch 14/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1416 - val_loss: 0.1476\nEpoch 15/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1383 - val_loss: 0.1494\nEpoch 16/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1360 - val_loss: 0.1507\nEpoch 17/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1343 - val_loss: 0.1481\nEpoch 18/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.1310 - val_loss: 0.1495\nEpoch 19/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1290 - val_loss: 0.1517\nEpoch 20/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1271 - val_loss: 0.1484\nEpoch 21/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1245 - val_loss: 0.1515\nEpoch 22/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1215 - val_loss: 0.1543\nEpoch 23/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1204 - val_loss: 0.1512\nEpoch 24/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1170 - val_loss: 0.1516\nEpoch 25/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.1144 - val_loss: 0.1539\nEpoch 26/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1130 - val_loss: 0.1592\nEpoch 27/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1106 - val_loss: 0.1561\nEpoch 28/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1084 - val_loss: 0.1593\nEpoch 29/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1058 - val_loss: 0.1562\nEpoch 30/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1036 - val_loss: 0.1566\nEpoch 31/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1013 - val_loss: 0.1591\nEpoch 32/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0993 - val_loss: 0.1595\nEpoch 33/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0973 - val_loss: 0.1612\nEpoch 34/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0954 - val_loss: 0.1597\nEpoch 35/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0936 - val_loss: 0.1632\nEpoch 36/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0915 - val_loss: 0.1652\nEpoch 37/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0890 - val_loss: 0.1693\nEpoch 38/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0882 - val_loss: 0.1661\nEpoch 39/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0849 - val_loss: 0.1680\nEpoch 40/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0836 - val_loss: 0.1711\nEpoch 41/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0813 - val_loss: 0.1770\nEpoch 42/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0793 - val_loss: 0.1744\nEpoch 43/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0782 - val_loss: 0.1710\nEpoch 44/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0758 - val_loss: 0.1768\nEpoch 45/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0743 - val_loss: 0.1758\nEpoch 46/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0727 - val_loss: 0.1770\nEpoch 47/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0706 - val_loss: 0.1825\nEpoch 48/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0687 - val_loss: 0.1815\nEpoch 49/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0676 - val_loss: 0.1852\nEpoch 50/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0658 - val_loss: 0.1828\nEpoch 51/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0634 - val_loss: 0.1939\nEpoch 52/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0626 - val_loss: 0.1888\nEpoch 53/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0604 - val_loss: 0.1895\nEpoch 54/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0593 - val_loss: 0.1917\nEpoch 55/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0575 - val_loss: 0.1936\nEpoch 56/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0566 - val_loss: 0.1976\nEpoch 57/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0543 - val_loss: 0.1968\nEpoch 58/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0529 - val_loss: 0.1938\nEpoch 59/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0518 - val_loss: 0.2017\nEpoch 60/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0510 - val_loss: 0.2008\nEpoch 61/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0489 - val_loss: 0.2029\nEpoch 62/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0475 - val_loss: 0.2029\nEpoch 63/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0468 - val_loss: 0.2064\nEpoch 64/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0454 - val_loss: 0.2113\nEpoch 65/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0442 - val_loss: 0.2128\nEpoch 66/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0426 - val_loss: 0.2108\nEpoch 67/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0426 - val_loss: 0.2130\nEpoch 68/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0410 - val_loss: 0.2129\nEpoch 69/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0398 - val_loss: 0.2138\nEpoch 70/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0384 - val_loss: 0.2185\nEpoch 71/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0369 - val_loss: 0.2176\nEpoch 72/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0359 - val_loss: 0.2241\nEpoch 73/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0354 - val_loss: 0.2258\nEpoch 74/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0343 - val_loss: 0.2218\nEpoch 75/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0336 - val_loss: 0.2237\nEpoch 76/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0322 - val_loss: 0.2280\nEpoch 77/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0313 - val_loss: 0.2290\nEpoch 78/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0303 - val_loss: 0.2358\nEpoch 79/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0297 - val_loss: 0.2360\nEpoch 80/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0287 - val_loss: 0.2388\nEpoch 81/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.2411\nEpoch 82/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0277 - val_loss: 0.2399\nEpoch 83/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0262 - val_loss: 0.2406\nEpoch 84/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0254 - val_loss: 0.2460\nEpoch 85/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0247 - val_loss: 0.2481\nEpoch 86/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0241 - val_loss: 0.2523\nEpoch 87/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0232 - val_loss: 0.2482\nEpoch 88/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0233 - val_loss: 0.2560\nEpoch 89/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0220 - val_loss: 0.2566\nEpoch 90/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0215 - val_loss: 0.2562\nEpoch 91/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0206 - val_loss: 0.2530\nEpoch 92/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0202 - val_loss: 0.2590\nEpoch 93/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0196 - val_loss: 0.2597\nEpoch 94/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0194 - val_loss: 0.2623\nEpoch 95/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0187 - val_loss: 0.2667\nEpoch 96/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0178 - val_loss: 0.2680\nEpoch 97/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0174 - val_loss: 0.2714\nEpoch 98/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0172 - val_loss: 0.2732\nEpoch 99/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0163 - val_loss: 0.2728\nEpoch 100/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0156 - val_loss: 0.2781\nEpoch 101/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0150 - val_loss: 0.2837\nEpoch 102/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0145 - val_loss: 0.2769\nEpoch 103/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0141 - val_loss: 0.2856\nEpoch 104/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0137 - val_loss: 0.2872\nEpoch 105/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0133 - val_loss: 0.2910\nEpoch 106/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0127 - val_loss: 0.2912\nEpoch 107/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0123 - val_loss: 0.2951\nEpoch 108/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0120 - val_loss: 0.3007\nEpoch 109/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0118 - val_loss: 0.3009\nEpoch 110/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0113 - val_loss: 0.2961\nEpoch 111/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0110 - val_loss: 0.3059\nEpoch 112/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0103 - val_loss: 0.3013\nEpoch 113/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0103 - val_loss: 0.3074\nEpoch 114/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0099 - val_loss: 0.3132\nEpoch 115/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0098 - val_loss: 0.3103\nEpoch 116/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0089 - val_loss: 0.3205\nEpoch 117/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0090 - val_loss: 0.3215\nEpoch 118/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0085 - val_loss: 0.3180\nEpoch 119/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0085 - val_loss: 0.3204\nEpoch 120/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0077 - val_loss: 0.3264\nEpoch 121/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0077 - val_loss: 0.3295\nEpoch 122/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0074 - val_loss: 0.3302\nEpoch 123/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0071 - val_loss: 0.3274\nEpoch 124/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0070 - val_loss: 0.3275\nEpoch 125/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0067 - val_loss: 0.3352\nEpoch 126/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0063 - val_loss: 0.3392\nEpoch 127/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0060 - val_loss: 0.3356\nEpoch 128/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0058 - val_loss: 0.3385\nEpoch 129/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0056 - val_loss: 0.3436\nEpoch 130/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0056 - val_loss: 0.3408\nEpoch 131/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0054 - val_loss: 0.3454\nEpoch 132/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0051 - val_loss: 0.3534\nEpoch 133/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0050 - val_loss: 0.3486\nEpoch 134/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0052 - val_loss: 0.3512\nEpoch 135/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0044 - val_loss: 0.3526\nEpoch 136/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0045 - val_loss: 0.3534\nEpoch 137/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0041 - val_loss: 0.3615\nEpoch 138/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0041 - val_loss: 0.3586\nEpoch 139/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0039 - val_loss: 0.3653\nEpoch 140/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0041 - val_loss: 0.3642\nEpoch 141/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 0.3645\nEpoch 142/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0035 - val_loss: 0.3676\nEpoch 143/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0035 - val_loss: 0.3678\nEpoch 144/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0033 - val_loss: 0.3809\nEpoch 145/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0033 - val_loss: 0.3756\nEpoch 146/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0031 - val_loss: 0.3740\nEpoch 147/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0028 - val_loss: 0.3740\nEpoch 148/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0028 - val_loss: 0.3861\nEpoch 149/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0027 - val_loss: 0.3786\nEpoch 150/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0026 - val_loss: 0.3882\nEpoch 151/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0023 - val_loss: 0.3860\nEpoch 152/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0025 - val_loss: 0.3872\nEpoch 153/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0023 - val_loss: 0.3964\nEpoch 154/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0022 - val_loss: 0.3970\nEpoch 155/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 0.3953\nEpoch 156/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 0.3968\nEpoch 157/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0021 - val_loss: 0.3991\nEpoch 158/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0018 - val_loss: 0.3994\nEpoch 159/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0022 - val_loss: 0.4142\nEpoch 160/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0020 - val_loss: 0.4083\nEpoch 161/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.4088\nEpoch 162/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.4119\nEpoch 163/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.4147\nEpoch 164/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0014 - val_loss: 0.4143\nEpoch 165/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.4166\nEpoch 166/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0013 - val_loss: 0.4225\nEpoch 167/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0014 - val_loss: 0.4240\nEpoch 168/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0012 - val_loss: 0.4248\nEpoch 169/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0012 - val_loss: 0.4307\nEpoch 170/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0011 - val_loss: 0.4403\nEpoch 171/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0011 - val_loss: 0.4307\nEpoch 172/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0011 - val_loss: 0.4342\nEpoch 173/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0010 - val_loss: 0.4464\nEpoch 174/200\n211/211 [==============================] - 0s 2ms/step - loss: 9.8080e-04 - val_loss: 0.4413\nEpoch 175/200\n211/211 [==============================] - 0s 2ms/step - loss: 9.9100e-04 - val_loss: 0.4446\nEpoch 176/200\n211/211 [==============================] - 1s 3ms/step - loss: 9.6380e-04 - val_loss: 0.4396\nEpoch 177/200\n211/211 [==============================] - 0s 2ms/step - loss: 9.2634e-04 - val_loss: 0.4481\nEpoch 178/200\n211/211 [==============================] - 0s 2ms/step - loss: 8.3025e-04 - val_loss: 0.4490\nEpoch 179/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.4549\nEpoch 180/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0021 - val_loss: 0.4529\nEpoch 181/200\n211/211 [==============================] - 0s 2ms/step - loss: 9.1844e-04 - val_loss: 0.4503\nEpoch 182/200\n211/211 [==============================] - 0s 2ms/step - loss: 7.1582e-04 - val_loss: 0.4560\nEpoch 183/200\n211/211 [==============================] - 0s 2ms/step - loss: 6.6687e-04 - val_loss: 0.4556\nEpoch 184/200\n211/211 [==============================] - 1s 3ms/step - loss: 6.5114e-04 - val_loss: 0.4599\nEpoch 185/200\n211/211 [==============================] - 0s 2ms/step - loss: 6.2982e-04 - val_loss: 0.4615\nEpoch 186/200\n211/211 [==============================] - 0s 2ms/step - loss: 6.0407e-04 - val_loss: 0.4614\nEpoch 187/200\n211/211 [==============================] - 0s 2ms/step - loss: 6.0037e-04 - val_loss: 0.4630\nEpoch 188/200\n211/211 [==============================] - 1s 3ms/step - loss: 5.3812e-04 - val_loss: 0.4623\nEpoch 189/200\n211/211 [==============================] - 1s 3ms/step - loss: 5.7122e-04 - val_loss: 0.4724\nEpoch 190/200\n211/211 [==============================] - 1s 3ms/step - loss: 5.5881e-04 - val_loss: 0.4678\nEpoch 191/200\n211/211 [==============================] - 0s 2ms/step - loss: 5.9475e-04 - val_loss: 0.4745\nEpoch 192/200\n211/211 [==============================] - 0s 2ms/step - loss: 6.5761e-04 - val_loss: 0.4754\nEpoch 193/200\n211/211 [==============================] - 1s 3ms/step - loss: 5.1031e-04 - val_loss: 0.4785\nEpoch 194/200\n211/211 [==============================] - 1s 2ms/step - loss: 4.8974e-04 - val_loss: 0.4806\nEpoch 195/200\n211/211 [==============================] - 0s 2ms/step - loss: 4.6926e-04 - val_loss: 0.4796\nEpoch 196/200\n211/211 [==============================] - 0s 2ms/step - loss: 4.9511e-04 - val_loss: 0.4854\nEpoch 197/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.4910\nEpoch 198/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0011 - val_loss: 0.4908\nEpoch 199/200\n211/211 [==============================] - 0s 2ms/step - loss: 4.3322e-04 - val_loss: 0.4907\nEpoch 200/200\n211/211 [==============================] - 0s 2ms/step - loss: 3.8434e-04 - val_loss: 0.4921\n88/88 [==============================] - 0s 1ms/step\nKeras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n...layers\n......dense\n.........vars\n............0\n............1\n......dense_1\n.........vars\n............0\n............1\n......normalization\n.........vars\n............0\n............1\n............2\n...metrics\n......mean\n.........vars\n............0\n............1\n...optimizer\n......vars\n.........0\n.........1\n.........2\n.........3\n.........4\n.........5\n.........6\n.........7\n.........8\n...vars\nKeras model archive saving:\nFile Name                                             Modified             Size\nconfig.json                                    2022-12-10 06:59:02         1990\nvariables.h5                                   2022-12-10 06:59:03        66096\nmetadata.json                                  2022-12-10 06:59:02           64\nKeras model archive loading:\nFile Name                                             Modified             Size\nconfig.json                                    2022-12-10 06:59:02         1990\nvariables.h5                                   2022-12-10 06:59:02        66096\nmetadata.json                                  2022-12-10 06:59:02           64\nEpoch 1/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.4227 - val_loss: 0.2126\nEpoch 2/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1985 - val_loss: 0.1622\nEpoch 3/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1815 - val_loss: 0.1559\nEpoch 4/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1766 - val_loss: 0.1536\nEpoch 5/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1726 - val_loss: 0.1524\nEpoch 6/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1704 - val_loss: 0.1512\nEpoch 7/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.1679 - val_loss: 0.1514\nEpoch 8/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1644 - val_loss: 0.1511\nEpoch 9/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1617 - val_loss: 0.1526\nEpoch 10/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1607 - val_loss: 0.1499\nEpoch 11/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1572 - val_loss: 0.1490\nEpoch 12/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.1558 - val_loss: 0.1485\nEpoch 13/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.1523 - val_loss: 0.1526\nEpoch 14/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1502 - val_loss: 0.1482\nEpoch 15/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1485 - val_loss: 0.1499\nEpoch 16/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1462 - val_loss: 0.1493\nEpoch 17/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.1445 - val_loss: 0.1498\nEpoch 18/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1412 - val_loss: 0.1501\nEpoch 19/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1393 - val_loss: 0.1485\nEpoch 20/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1372 - val_loss: 0.1490\nEpoch 21/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1348 - val_loss: 0.1506\nEpoch 22/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.1318 - val_loss: 0.1525\nEpoch 23/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1298 - val_loss: 0.1509\nEpoch 24/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1270 - val_loss: 0.1521\nEpoch 25/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1252 - val_loss: 0.1529\nEpoch 26/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1226 - val_loss: 0.1576\nEpoch 27/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1208 - val_loss: 0.1590\nEpoch 28/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1181 - val_loss: 0.1545\nEpoch 29/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1151 - val_loss: 0.1564\nEpoch 30/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.1125 - val_loss: 0.1566\nEpoch 31/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.1122 - val_loss: 0.1563\nEpoch 32/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1085 - val_loss: 0.1619\nEpoch 33/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1071 - val_loss: 0.1601\nEpoch 34/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.1050 - val_loss: 0.1611\nEpoch 35/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.1019 - val_loss: 0.1680\nEpoch 36/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0995 - val_loss: 0.1637\nEpoch 37/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0987 - val_loss: 0.1662\nEpoch 38/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0955 - val_loss: 0.1665\nEpoch 39/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0941 - val_loss: 0.1662\nEpoch 40/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0911 - val_loss: 0.1690\nEpoch 41/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0898 - val_loss: 0.1712\nEpoch 42/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0869 - val_loss: 0.1750\nEpoch 43/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0856 - val_loss: 0.1707\nEpoch 44/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0834 - val_loss: 0.1729\nEpoch 45/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0817 - val_loss: 0.1713\nEpoch 46/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0803 - val_loss: 0.1748\nEpoch 47/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0779 - val_loss: 0.1766\nEpoch 48/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0766 - val_loss: 0.1782\nEpoch 49/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0748 - val_loss: 0.1793\nEpoch 50/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0718 - val_loss: 0.1790\nEpoch 51/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0712 - val_loss: 0.1873\nEpoch 52/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0695 - val_loss: 0.1843\nEpoch 53/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0678 - val_loss: 0.1844\nEpoch 54/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0655 - val_loss: 0.1812\nEpoch 55/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0647 - val_loss: 0.1864\nEpoch 56/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0637 - val_loss: 0.1919\nEpoch 57/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0614 - val_loss: 0.1854\nEpoch 58/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0600 - val_loss: 0.1919\nEpoch 59/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0584 - val_loss: 0.1909\nEpoch 60/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0566 - val_loss: 0.1933\nEpoch 61/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0562 - val_loss: 0.1936\nEpoch 62/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0532 - val_loss: 0.1955\nEpoch 63/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0523 - val_loss: 0.1942\nEpoch 64/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0510 - val_loss: 0.1981\nEpoch 65/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0498 - val_loss: 0.1980\nEpoch 66/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0487 - val_loss: 0.2004\nEpoch 67/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0472 - val_loss: 0.2045\nEpoch 68/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0464 - val_loss: 0.2048\nEpoch 69/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0453 - val_loss: 0.2030\nEpoch 70/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0432 - val_loss: 0.2104\nEpoch 71/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0429 - val_loss: 0.2076\nEpoch 72/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0406 - val_loss: 0.2108\nEpoch 73/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0407 - val_loss: 0.2096\nEpoch 74/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0389 - val_loss: 0.2099\nEpoch 75/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0374 - val_loss: 0.2155\nEpoch 76/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0366 - val_loss: 0.2194\nEpoch 77/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0365 - val_loss: 0.2189\nEpoch 78/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0345 - val_loss: 0.2203\nEpoch 79/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0331 - val_loss: 0.2207\nEpoch 80/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0326 - val_loss: 0.2219\nEpoch 81/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0319 - val_loss: 0.2212\nEpoch 82/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0308 - val_loss: 0.2271\nEpoch 83/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0300 - val_loss: 0.2292\nEpoch 84/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0289 - val_loss: 0.2309\nEpoch 85/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0283 - val_loss: 0.2348\nEpoch 86/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0273 - val_loss: 0.2324\nEpoch 87/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0263 - val_loss: 0.2448\nEpoch 88/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0261 - val_loss: 0.2356\nEpoch 89/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0245 - val_loss: 0.2374\nEpoch 90/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0237 - val_loss: 0.2438\nEpoch 91/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0228 - val_loss: 0.2476\nEpoch 92/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0223 - val_loss: 0.2452\nEpoch 93/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0219 - val_loss: 0.2496\nEpoch 94/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0210 - val_loss: 0.2520\nEpoch 95/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0200 - val_loss: 0.2457\nEpoch 96/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0199 - val_loss: 0.2508\nEpoch 97/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0195 - val_loss: 0.2562\nEpoch 98/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0179 - val_loss: 0.2619\nEpoch 99/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0179 - val_loss: 0.2697\nEpoch 100/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0173 - val_loss: 0.2663\nEpoch 101/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0167 - val_loss: 0.2698\nEpoch 102/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0164 - val_loss: 0.2674\nEpoch 103/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0159 - val_loss: 0.2669\nEpoch 104/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0158 - val_loss: 0.2817\nEpoch 105/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0145 - val_loss: 0.2724\nEpoch 106/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0138 - val_loss: 0.2764\nEpoch 107/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0141 - val_loss: 0.2745\nEpoch 108/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0135 - val_loss: 0.2844\nEpoch 109/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0130 - val_loss: 0.2828\nEpoch 110/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0123 - val_loss: 0.2871\nEpoch 111/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0121 - val_loss: 0.2891\nEpoch 112/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0117 - val_loss: 0.2963\nEpoch 113/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0112 - val_loss: 0.2936\nEpoch 114/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0105 - val_loss: 0.2910\nEpoch 115/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0103 - val_loss: 0.2969\nEpoch 116/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0101 - val_loss: 0.3009\nEpoch 117/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0099 - val_loss: 0.3037\nEpoch 118/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0093 - val_loss: 0.3011\nEpoch 119/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0092 - val_loss: 0.3007\nEpoch 120/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0089 - val_loss: 0.3063\nEpoch 121/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0083 - val_loss: 0.3099\nEpoch 122/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0077 - val_loss: 0.3180\nEpoch 123/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0079 - val_loss: 0.3099\nEpoch 124/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0073 - val_loss: 0.3172\nEpoch 125/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0073 - val_loss: 0.3143\nEpoch 126/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0069 - val_loss: 0.3202\nEpoch 127/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0070 - val_loss: 0.3276\nEpoch 128/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0064 - val_loss: 0.3240\nEpoch 129/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0059 - val_loss: 0.3329\nEpoch 130/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0060 - val_loss: 0.3370\nEpoch 131/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0057 - val_loss: 0.3479\nEpoch 132/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0072 - val_loss: 0.3358\nEpoch 133/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0055 - val_loss: 0.3349\nEpoch 134/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0050 - val_loss: 0.3382\nEpoch 135/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0052 - val_loss: 0.3392\nEpoch 136/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0047 - val_loss: 0.3510\nEpoch 137/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0043 - val_loss: 0.3459\nEpoch 138/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0045 - val_loss: 0.3455\nEpoch 139/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0041 - val_loss: 0.3484\nEpoch 140/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0041 - val_loss: 0.3475\nEpoch 141/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0040 - val_loss: 0.3504\nEpoch 142/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0036 - val_loss: 0.3514\nEpoch 143/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0037 - val_loss: 0.3569\nEpoch 144/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0037 - val_loss: 0.3582\nEpoch 145/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0033 - val_loss: 0.3615\nEpoch 146/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0033 - val_loss: 0.3615\nEpoch 147/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0033 - val_loss: 0.3685\nEpoch 148/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0032 - val_loss: 0.3708\nEpoch 149/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0030 - val_loss: 0.3675\nEpoch 150/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0026 - val_loss: 0.3741\nEpoch 151/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0028 - val_loss: 0.3774\nEpoch 152/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0026 - val_loss: 0.3783\nEpoch 153/200\n211/211 [==============================] - 1s 2ms/step - loss: 0.0025 - val_loss: 0.3834\nEpoch 154/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0034 - val_loss: 0.3754\nEpoch 155/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0030 - val_loss: 0.3782\nEpoch 156/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0021 - val_loss: 0.3777\nEpoch 157/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0021 - val_loss: 0.3821\nEpoch 158/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0022 - val_loss: 0.3815\nEpoch 159/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0019 - val_loss: 0.3918\nEpoch 160/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0020 - val_loss: 0.3909\nEpoch 161/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0018 - val_loss: 0.3909\nEpoch 162/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0018 - val_loss: 0.3891\nEpoch 163/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0017 - val_loss: 0.3965\nEpoch 164/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0016 - val_loss: 0.4031\nEpoch 165/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0015 - val_loss: 0.3974\nEpoch 166/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.4023\nEpoch 167/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.4057\nEpoch 168/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.4188\nEpoch 169/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0013 - val_loss: 0.4126\nEpoch 170/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0015 - val_loss: 0.4054\nEpoch 171/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0013 - val_loss: 0.4200\nEpoch 172/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0012 - val_loss: 0.4144\nEpoch 173/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0011 - val_loss: 0.4201\nEpoch 174/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0010 - val_loss: 0.4246\nEpoch 175/200\n211/211 [==============================] - 0s 2ms/step - loss: 9.8741e-04 - val_loss: 0.4263\nEpoch 176/200\n211/211 [==============================] - 1s 3ms/step - loss: 0.0010 - val_loss: 0.4284\nEpoch 177/200\n211/211 [==============================] - 1s 2ms/step - loss: 9.2253e-04 - val_loss: 0.4351\nEpoch 178/200\n211/211 [==============================] - 0s 2ms/step - loss: 9.0728e-04 - val_loss: 0.4288\nEpoch 179/200\n211/211 [==============================] - 0s 2ms/step - loss: 8.2431e-04 - val_loss: 0.4323\nEpoch 180/200\n211/211 [==============================] - 0s 2ms/step - loss: 8.6029e-04 - val_loss: 0.4370\nEpoch 181/200\n211/211 [==============================] - 0s 2ms/step - loss: 9.7339e-04 - val_loss: 0.4360\nEpoch 182/200\n211/211 [==============================] - 0s 2ms/step - loss: 7.9697e-04 - val_loss: 0.4395\nEpoch 183/200\n211/211 [==============================] - 1s 3ms/step - loss: 7.1516e-04 - val_loss: 0.4409\nEpoch 184/200\n211/211 [==============================] - 0s 2ms/step - loss: 6.9561e-04 - val_loss: 0.4447\nEpoch 185/200\n211/211 [==============================] - 1s 3ms/step - loss: 6.2967e-04 - val_loss: 0.4465\nEpoch 186/200\n211/211 [==============================] - 1s 3ms/step - loss: 5.8785e-04 - val_loss: 0.4487\nEpoch 187/200\n211/211 [==============================] - 0s 2ms/step - loss: 5.7284e-04 - val_loss: 0.4521\nEpoch 188/200\n211/211 [==============================] - 1s 2ms/step - loss: 5.7711e-04 - val_loss: 0.4524\nEpoch 189/200\n211/211 [==============================] - 1s 2ms/step - loss: 5.4802e-04 - val_loss: 0.4557\nEpoch 190/200\n211/211 [==============================] - 1s 3ms/step - loss: 5.0956e-04 - val_loss: 0.4575\nEpoch 191/200\n211/211 [==============================] - 1s 2ms/step - loss: 5.0317e-04 - val_loss: 0.4564\nEpoch 192/200\n211/211 [==============================] - 1s 2ms/step - loss: 6.4447e-04 - val_loss: 0.4695\nEpoch 193/200\n211/211 [==============================] - 1s 2ms/step - loss: 8.1065e-04 - val_loss: 0.4638\nEpoch 194/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0026 - val_loss: 0.4650\nEpoch 195/200\n211/211 [==============================] - 0s 2ms/step - loss: 0.0014 - val_loss: 0.4732\nEpoch 196/200\n211/211 [==============================] - 1s 2ms/step - loss: 5.2331e-04 - val_loss: 0.4733\nEpoch 197/200\n211/211 [==============================] - 0s 2ms/step - loss: 4.1229e-04 - val_loss: 0.4717\nEpoch 198/200\n211/211 [==============================] - 0s 2ms/step - loss: 3.7999e-04 - val_loss: 0.4708\nEpoch 199/200\n211/211 [==============================] - 1s 2ms/step - loss: 3.6506e-04 - val_loss: 0.4702\nEpoch 200/200\n211/211 [==============================] - 1s 2ms/step - loss: 3.6734e-04 - val_loss: 0.4704\n88/88 [==============================] - 0s 1ms/step\n     .. fold 4 trained/predicted\n     . Optimising model hyperparameters\n     .. fold 2 trained/predicted\n     .. fold 4 trained/predicted\n     . Optimising model hyperparameters\n     .. fold 2 trained/predicted\n     .. fold 4 trained/predicted\n     . Optimising model hyperparameters\n     .. fold 2 trained/predicted\n     .. fold 4 trained/predicted\n     . Optimising model hyperparameters\n     .. fold 2 trained/predicted\n     .. fold 4 trained/predicted\n     . Optimising model hyperparameters\n     .. fold 2 trained/predicted\n     .. fold 4 trained/predicted\n     . Optimising model hyperparameters\n     .. fold 2 trained/predicted\n     .. fold 4 trained/predicted\n     . Optimising model hyperparameters\n     .. fold 2 trained/predicted\n     .. fold 4 trained/predicted\n     . Optimising model hyperparameters\n     .. fold 2 trained/predicted\n     .. fold 4 trained/predicted\n\n   - Training/predicting with layer_2 models\n     . Optimising model hyperparameters\n     .. fold 2 trained/predicted\n     .. fold 4 trained/predicted\n     . Optimising model hyperparameters\n     .. fold 2 trained/predicted\n     .. fold 4 trained/predicted\n\nTime elapsed for fit_transform execution: 28 min 6.64 sec\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"print(stacked_train_1.shape)\nstacked_train_1.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-12-10T07:03:08.104942Z","iopub.execute_input":"2022-12-10T07:03:08.105996Z","iopub.status.idle":"2022-12-10T07:03:08.242548Z","shell.execute_reply.started":"2022-12-10T07:03:08.105914Z","shell.execute_reply":"2022-12-10T07:03:08.241738Z"},"trusted":true},"outputs":[{"name":"stdout","text":"(11253, 167)\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"   ANONYMOUS_1      YEAR  SAMPLE_TRANSFER_DAY  ANONYMOUS_2       AG        AL  \\\n0    -0.023281 -1.173427             1.711478     1.034058 -0.15342 -0.117202   \n1    -1.105403 -0.416834            -0.731397    -0.623976 -0.15342  0.347832   \n2     0.153575 -0.164636             0.659389    -0.623976 -0.15342  3.425744   \n\n          B        BA        BE        CA        CD        CO        CR  \\\n0 -0.641141 -0.494248 -0.054482  0.592335 -0.103225 -0.139071 -0.894855   \n1  1.110267  0.748363 -0.054482  1.230336 -0.103225 -0.139071  0.707267   \n2  1.049182 -0.494248 -0.054482 -0.882156 -0.103225 -0.139071 -0.093794   \n\n         CU      FH2O      FNOX  FOPTIMETHGLY     FOXID      FSO4     FTBN  \\\n0  1.969028  0.142333  0.015145     -0.393591 -0.019127 -0.010975 -0.12846   \n1  1.592205  0.142333  0.015145     -0.393591 -0.019127 -0.010975 -0.12846   \n2 -0.255395  0.142333  0.015145     -0.393591 -0.019127 -0.010975 -0.12846   \n\n         FE      FUEL       H2O         K        LI        MG        MN  \\\n0 -0.556427 -0.132286 -0.097601 -0.931546 -0.260909 -1.307430 -0.788343   \n1  1.981607 -0.132286 -0.097601  0.324543 -0.260909  0.869967  3.184068   \n2  0.749926 -0.132286 -0.097601  0.653461 -0.260909 -0.369677  0.439143   \n\n         MO        NA        NI         P        PB   PQINDEX         S  \\\n0 -0.372594 -1.217035 -0.445175 -0.759178  0.257713 -0.800756 -1.148595   \n1  0.800960  0.023809  1.546017  0.590266  0.257713  2.409786 -0.435880   \n2 -0.776308  0.600769 -0.445175  1.728853 -0.695598  0.886489  2.487035   \n\n         SB        SI        SN  SOOTPERCENTAGE        TI      U100       U75  \\\n0 -0.457625 -1.689316 -0.550961       -0.463603 -0.264231 -0.109683 -0.153248   \n1  1.083268  0.631473  2.363609       -0.463603 -0.264231 -0.109683 -0.153248   \n2 -0.457625  0.631473 -0.550961       -0.463603 -0.264231 -0.109683 -0.153248   \n\n        U50       U25       U20       U14        U6        U4         V  \\\n0 -0.258734  1.702452  1.467534  2.480778  2.284467  2.529955 -0.154662   \n1 -0.258734 -0.377342 -0.391875 -0.393032 -0.411170 -0.416807 -0.154662   \n2 -0.258734 -0.377342 -0.391875 -0.393032 -0.411170 -0.416807 -0.154662   \n\n       V100       V40        ZN  COMPONENT_ARBITRARY_COMPONENT1  \\\n0 -0.057476 -1.927948  0.118657                               0   \n1 -0.057476 -1.570778  1.369311                               0   \n2 -0.057476  0.724583 -1.054889                               0   \n\n   COMPONENT_ARBITRARY_COMPONENT2  COMPONENT_ARBITRARY_COMPONENT3  \\\n0                               1                               0   \n1                               0                               1   \n2                               0                               1   \n\n   COMPONENT_ARBITRARY_COMPONENT4  layer_1_0  layer_1_1  layer_1_2  \\\n0                               0   0.050038   0.101665   0.013456   \n1                               0   0.011131   0.012259   0.003201   \n2                               0   0.931095   0.913152   0.991538   \n\n      layer_1_3  layer_1_4     layer_1_5  layer_1_6  layer_1_7  layer_1_8  \\\n0  1.142380e-05        1.0  4.921171e-05   0.000000   0.046065   0.034407   \n1  4.275239e-13        1.0  2.142181e-13   0.013793   0.065538   0.012406   \n2  1.000000e+00        1.0  9.996576e-01   1.000000   0.596329   0.942769   \n\n   layer_1_9  layer_1_10  layer_1_11  layer_1_12  layer_1_13  \\\n0   0.058801     0.08666         0.0         0.0    0.015896   \n1   0.059710     0.00000         0.0         0.0    0.011605   \n2   0.526827     0.29385         1.0         1.0    1.000000   \n\n   diff_layer_1_0_layer_1_1  diff_layer_1_0_layer_1_2  \\\n0                 -0.051627                  0.036582   \n1                 -0.001128                  0.007931   \n2                  0.017943                 -0.060444   \n\n   diff_layer_1_0_layer_1_3  diff_layer_1_0_layer_1_4  \\\n0                  0.050026                 -0.949962   \n1                  0.011131                 -0.988869   \n2                 -0.068905                 -0.068905   \n\n   diff_layer_1_0_layer_1_5  diff_layer_1_0_layer_1_6  \\\n0                  0.049989                  0.050038   \n1                  0.011131                 -0.002662   \n2                 -0.068563                 -0.068905   \n\n   diff_layer_1_0_layer_1_7  diff_layer_1_0_layer_1_8  \\\n0                  0.003973                  0.015631   \n1                 -0.054407                 -0.001275   \n2                  0.334766                 -0.011674   \n\n   diff_layer_1_0_layer_1_9  diff_layer_1_0_layer_1_10  \\\n0                 -0.008764                  -0.036622   \n1                 -0.048579                   0.011131   \n2                  0.404268                   0.637245   \n\n   diff_layer_1_0_layer_1_11  diff_layer_1_0_layer_1_12  \\\n0                   0.050038                   0.050038   \n1                   0.011131                   0.011131   \n2                  -0.068905                  -0.068905   \n\n   diff_layer_1_0_layer_1_13  diff_layer_1_1_layer_1_2  \\\n0                   0.034142                  0.088208   \n1                  -0.000474                  0.009059   \n2                  -0.068905                 -0.078387   \n\n   diff_layer_1_1_layer_1_3  diff_layer_1_1_layer_1_4  \\\n0                  0.101653                 -0.898335   \n1                  0.012259                 -0.987741   \n2                 -0.086848                 -0.086848   \n\n   diff_layer_1_1_layer_1_5  diff_layer_1_1_layer_1_6  \\\n0                  0.101615                  0.101665   \n1                  0.012259                 -0.001534   \n2                 -0.086506                 -0.086848   \n\n   diff_layer_1_1_layer_1_7  diff_layer_1_1_layer_1_8  \\\n0                  0.055600                  0.067258   \n1                 -0.053279                 -0.000147   \n2                  0.316823                 -0.029617   \n\n   diff_layer_1_1_layer_1_9  diff_layer_1_1_layer_1_10  \\\n0                  0.042863                   0.015005   \n1                 -0.047451                   0.012259   \n2                  0.386324                   0.619301   \n\n   diff_layer_1_1_layer_1_11  diff_layer_1_1_layer_1_12  \\\n0                   0.101665                   0.101665   \n1                   0.012259                   0.012259   \n2                  -0.086848                  -0.086848   \n\n   diff_layer_1_1_layer_1_13  diff_layer_1_2_layer_1_3  \\\n0                   0.085769                  0.013445   \n1                   0.000654                  0.003201   \n2                  -0.086848                 -0.008462   \n\n   diff_layer_1_2_layer_1_4  diff_layer_1_2_layer_1_5  \\\n0                 -0.986544                  0.013407   \n1                 -0.996799                  0.003201   \n2                 -0.008462                 -0.008119   \n\n   diff_layer_1_2_layer_1_6  diff_layer_1_2_layer_1_7  \\\n0                  0.013456                 -0.032609   \n1                 -0.010592                 -0.062337   \n2                 -0.008462                  0.395210   \n\n   diff_layer_1_2_layer_1_8  diff_layer_1_2_layer_1_9  \\\n0                 -0.020951                 -0.045345   \n1                 -0.009205                 -0.056509   \n2                  0.048770                  0.464711   \n\n   diff_layer_1_2_layer_1_10  diff_layer_1_2_layer_1_11  \\\n0                  -0.073204                   0.013456   \n1                   0.003201                   0.003201   \n2                   0.697688                  -0.008462   \n\n   diff_layer_1_2_layer_1_12  diff_layer_1_2_layer_1_13  \\\n0                   0.013456                  -0.002440   \n1                   0.003201                  -0.008405   \n2                  -0.008462                  -0.008462   \n\n   diff_layer_1_3_layer_1_4  diff_layer_1_3_layer_1_5  \\\n0                 -0.999989             -3.778791e-05   \n1                 -1.000000              2.133057e-13   \n2                  0.000000              3.423691e-04   \n\n   diff_layer_1_3_layer_1_6  diff_layer_1_3_layer_1_7  \\\n0                  0.000011                 -0.046053   \n1                 -0.013793                 -0.065538   \n2                  0.000000                  0.403671   \n\n   diff_layer_1_3_layer_1_8  diff_layer_1_3_layer_1_9  \\\n0                 -0.034396                 -0.058790   \n1                 -0.012406                 -0.059710   \n2                  0.057231                  0.473173   \n\n   diff_layer_1_3_layer_1_10  diff_layer_1_3_layer_1_11  \\\n0              -8.664836e-02               1.142380e-05   \n1               4.275239e-13               4.275239e-13   \n2               7.061496e-01               0.000000e+00   \n\n   diff_layer_1_3_layer_1_12  diff_layer_1_3_layer_1_13  \\\n0               1.142380e-05                  -0.015885   \n1               4.275239e-13                  -0.011605   \n2               0.000000e+00                   0.000000   \n\n   diff_layer_1_4_layer_1_5  diff_layer_1_4_layer_1_6  \\\n0                  0.999951                  1.000000   \n1                  1.000000                  0.986207   \n2                  0.000342                  0.000000   \n\n   diff_layer_1_4_layer_1_7  diff_layer_1_4_layer_1_8  \\\n0                  0.953935                  0.965593   \n1                  0.934462                  0.987594   \n2                  0.403671                  0.057231   \n\n   diff_layer_1_4_layer_1_9  diff_layer_1_4_layer_1_10  \\\n0                  0.941199                    0.91334   \n1                  0.940290                    1.00000   \n2                  0.473173                    0.70615   \n\n   diff_layer_1_4_layer_1_11  diff_layer_1_4_layer_1_12  \\\n0                        1.0                        1.0   \n1                        1.0                        1.0   \n2                        0.0                        0.0   \n\n   diff_layer_1_4_layer_1_13  diff_layer_1_5_layer_1_6  \\\n0                   0.984104                  0.000049   \n1                   0.988395                 -0.013793   \n2                   0.000000                 -0.000342   \n\n   diff_layer_1_5_layer_1_7  diff_layer_1_5_layer_1_8  \\\n0                 -0.046016                 -0.034358   \n1                 -0.065538                 -0.012406   \n2                  0.403329                  0.056889   \n\n   diff_layer_1_5_layer_1_9  diff_layer_1_5_layer_1_10  \\\n0                 -0.058752              -8.661057e-02   \n1                 -0.059710               2.142181e-13   \n2                  0.472831               7.058073e-01   \n\n   diff_layer_1_5_layer_1_11  diff_layer_1_5_layer_1_12  \\\n0               4.921171e-05               4.921171e-05   \n1               2.142181e-13               2.142181e-13   \n2              -3.423691e-04              -3.423691e-04   \n\n   diff_layer_1_5_layer_1_13  diff_layer_1_6_layer_1_7  \\\n0                  -0.015847                 -0.046065   \n1                  -0.011605                 -0.051745   \n2                  -0.000342                  0.403671   \n\n   diff_layer_1_6_layer_1_8  diff_layer_1_6_layer_1_9  \\\n0                 -0.034407                 -0.058801   \n1                  0.001387                 -0.045917   \n2                  0.057231                  0.473173   \n\n   diff_layer_1_6_layer_1_10  diff_layer_1_6_layer_1_11  \\\n0                  -0.086660                   0.000000   \n1                   0.013793                   0.013793   \n2                   0.706150                   0.000000   \n\n   diff_layer_1_6_layer_1_12  diff_layer_1_6_layer_1_13  \\\n0                   0.000000                  -0.015896   \n1                   0.013793                   0.002188   \n2                   0.000000                   0.000000   \n\n   diff_layer_1_7_layer_1_8  diff_layer_1_7_layer_1_9  \\\n0                  0.011658                 -0.012736   \n1                  0.053132                  0.005828   \n2                 -0.346440                  0.069501   \n\n   diff_layer_1_7_layer_1_10  diff_layer_1_7_layer_1_11  \\\n0                  -0.040595                   0.046065   \n1                   0.065538                   0.065538   \n2                   0.302478                  -0.403671   \n\n   diff_layer_1_7_layer_1_12  diff_layer_1_7_layer_1_13  \\\n0                   0.046065                   0.030169   \n1                   0.065538                   0.053932   \n2                  -0.403671                  -0.403671   \n\n   diff_layer_1_8_layer_1_9  diff_layer_1_8_layer_1_10  \\\n0                 -0.024394                  -0.052253   \n1                 -0.047304                   0.012406   \n2                  0.415941                   0.648918   \n\n   diff_layer_1_8_layer_1_11  diff_layer_1_8_layer_1_12  \\\n0                   0.034407                   0.034407   \n1                   0.012406                   0.012406   \n2                  -0.057231                  -0.057231   \n\n   diff_layer_1_8_layer_1_13  diff_layer_1_9_layer_1_10  \\\n0                   0.018511                  -0.027859   \n1                   0.000800                   0.059710   \n2                  -0.057231                   0.232977   \n\n   diff_layer_1_9_layer_1_11  diff_layer_1_9_layer_1_12  \\\n0                   0.058801                   0.058801   \n1                   0.059710                   0.059710   \n2                  -0.473173                  -0.473173   \n\n   diff_layer_1_9_layer_1_13  diff_layer_1_10_layer_1_11  \\\n0                   0.042905                     0.08666   \n1                   0.048104                     0.00000   \n2                  -0.473173                    -0.70615   \n\n   diff_layer_1_10_layer_1_12  diff_layer_1_10_layer_1_13  \\\n0                     0.08666                    0.070764   \n1                     0.00000                   -0.011605   \n2                    -0.70615                   -0.706150   \n\n   diff_layer_1_11_layer_1_12  diff_layer_1_11_layer_1_13  \\\n0                         0.0                   -0.015896   \n1                         0.0                   -0.011605   \n2                         0.0                    0.000000   \n\n   diff_layer_1_12_layer_1_13  layer_1_std  layer_1_mean  layer_2_0  \\\n0                   -0.015896     0.261083      0.100503   0.078065   \n1                   -0.011605     0.264216      0.084975   0.010991   \n2                    0.000000     0.226761      0.871087   0.985101   \n\n   layer_2_1  diff_layer_2_0_layer_2_1  layer_2_std  layer_2_mean  \n0   0.034265                  0.043800     0.030971      0.056165  \n1   0.030872                 -0.019880     0.014057      0.020932  \n2   0.991199                 -0.006097     0.004311      0.988150  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ANONYMOUS_1</th>\n      <th>YEAR</th>\n      <th>SAMPLE_TRANSFER_DAY</th>\n      <th>ANONYMOUS_2</th>\n      <th>AG</th>\n      <th>AL</th>\n      <th>B</th>\n      <th>BA</th>\n      <th>BE</th>\n      <th>CA</th>\n      <th>CD</th>\n      <th>CO</th>\n      <th>CR</th>\n      <th>CU</th>\n      <th>FH2O</th>\n      <th>FNOX</th>\n      <th>FOPTIMETHGLY</th>\n      <th>FOXID</th>\n      <th>FSO4</th>\n      <th>FTBN</th>\n      <th>FE</th>\n      <th>FUEL</th>\n      <th>H2O</th>\n      <th>K</th>\n      <th>LI</th>\n      <th>MG</th>\n      <th>MN</th>\n      <th>MO</th>\n      <th>NA</th>\n      <th>NI</th>\n      <th>P</th>\n      <th>PB</th>\n      <th>PQINDEX</th>\n      <th>S</th>\n      <th>SB</th>\n      <th>SI</th>\n      <th>SN</th>\n      <th>SOOTPERCENTAGE</th>\n      <th>TI</th>\n      <th>U100</th>\n      <th>U75</th>\n      <th>U50</th>\n      <th>U25</th>\n      <th>U20</th>\n      <th>U14</th>\n      <th>U6</th>\n      <th>U4</th>\n      <th>V</th>\n      <th>V100</th>\n      <th>V40</th>\n      <th>ZN</th>\n      <th>COMPONENT_ARBITRARY_COMPONENT1</th>\n      <th>COMPONENT_ARBITRARY_COMPONENT2</th>\n      <th>COMPONENT_ARBITRARY_COMPONENT3</th>\n      <th>COMPONENT_ARBITRARY_COMPONENT4</th>\n      <th>layer_1_0</th>\n      <th>layer_1_1</th>\n      <th>layer_1_2</th>\n      <th>layer_1_3</th>\n      <th>layer_1_4</th>\n      <th>layer_1_5</th>\n      <th>layer_1_6</th>\n      <th>layer_1_7</th>\n      <th>layer_1_8</th>\n      <th>layer_1_9</th>\n      <th>layer_1_10</th>\n      <th>layer_1_11</th>\n      <th>layer_1_12</th>\n      <th>layer_1_13</th>\n      <th>diff_layer_1_0_layer_1_1</th>\n      <th>diff_layer_1_0_layer_1_2</th>\n      <th>diff_layer_1_0_layer_1_3</th>\n      <th>diff_layer_1_0_layer_1_4</th>\n      <th>diff_layer_1_0_layer_1_5</th>\n      <th>diff_layer_1_0_layer_1_6</th>\n      <th>diff_layer_1_0_layer_1_7</th>\n      <th>diff_layer_1_0_layer_1_8</th>\n      <th>diff_layer_1_0_layer_1_9</th>\n      <th>diff_layer_1_0_layer_1_10</th>\n      <th>diff_layer_1_0_layer_1_11</th>\n      <th>diff_layer_1_0_layer_1_12</th>\n      <th>diff_layer_1_0_layer_1_13</th>\n      <th>diff_layer_1_1_layer_1_2</th>\n      <th>diff_layer_1_1_layer_1_3</th>\n      <th>diff_layer_1_1_layer_1_4</th>\n      <th>diff_layer_1_1_layer_1_5</th>\n      <th>diff_layer_1_1_layer_1_6</th>\n      <th>diff_layer_1_1_layer_1_7</th>\n      <th>diff_layer_1_1_layer_1_8</th>\n      <th>diff_layer_1_1_layer_1_9</th>\n      <th>diff_layer_1_1_layer_1_10</th>\n      <th>diff_layer_1_1_layer_1_11</th>\n      <th>diff_layer_1_1_layer_1_12</th>\n      <th>diff_layer_1_1_layer_1_13</th>\n      <th>diff_layer_1_2_layer_1_3</th>\n      <th>diff_layer_1_2_layer_1_4</th>\n      <th>diff_layer_1_2_layer_1_5</th>\n      <th>diff_layer_1_2_layer_1_6</th>\n      <th>diff_layer_1_2_layer_1_7</th>\n      <th>diff_layer_1_2_layer_1_8</th>\n      <th>diff_layer_1_2_layer_1_9</th>\n      <th>diff_layer_1_2_layer_1_10</th>\n      <th>diff_layer_1_2_layer_1_11</th>\n      <th>diff_layer_1_2_layer_1_12</th>\n      <th>diff_layer_1_2_layer_1_13</th>\n      <th>diff_layer_1_3_layer_1_4</th>\n      <th>diff_layer_1_3_layer_1_5</th>\n      <th>diff_layer_1_3_layer_1_6</th>\n      <th>diff_layer_1_3_layer_1_7</th>\n      <th>diff_layer_1_3_layer_1_8</th>\n      <th>diff_layer_1_3_layer_1_9</th>\n      <th>diff_layer_1_3_layer_1_10</th>\n      <th>diff_layer_1_3_layer_1_11</th>\n      <th>diff_layer_1_3_layer_1_12</th>\n      <th>diff_layer_1_3_layer_1_13</th>\n      <th>diff_layer_1_4_layer_1_5</th>\n      <th>diff_layer_1_4_layer_1_6</th>\n      <th>diff_layer_1_4_layer_1_7</th>\n      <th>diff_layer_1_4_layer_1_8</th>\n      <th>diff_layer_1_4_layer_1_9</th>\n      <th>diff_layer_1_4_layer_1_10</th>\n      <th>diff_layer_1_4_layer_1_11</th>\n      <th>diff_layer_1_4_layer_1_12</th>\n      <th>diff_layer_1_4_layer_1_13</th>\n      <th>diff_layer_1_5_layer_1_6</th>\n      <th>diff_layer_1_5_layer_1_7</th>\n      <th>diff_layer_1_5_layer_1_8</th>\n      <th>diff_layer_1_5_layer_1_9</th>\n      <th>diff_layer_1_5_layer_1_10</th>\n      <th>diff_layer_1_5_layer_1_11</th>\n      <th>diff_layer_1_5_layer_1_12</th>\n      <th>diff_layer_1_5_layer_1_13</th>\n      <th>diff_layer_1_6_layer_1_7</th>\n      <th>diff_layer_1_6_layer_1_8</th>\n      <th>diff_layer_1_6_layer_1_9</th>\n      <th>diff_layer_1_6_layer_1_10</th>\n      <th>diff_layer_1_6_layer_1_11</th>\n      <th>diff_layer_1_6_layer_1_12</th>\n      <th>diff_layer_1_6_layer_1_13</th>\n      <th>diff_layer_1_7_layer_1_8</th>\n      <th>diff_layer_1_7_layer_1_9</th>\n      <th>diff_layer_1_7_layer_1_10</th>\n      <th>diff_layer_1_7_layer_1_11</th>\n      <th>diff_layer_1_7_layer_1_12</th>\n      <th>diff_layer_1_7_layer_1_13</th>\n      <th>diff_layer_1_8_layer_1_9</th>\n      <th>diff_layer_1_8_layer_1_10</th>\n      <th>diff_layer_1_8_layer_1_11</th>\n      <th>diff_layer_1_8_layer_1_12</th>\n      <th>diff_layer_1_8_layer_1_13</th>\n      <th>diff_layer_1_9_layer_1_10</th>\n      <th>diff_layer_1_9_layer_1_11</th>\n      <th>diff_layer_1_9_layer_1_12</th>\n      <th>diff_layer_1_9_layer_1_13</th>\n      <th>diff_layer_1_10_layer_1_11</th>\n      <th>diff_layer_1_10_layer_1_12</th>\n      <th>diff_layer_1_10_layer_1_13</th>\n      <th>diff_layer_1_11_layer_1_12</th>\n      <th>diff_layer_1_11_layer_1_13</th>\n      <th>diff_layer_1_12_layer_1_13</th>\n      <th>layer_1_std</th>\n      <th>layer_1_mean</th>\n      <th>layer_2_0</th>\n      <th>layer_2_1</th>\n      <th>diff_layer_2_0_layer_2_1</th>\n      <th>layer_2_std</th>\n      <th>layer_2_mean</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.023281</td>\n      <td>-1.173427</td>\n      <td>1.711478</td>\n      <td>1.034058</td>\n      <td>-0.15342</td>\n      <td>-0.117202</td>\n      <td>-0.641141</td>\n      <td>-0.494248</td>\n      <td>-0.054482</td>\n      <td>0.592335</td>\n      <td>-0.103225</td>\n      <td>-0.139071</td>\n      <td>-0.894855</td>\n      <td>1.969028</td>\n      <td>0.142333</td>\n      <td>0.015145</td>\n      <td>-0.393591</td>\n      <td>-0.019127</td>\n      <td>-0.010975</td>\n      <td>-0.12846</td>\n      <td>-0.556427</td>\n      <td>-0.132286</td>\n      <td>-0.097601</td>\n      <td>-0.931546</td>\n      <td>-0.260909</td>\n      <td>-1.307430</td>\n      <td>-0.788343</td>\n      <td>-0.372594</td>\n      <td>-1.217035</td>\n      <td>-0.445175</td>\n      <td>-0.759178</td>\n      <td>0.257713</td>\n      <td>-0.800756</td>\n      <td>-1.148595</td>\n      <td>-0.457625</td>\n      <td>-1.689316</td>\n      <td>-0.550961</td>\n      <td>-0.463603</td>\n      <td>-0.264231</td>\n      <td>-0.109683</td>\n      <td>-0.153248</td>\n      <td>-0.258734</td>\n      <td>1.702452</td>\n      <td>1.467534</td>\n      <td>2.480778</td>\n      <td>2.284467</td>\n      <td>2.529955</td>\n      <td>-0.154662</td>\n      <td>-0.057476</td>\n      <td>-1.927948</td>\n      <td>0.118657</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.050038</td>\n      <td>0.101665</td>\n      <td>0.013456</td>\n      <td>1.142380e-05</td>\n      <td>1.0</td>\n      <td>4.921171e-05</td>\n      <td>0.000000</td>\n      <td>0.046065</td>\n      <td>0.034407</td>\n      <td>0.058801</td>\n      <td>0.08666</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.015896</td>\n      <td>-0.051627</td>\n      <td>0.036582</td>\n      <td>0.050026</td>\n      <td>-0.949962</td>\n      <td>0.049989</td>\n      <td>0.050038</td>\n      <td>0.003973</td>\n      <td>0.015631</td>\n      <td>-0.008764</td>\n      <td>-0.036622</td>\n      <td>0.050038</td>\n      <td>0.050038</td>\n      <td>0.034142</td>\n      <td>0.088208</td>\n      <td>0.101653</td>\n      <td>-0.898335</td>\n      <td>0.101615</td>\n      <td>0.101665</td>\n      <td>0.055600</td>\n      <td>0.067258</td>\n      <td>0.042863</td>\n      <td>0.015005</td>\n      <td>0.101665</td>\n      <td>0.101665</td>\n      <td>0.085769</td>\n      <td>0.013445</td>\n      <td>-0.986544</td>\n      <td>0.013407</td>\n      <td>0.013456</td>\n      <td>-0.032609</td>\n      <td>-0.020951</td>\n      <td>-0.045345</td>\n      <td>-0.073204</td>\n      <td>0.013456</td>\n      <td>0.013456</td>\n      <td>-0.002440</td>\n      <td>-0.999989</td>\n      <td>-3.778791e-05</td>\n      <td>0.000011</td>\n      <td>-0.046053</td>\n      <td>-0.034396</td>\n      <td>-0.058790</td>\n      <td>-8.664836e-02</td>\n      <td>1.142380e-05</td>\n      <td>1.142380e-05</td>\n      <td>-0.015885</td>\n      <td>0.999951</td>\n      <td>1.000000</td>\n      <td>0.953935</td>\n      <td>0.965593</td>\n      <td>0.941199</td>\n      <td>0.91334</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.984104</td>\n      <td>0.000049</td>\n      <td>-0.046016</td>\n      <td>-0.034358</td>\n      <td>-0.058752</td>\n      <td>-8.661057e-02</td>\n      <td>4.921171e-05</td>\n      <td>4.921171e-05</td>\n      <td>-0.015847</td>\n      <td>-0.046065</td>\n      <td>-0.034407</td>\n      <td>-0.058801</td>\n      <td>-0.086660</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>-0.015896</td>\n      <td>0.011658</td>\n      <td>-0.012736</td>\n      <td>-0.040595</td>\n      <td>0.046065</td>\n      <td>0.046065</td>\n      <td>0.030169</td>\n      <td>-0.024394</td>\n      <td>-0.052253</td>\n      <td>0.034407</td>\n      <td>0.034407</td>\n      <td>0.018511</td>\n      <td>-0.027859</td>\n      <td>0.058801</td>\n      <td>0.058801</td>\n      <td>0.042905</td>\n      <td>0.08666</td>\n      <td>0.08666</td>\n      <td>0.070764</td>\n      <td>0.0</td>\n      <td>-0.015896</td>\n      <td>-0.015896</td>\n      <td>0.261083</td>\n      <td>0.100503</td>\n      <td>0.078065</td>\n      <td>0.034265</td>\n      <td>0.043800</td>\n      <td>0.030971</td>\n      <td>0.056165</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-1.105403</td>\n      <td>-0.416834</td>\n      <td>-0.731397</td>\n      <td>-0.623976</td>\n      <td>-0.15342</td>\n      <td>0.347832</td>\n      <td>1.110267</td>\n      <td>0.748363</td>\n      <td>-0.054482</td>\n      <td>1.230336</td>\n      <td>-0.103225</td>\n      <td>-0.139071</td>\n      <td>0.707267</td>\n      <td>1.592205</td>\n      <td>0.142333</td>\n      <td>0.015145</td>\n      <td>-0.393591</td>\n      <td>-0.019127</td>\n      <td>-0.010975</td>\n      <td>-0.12846</td>\n      <td>1.981607</td>\n      <td>-0.132286</td>\n      <td>-0.097601</td>\n      <td>0.324543</td>\n      <td>-0.260909</td>\n      <td>0.869967</td>\n      <td>3.184068</td>\n      <td>0.800960</td>\n      <td>0.023809</td>\n      <td>1.546017</td>\n      <td>0.590266</td>\n      <td>0.257713</td>\n      <td>2.409786</td>\n      <td>-0.435880</td>\n      <td>1.083268</td>\n      <td>0.631473</td>\n      <td>2.363609</td>\n      <td>-0.463603</td>\n      <td>-0.264231</td>\n      <td>-0.109683</td>\n      <td>-0.153248</td>\n      <td>-0.258734</td>\n      <td>-0.377342</td>\n      <td>-0.391875</td>\n      <td>-0.393032</td>\n      <td>-0.411170</td>\n      <td>-0.416807</td>\n      <td>-0.154662</td>\n      <td>-0.057476</td>\n      <td>-1.570778</td>\n      <td>1.369311</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.011131</td>\n      <td>0.012259</td>\n      <td>0.003201</td>\n      <td>4.275239e-13</td>\n      <td>1.0</td>\n      <td>2.142181e-13</td>\n      <td>0.013793</td>\n      <td>0.065538</td>\n      <td>0.012406</td>\n      <td>0.059710</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.011605</td>\n      <td>-0.001128</td>\n      <td>0.007931</td>\n      <td>0.011131</td>\n      <td>-0.988869</td>\n      <td>0.011131</td>\n      <td>-0.002662</td>\n      <td>-0.054407</td>\n      <td>-0.001275</td>\n      <td>-0.048579</td>\n      <td>0.011131</td>\n      <td>0.011131</td>\n      <td>0.011131</td>\n      <td>-0.000474</td>\n      <td>0.009059</td>\n      <td>0.012259</td>\n      <td>-0.987741</td>\n      <td>0.012259</td>\n      <td>-0.001534</td>\n      <td>-0.053279</td>\n      <td>-0.000147</td>\n      <td>-0.047451</td>\n      <td>0.012259</td>\n      <td>0.012259</td>\n      <td>0.012259</td>\n      <td>0.000654</td>\n      <td>0.003201</td>\n      <td>-0.996799</td>\n      <td>0.003201</td>\n      <td>-0.010592</td>\n      <td>-0.062337</td>\n      <td>-0.009205</td>\n      <td>-0.056509</td>\n      <td>0.003201</td>\n      <td>0.003201</td>\n      <td>0.003201</td>\n      <td>-0.008405</td>\n      <td>-1.000000</td>\n      <td>2.133057e-13</td>\n      <td>-0.013793</td>\n      <td>-0.065538</td>\n      <td>-0.012406</td>\n      <td>-0.059710</td>\n      <td>4.275239e-13</td>\n      <td>4.275239e-13</td>\n      <td>4.275239e-13</td>\n      <td>-0.011605</td>\n      <td>1.000000</td>\n      <td>0.986207</td>\n      <td>0.934462</td>\n      <td>0.987594</td>\n      <td>0.940290</td>\n      <td>1.00000</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.988395</td>\n      <td>-0.013793</td>\n      <td>-0.065538</td>\n      <td>-0.012406</td>\n      <td>-0.059710</td>\n      <td>2.142181e-13</td>\n      <td>2.142181e-13</td>\n      <td>2.142181e-13</td>\n      <td>-0.011605</td>\n      <td>-0.051745</td>\n      <td>0.001387</td>\n      <td>-0.045917</td>\n      <td>0.013793</td>\n      <td>0.013793</td>\n      <td>0.013793</td>\n      <td>0.002188</td>\n      <td>0.053132</td>\n      <td>0.005828</td>\n      <td>0.065538</td>\n      <td>0.065538</td>\n      <td>0.065538</td>\n      <td>0.053932</td>\n      <td>-0.047304</td>\n      <td>0.012406</td>\n      <td>0.012406</td>\n      <td>0.012406</td>\n      <td>0.000800</td>\n      <td>0.059710</td>\n      <td>0.059710</td>\n      <td>0.059710</td>\n      <td>0.048104</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>-0.011605</td>\n      <td>0.0</td>\n      <td>-0.011605</td>\n      <td>-0.011605</td>\n      <td>0.264216</td>\n      <td>0.084975</td>\n      <td>0.010991</td>\n      <td>0.030872</td>\n      <td>-0.019880</td>\n      <td>0.014057</td>\n      <td>0.020932</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.153575</td>\n      <td>-0.164636</td>\n      <td>0.659389</td>\n      <td>-0.623976</td>\n      <td>-0.15342</td>\n      <td>3.425744</td>\n      <td>1.049182</td>\n      <td>-0.494248</td>\n      <td>-0.054482</td>\n      <td>-0.882156</td>\n      <td>-0.103225</td>\n      <td>-0.139071</td>\n      <td>-0.093794</td>\n      <td>-0.255395</td>\n      <td>0.142333</td>\n      <td>0.015145</td>\n      <td>-0.393591</td>\n      <td>-0.019127</td>\n      <td>-0.010975</td>\n      <td>-0.12846</td>\n      <td>0.749926</td>\n      <td>-0.132286</td>\n      <td>-0.097601</td>\n      <td>0.653461</td>\n      <td>-0.260909</td>\n      <td>-0.369677</td>\n      <td>0.439143</td>\n      <td>-0.776308</td>\n      <td>0.600769</td>\n      <td>-0.445175</td>\n      <td>1.728853</td>\n      <td>-0.695598</td>\n      <td>0.886489</td>\n      <td>2.487035</td>\n      <td>-0.457625</td>\n      <td>0.631473</td>\n      <td>-0.550961</td>\n      <td>-0.463603</td>\n      <td>-0.264231</td>\n      <td>-0.109683</td>\n      <td>-0.153248</td>\n      <td>-0.258734</td>\n      <td>-0.377342</td>\n      <td>-0.391875</td>\n      <td>-0.393032</td>\n      <td>-0.411170</td>\n      <td>-0.416807</td>\n      <td>-0.154662</td>\n      <td>-0.057476</td>\n      <td>0.724583</td>\n      <td>-1.054889</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.931095</td>\n      <td>0.913152</td>\n      <td>0.991538</td>\n      <td>1.000000e+00</td>\n      <td>1.0</td>\n      <td>9.996576e-01</td>\n      <td>1.000000</td>\n      <td>0.596329</td>\n      <td>0.942769</td>\n      <td>0.526827</td>\n      <td>0.29385</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.000000</td>\n      <td>0.017943</td>\n      <td>-0.060444</td>\n      <td>-0.068905</td>\n      <td>-0.068905</td>\n      <td>-0.068563</td>\n      <td>-0.068905</td>\n      <td>0.334766</td>\n      <td>-0.011674</td>\n      <td>0.404268</td>\n      <td>0.637245</td>\n      <td>-0.068905</td>\n      <td>-0.068905</td>\n      <td>-0.068905</td>\n      <td>-0.078387</td>\n      <td>-0.086848</td>\n      <td>-0.086848</td>\n      <td>-0.086506</td>\n      <td>-0.086848</td>\n      <td>0.316823</td>\n      <td>-0.029617</td>\n      <td>0.386324</td>\n      <td>0.619301</td>\n      <td>-0.086848</td>\n      <td>-0.086848</td>\n      <td>-0.086848</td>\n      <td>-0.008462</td>\n      <td>-0.008462</td>\n      <td>-0.008119</td>\n      <td>-0.008462</td>\n      <td>0.395210</td>\n      <td>0.048770</td>\n      <td>0.464711</td>\n      <td>0.697688</td>\n      <td>-0.008462</td>\n      <td>-0.008462</td>\n      <td>-0.008462</td>\n      <td>0.000000</td>\n      <td>3.423691e-04</td>\n      <td>0.000000</td>\n      <td>0.403671</td>\n      <td>0.057231</td>\n      <td>0.473173</td>\n      <td>7.061496e-01</td>\n      <td>0.000000e+00</td>\n      <td>0.000000e+00</td>\n      <td>0.000000</td>\n      <td>0.000342</td>\n      <td>0.000000</td>\n      <td>0.403671</td>\n      <td>0.057231</td>\n      <td>0.473173</td>\n      <td>0.70615</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>-0.000342</td>\n      <td>0.403329</td>\n      <td>0.056889</td>\n      <td>0.472831</td>\n      <td>7.058073e-01</td>\n      <td>-3.423691e-04</td>\n      <td>-3.423691e-04</td>\n      <td>-0.000342</td>\n      <td>0.403671</td>\n      <td>0.057231</td>\n      <td>0.473173</td>\n      <td>0.706150</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>-0.346440</td>\n      <td>0.069501</td>\n      <td>0.302478</td>\n      <td>-0.403671</td>\n      <td>-0.403671</td>\n      <td>-0.403671</td>\n      <td>0.415941</td>\n      <td>0.648918</td>\n      <td>-0.057231</td>\n      <td>-0.057231</td>\n      <td>-0.057231</td>\n      <td>0.232977</td>\n      <td>-0.473173</td>\n      <td>-0.473173</td>\n      <td>-0.473173</td>\n      <td>-0.70615</td>\n      <td>-0.70615</td>\n      <td>-0.706150</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.226761</td>\n      <td>0.871087</td>\n      <td>0.985101</td>\n      <td>0.991199</td>\n      <td>-0.006097</td>\n      <td>0.004311</td>\n      <td>0.988150</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"stacked_train_1.to_csv('stacked_train_1', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-12-10T07:03:08.243842Z","iopub.execute_input":"2022-12-10T07:03:08.244347Z","iopub.status.idle":"2022-12-10T07:03:11.034664Z","shell.execute_reply.started":"2022-12-10T07:03:08.244317Z","shell.execute_reply":"2022-12-10T07:03:11.033522Z"},"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"code","source":"joblib.dump(Stacker_1, 'Stacker_1.h5')\nStacker_1","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-12-10T07:36:55.111014Z","iopub.execute_input":"2022-12-10T07:36:55.111443Z","iopub.status.idle":"2022-12-10T07:36:56.784975Z","shell.execute_reply.started":"2022-12-10T07:36:55.111411Z","shell.execute_reply":"2022-12-10T07:36:56.783955Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Keras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n...layers\n......dense\n.........vars\n............0\n............1\n......dense_1\n.........vars\n............0\n............1\n......dense_2\n.........vars\n............0\n............1\n......dense_3\n.........vars\n............0\n............1\n......normalization\n.........vars\n............0\n............1\n............2\n...metrics\n......mean\n.........vars\n............0\n............1\n...optimizer\n......vars\n.........0\n.........1\n.........10\n.........11\n.........12\n.........13\n.........14\n.........15\n.........16\n.........2\n.........3\n.........4\n.........5\n.........6\n.........7\n.........8\n.........9\n...vars\nKeras model archive saving:\nFile Name                                             Modified             Size\nconfig.json                                    2022-12-10 07:36:55         2825\nvariables.h5                                   2022-12-10 07:36:55        98832\nmetadata.json                                  2022-12-10 07:36:55           64\nKeras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n...layers\n......dense\n.........vars\n............0\n............1\n......dense_1\n.........vars\n............0\n............1\n......dense_2\n.........vars\n............0\n............1\n......dense_3\n.........vars\n............0\n............1\n......normalization\n.........vars\n............0\n............1\n............2\n...metrics\n......mean\n.........vars\n............0\n............1\n...optimizer\n......vars\n.........0\n.........1\n.........10\n.........11\n.........12\n.........13\n.........14\n.........15\n.........16\n.........2\n.........3\n.........4\n.........5\n.........6\n.........7\n.........8\n.........9\n...vars\nKeras model archive saving:\nFile Name                                             Modified             Size\nconfig.json                                    2022-12-10 07:36:55         2833\nvariables.h5                                   2022-12-10 07:36:55        98832\nmetadata.json                                  2022-12-10 07:36:55           64\nKeras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n...layers\n......dense\n.........vars\n............0\n............1\n......dense_1\n.........vars\n............0\n............1\n......dense_2\n.........vars\n............0\n............1\n......dense_3\n.........vars\n............0\n............1\n......normalization\n.........vars\n............0\n............1\n............2\n...metrics\n......mean\n.........vars\n............0\n............1\n...optimizer\n......vars\n.........0\n.........1\n.........10\n.........11\n.........12\n.........13\n.........14\n.........15\n.........16\n.........2\n.........3\n.........4\n.........5\n.........6\n.........7\n.........8\n.........9\n...vars\nKeras model archive saving:\nFile Name                                             Modified             Size\nconfig.json                                    2022-12-10 07:36:55         2835\nvariables.h5                                   2022-12-10 07:36:55        98832\nmetadata.json                                  2022-12-10 07:36:55           64\nKeras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n...layers\n......dense\n.........vars\n............0\n............1\n......dense_1\n.........vars\n............0\n............1\n......dense_2\n.........vars\n............0\n............1\n......dense_3\n.........vars\n............0\n............1\n......normalization\n.........vars\n............0\n............1\n............2\n...metrics\n......mean\n.........vars\n............0\n............1\n...optimizer\n......vars\n.........0\n.........1\n.........10\n.........11\n.........12\n.........13\n.........14\n.........15\n.........16\n.........2\n.........3\n.........4\n.........5\n.........6\n.........7\n.........8\n.........9\n...vars\nKeras model archive saving:\nFile Name                                             Modified             Size\nconfig.json                                    2022-12-10 07:36:55         2837\nvariables.h5                                   2022-12-10 07:36:55        98832\nmetadata.json                                  2022-12-10 07:36:55           64\nKeras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n...layers\n......dense\n.........vars\n............0\n............1\n......dense_1\n.........vars\n............0\n............1\n......dense_2\n.........vars\n............0\n............1\n......normalization\n.........vars\n............0\n............1\n............2\n...metrics\n......mean\n.........vars\n............0\n............1\n...optimizer\n......vars\n.........0\n.........1\n.........10\n.........11\n.........12\n.........2\n.........3\n.........4\n.........5\n.........6\n.........7\n.........8\n.........9\n...vars\nKeras model archive saving:\nFile Name                                             Modified             Size\nconfig.json                                    2022-12-10 07:36:55         2413\nvariables.h5                                   2022-12-10 07:36:55       135656\nmetadata.json                                  2022-12-10 07:36:55           64\nKeras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n...layers\n......dense\n.........vars\n............0\n............1\n......dense_1\n.........vars\n............0\n............1\n......dense_2\n.........vars\n............0\n............1\n......normalization\n.........vars\n............0\n............1\n............2\n...metrics\n......mean\n.........vars\n............0\n............1\n...optimizer\n......vars\n.........0\n.........1\n.........10\n.........11\n.........12\n.........2\n.........3\n.........4\n.........5\n.........6\n.........7\n.........8\n.........9\n...vars\nKeras model archive saving:\nFile Name                                             Modified             Size\nconfig.json                                    2022-12-10 07:36:55         2413\nvariables.h5                                   2022-12-10 07:36:55       135656\nmetadata.json                                  2022-12-10 07:36:55           64\nKeras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n...layers\n......dense\n.........vars\n............0\n............1\n......dense_1\n.........vars\n............0\n............1\n......dense_2\n.........vars\n............0\n............1\n......normalization\n.........vars\n............0\n............1\n............2\n...metrics\n......mean\n.........vars\n............0\n............1\n...optimizer\n......vars\n.........0\n.........1\n.........10\n.........11\n.........12\n.........2\n.........3\n.........4\n.........5\n.........6\n.........7\n.........8\n.........9\n...vars\nKeras model archive saving:\nFile Name                                             Modified             Size\nconfig.json                                    2022-12-10 07:36:55         2413\nvariables.h5                                   2022-12-10 07:36:55       135656\nmetadata.json                                  2022-12-10 07:36:55           64\nKeras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n...layers\n......dense\n.........vars\n............0\n............1\n......dense_1\n.........vars\n............0\n............1\n......dense_2\n.........vars\n............0\n............1\n......normalization\n.........vars\n............0\n............1\n............2\n...metrics\n......mean\n.........vars\n............0\n............1\n...optimizer\n......vars\n.........0\n.........1\n.........10\n.........11\n.........12\n.........2\n.........3\n.........4\n.........5\n.........6\n.........7\n.........8\n.........9\n...vars\nKeras model archive saving:\nFile Name                                             Modified             Size\nconfig.json                                    2022-12-10 07:36:55         2413\nvariables.h5                                   2022-12-10 07:36:55       135656\nmetadata.json                                  2022-12-10 07:36:55           64\nKeras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n...layers\n......dense\n.........vars\n............0\n............1\n......dense_1\n.........vars\n............0\n............1\n......normalization\n.........vars\n............0\n............1\n............2\n...metrics\n......mean\n.........vars\n............0\n............1\n...optimizer\n......vars\n.........0\n.........1\n.........2\n.........3\n.........4\n.........5\n.........6\n.........7\n.........8\n...vars\nKeras model archive saving:\nFile Name                                             Modified             Size\nconfig.json                                    2022-12-10 07:36:55         1987\nvariables.h5                                   2022-12-10 07:36:55        66096\nmetadata.json                                  2022-12-10 07:36:55           64\nKeras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n...layers\n......dense\n.........vars\n............0\n............1\n......dense_1\n.........vars\n............0\n............1\n......normalization\n.........vars\n............0\n............1\n............2\n...metrics\n......mean\n.........vars\n............0\n............1\n...optimizer\n......vars\n.........0\n.........1\n.........2\n.........3\n.........4\n.........5\n.........6\n.........7\n.........8\n...vars\nKeras model archive saving:\nFile Name                                             Modified             Size\nconfig.json                                    2022-12-10 07:36:55         1987\nvariables.h5                                   2022-12-10 07:36:55        66096\nmetadata.json                                  2022-12-10 07:36:55           64\nKeras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n...layers\n......dense\n.........vars\n............0\n............1\n......dense_1\n.........vars\n............0\n............1\n......normalization\n.........vars\n............0\n............1\n............2\n...metrics\n......mean\n.........vars\n............0\n............1\n...optimizer\n......vars\n.........0\n.........1\n.........2\n.........3\n.........4\n.........5\n.........6\n.........7\n.........8\n...vars\nKeras model archive saving:\nFile Name                                             Modified             Size\nconfig.json                                    2022-12-10 07:36:55         1990\nvariables.h5                                   2022-12-10 07:36:55        66096\nmetadata.json                                  2022-12-10 07:36:55           64\nKeras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n...layers\n......dense\n.........vars\n............0\n............1\n......dense_1\n.........vars\n............0\n............1\n......normalization\n.........vars\n............0\n............1\n............2\n...metrics\n......mean\n.........vars\n............0\n............1\n...optimizer\n......vars\n.........0\n.........1\n.........2\n.........3\n.........4\n.........5\n.........6\n.........7\n.........8\n...vars\nKeras model archive saving:\nFile Name                                             Modified             Size\nconfig.json                                    2022-12-10 07:36:55         1990\nvariables.h5                                   2022-12-10 07:36:55        66096\nmetadata.json                                  2022-12-10 07:36:55           64\n","output_type":"stream"},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"Stacker(objective: binary            \n        auto: True            \n        num_auto_layers: 2            \n        metafeats: True            \n        epochs : 200            \n        gridsearch_iterations: 10            \n        stacking_feats_depth: 1            \n        include_X: False            \n        verbose : True)"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Stacker_1_trained_models = Stacker_1.trained_models","metadata":{"execution":{"iopub.status.busy":"2022-12-10T07:03:11.976034Z","iopub.execute_input":"2022-12-10T07:03:11.976405Z","iopub.status.idle":"2022-12-10T07:03:11.982529Z","shell.execute_reply.started":"2022-12-10T07:03:11.976374Z","shell.execute_reply":"2022-12-10T07:03:11.981598Z"},"trusted":true},"outputs":[],"execution_count":10},{"cell_type":"code","source":"joblib.dump(Stacker_1_trained_models, 'Stacker_1_trained_models.pkl')","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-12-10T07:03:11.983811Z","iopub.execute_input":"2022-12-10T07:03:11.984820Z","iopub.status.idle":"2022-12-10T07:03:12.942874Z","shell.execute_reply.started":"2022-12-10T07:03:11.984786Z","shell.execute_reply":"2022-12-10T07:03:12.941766Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Keras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n...layers\n......dense\n.........vars\n............0\n............1\n......dense_1\n.........vars\n............0\n............1\n......dense_2\n.........vars\n............0\n............1\n......dense_3\n.........vars\n............0\n............1\n......normalization\n.........vars\n............0\n............1\n............2\n...metrics\n......mean\n.........vars\n............0\n............1\n...optimizer\n......vars\n.........0\n.........1\n.........10\n.........11\n.........12\n.........13\n.........14\n.........15\n.........16\n.........2\n.........3\n.........4\n.........5\n.........6\n.........7\n.........8\n.........9\n...vars\nKeras model archive saving:\nFile Name                                             Modified             Size\nconfig.json                                    2022-12-10 07:03:12         2825\nvariables.h5                                   2022-12-10 07:03:12        98832\nmetadata.json                                  2022-12-10 07:03:12           64\nKeras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n...layers\n......dense\n.........vars\n............0\n............1\n......dense_1\n.........vars\n............0\n............1\n......dense_2\n.........vars\n............0\n............1\n......dense_3\n.........vars\n............0\n............1\n......normalization\n.........vars\n............0\n............1\n............2\n...metrics\n......mean\n.........vars\n............0\n............1\n...optimizer\n......vars\n.........0\n.........1\n.........10\n.........11\n.........12\n.........13\n.........14\n.........15\n.........16\n.........2\n.........3\n.........4\n.........5\n.........6\n.........7\n.........8\n.........9\n...vars\nKeras model archive saving:\nFile Name                                             Modified             Size\nconfig.json                                    2022-12-10 07:03:12         2833\nvariables.h5                                   2022-12-10 07:03:12        98832\nmetadata.json                                  2022-12-10 07:03:12           64\nKeras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n...layers\n......dense\n.........vars\n............0\n............1\n......dense_1\n.........vars\n............0\n............1\n......dense_2\n.........vars\n............0\n............1\n......dense_3\n.........vars\n............0\n............1\n......normalization\n.........vars\n............0\n............1\n............2\n...metrics\n......mean\n.........vars\n............0\n............1\n...optimizer\n......vars\n.........0\n.........1\n.........10\n.........11\n.........12\n.........13\n.........14\n.........15\n.........16\n.........2\n.........3\n.........4\n.........5\n.........6\n.........7\n.........8\n.........9\n...vars\nKeras model archive saving:\nFile Name                                             Modified             Size\nconfig.json                                    2022-12-10 07:03:12         2835\nvariables.h5                                   2022-12-10 07:03:12        98832\nmetadata.json                                  2022-12-10 07:03:12           64\nKeras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n...layers\n......dense\n.........vars\n............0\n............1\n......dense_1\n.........vars\n............0\n............1\n......dense_2\n.........vars\n............0\n............1\n......dense_3\n.........vars\n............0\n............1\n......normalization\n.........vars\n............0\n............1\n............2\n...metrics\n......mean\n.........vars\n............0\n............1\n...optimizer\n......vars\n.........0\n.........1\n.........10\n.........11\n.........12\n.........13\n.........14\n.........15\n.........16\n.........2\n.........3\n.........4\n.........5\n.........6\n.........7\n.........8\n.........9\n...vars\nKeras model archive saving:\nFile Name                                             Modified             Size\nconfig.json                                    2022-12-10 07:03:12         2837\nvariables.h5                                   2022-12-10 07:03:12        98832\nmetadata.json                                  2022-12-10 07:03:12           64\nKeras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n...layers\n......dense\n.........vars\n............0\n............1\n......dense_1\n.........vars\n............0\n............1\n......dense_2\n.........vars\n............0\n............1\n......normalization\n.........vars\n............0\n............1\n............2\n...metrics\n......mean\n.........vars\n............0\n............1\n...optimizer\n......vars\n.........0\n.........1\n.........10\n.........11\n.........12\n.........2\n.........3\n.........4\n.........5\n.........6\n.........7\n.........8\n.........9\n...vars\nKeras model archive saving:\nFile Name                                             Modified             Size\nconfig.json                                    2022-12-10 07:03:12         2413\nvariables.h5                                   2022-12-10 07:03:12       135656\nmetadata.json                                  2022-12-10 07:03:12           64\nKeras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n...layers\n......dense\n.........vars\n............0\n............1\n......dense_1\n.........vars\n............0\n............1\n......dense_2\n.........vars\n............0\n............1\n......normalization\n.........vars\n............0\n............1\n............2\n...metrics\n......mean\n.........vars\n............0\n............1\n...optimizer\n......vars\n.........0\n.........1\n.........10\n.........11\n.........12\n.........2\n.........3\n.........4\n.........5\n.........6\n.........7\n.........8\n.........9\n...vars\nKeras model archive saving:\nFile Name                                             Modified             Size\nconfig.json                                    2022-12-10 07:03:12         2413\nvariables.h5                                   2022-12-10 07:03:12       135656\nmetadata.json                                  2022-12-10 07:03:12           64\nKeras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n...layers\n......dense\n.........vars\n............0\n............1\n......dense_1\n.........vars\n............0\n............1\n......dense_2\n.........vars\n............0\n............1\n......normalization\n.........vars\n............0\n............1\n............2\n...metrics\n......mean\n.........vars\n............0\n............1\n...optimizer\n......vars\n.........0\n.........1\n.........10\n.........11\n.........12\n.........2\n.........3\n.........4\n.........5\n.........6\n.........7\n.........8\n.........9\n...vars\nKeras model archive saving:\nFile Name                                             Modified             Size\nconfig.json                                    2022-12-10 07:03:12         2413\nvariables.h5                                   2022-12-10 07:03:12       135656\nmetadata.json                                  2022-12-10 07:03:12           64\nKeras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n...layers\n......dense\n.........vars\n............0\n............1\n......dense_1\n.........vars\n............0\n............1\n......dense_2\n.........vars\n............0\n............1\n......normalization\n.........vars\n............0\n............1\n............2\n...metrics\n......mean\n.........vars\n............0\n............1\n...optimizer\n......vars\n.........0\n.........1\n.........10\n.........11\n.........12\n.........2\n.........3\n.........4\n.........5\n.........6\n.........7\n.........8\n.........9\n...vars\nKeras model archive saving:\nFile Name                                             Modified             Size\nconfig.json                                    2022-12-10 07:03:12         2413\nvariables.h5                                   2022-12-10 07:03:12       135656\nmetadata.json                                  2022-12-10 07:03:12           64\nKeras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n...layers\n......dense\n.........vars\n............0\n............1\n......dense_1\n.........vars\n............0\n............1\n......normalization\n.........vars\n............0\n............1\n............2\n...metrics\n......mean\n.........vars\n............0\n............1\n...optimizer\n......vars\n.........0\n.........1\n.........2\n.........3\n.........4\n.........5\n.........6\n.........7\n.........8\n...vars\nKeras model archive saving:\nFile Name                                             Modified             Size\nconfig.json                                    2022-12-10 07:03:12         1987\nvariables.h5                                   2022-12-10 07:03:12        66096\nmetadata.json                                  2022-12-10 07:03:12           64\nKeras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n...layers\n......dense\n.........vars\n............0\n............1\n......dense_1\n.........vars\n............0\n............1\n......normalization\n.........vars\n............0\n............1\n............2\n...metrics\n......mean\n.........vars\n............0\n............1\n...optimizer\n......vars\n.........0\n.........1\n.........2\n.........3\n.........4\n.........5\n.........6\n.........7\n.........8\n...vars\nKeras model archive saving:\nFile Name                                             Modified             Size\nconfig.json                                    2022-12-10 07:03:12         1987\nvariables.h5                                   2022-12-10 07:03:12        66096\nmetadata.json                                  2022-12-10 07:03:12           64\nKeras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n...layers\n......dense\n.........vars\n............0\n............1\n......dense_1\n.........vars\n............0\n............1\n......normalization\n.........vars\n............0\n............1\n............2\n...metrics\n......mean\n.........vars\n............0\n............1\n...optimizer\n......vars\n.........0\n.........1\n.........2\n.........3\n.........4\n.........5\n.........6\n.........7\n.........8\n...vars\nKeras model archive saving:\nFile Name                                             Modified             Size\nconfig.json                                    2022-12-10 07:03:12         1990\nvariables.h5                                   2022-12-10 07:03:12        66096\nmetadata.json                                  2022-12-10 07:03:12           64\nKeras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n...layers\n......dense\n.........vars\n............0\n............1\n......dense_1\n.........vars\n............0\n............1\n......normalization\n.........vars\n............0\n............1\n............2\n...metrics\n......mean\n.........vars\n............0\n............1\n...optimizer\n......vars\n.........0\n.........1\n.........2\n.........3\n.........4\n.........5\n.........6\n.........7\n.........8\n...vars\nKeras model archive saving:\nFile Name                                             Modified             Size\nconfig.json                                    2022-12-10 07:03:12         1990\nvariables.h5                                   2022-12-10 07:03:12        66096\nmetadata.json                                  2022-12-10 07:03:12           64\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"['Stacker_1_trained_models.pkl']"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"Stacker_1","metadata":{"execution":{"iopub.status.busy":"2022-12-10T07:36:35.007290Z","iopub.execute_input":"2022-12-10T07:36:35.008239Z","iopub.status.idle":"2022-12-10T07:36:35.019194Z","shell.execute_reply.started":"2022-12-10T07:36:35.008196Z","shell.execute_reply":"2022-12-10T07:36:35.017609Z"},"trusted":true},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"Stacker(objective: binary            \n        auto: True            \n        num_auto_layers: 2            \n        metafeats: True            \n        epochs : 200            \n        gridsearch_iterations: 10            \n        stacking_feats_depth: 1            \n        include_X: False            \n        verbose : True)"},"metadata":{}}],"execution_count":15},{"cell_type":"markdown","source":"## **4. 테스트 특성만 이용하여 train_set을 Stacker로 fit_transform해 줌 => Stacker_2, stacked_train_2라 명명 후 둘 다 저장**","metadata":{}},{"cell_type":"markdown","source":"test_df = pd.read_csv('/kaggle/input/daconio-117/test_df.csv')\ntest_col = test_df.columns\nX_train_test_col = X_train[test_col]\nX_train_test_col.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-12-09T14:00:01.005265Z","iopub.status.idle":"2022-12-09T14:00:01.005979Z","shell.execute_reply.started":"2022-12-09T14:00:01.005747Z","shell.execute_reply":"2022-12-09T14:00:01.005774Z"}}},{"cell_type":"markdown","source":"Stacker_2 = Stacker(objective = 'binary', auto = True)\nstacked_train_2 = Stacker_2.fit_transform(X_train_test_col, y_train)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-12-09T14:00:01.007886Z","iopub.status.idle":"2022-12-09T14:00:01.008277Z","shell.execute_reply.started":"2022-12-09T14:00:01.008090Z","shell.execute_reply":"2022-12-09T14:00:01.008108Z"}}},{"cell_type":"markdown","source":"print(stacked_train_2.shape)\nstacked_train_2.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-12-09T14:00:01.009811Z","iopub.status.idle":"2022-12-09T14:00:01.010189Z","shell.execute_reply.started":"2022-12-09T14:00:01.010006Z","shell.execute_reply":"2022-12-09T14:00:01.010023Z"}}},{"cell_type":"markdown","source":"stacked_train_2.to_csv('stacked_train_2', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-12-09T14:00:01.011446Z","iopub.status.idle":"2022-12-09T14:00:01.011826Z","shell.execute_reply.started":"2022-12-09T14:00:01.011636Z","shell.execute_reply":"2022-12-09T14:00:01.011653Z"}}},{"cell_type":"markdown","source":"joblib.dump(Stacker_2, 'Stacker_2.pkl')\nStacker_2","metadata":{"execution":{"iopub.status.busy":"2022-12-09T14:00:01.013361Z","iopub.status.idle":"2022-12-09T14:00:01.014033Z","shell.execute_reply.started":"2022-12-09T14:00:01.013731Z","shell.execute_reply":"2022-12-09T14:00:01.013759Z"}}},{"cell_type":"markdown","source":"## **5. Stacker_1을 이용하여 valid_set transform해주어 stacked_valid_1을 만듦 => stacked_valid_1 저장 후 호출하는 함수 만들기**","metadata":{}},{"cell_type":"markdown","source":"X_valid.reset_index(inplace=True, drop=True)\ny_valid.reset_index(drop=True, inplace=True)\nX_valid.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-12-09T14:00:01.015362Z","iopub.status.idle":"2022-12-09T14:00:01.016222Z","shell.execute_reply.started":"2022-12-09T14:00:01.015895Z","shell.execute_reply":"2022-12-09T14:00:01.015923Z"}}},{"cell_type":"markdown","source":"stacked_valid_1 = Stacker_1.transform(X_valid)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-12-09T14:00:01.018254Z","iopub.status.idle":"2022-12-09T14:00:01.019368Z","shell.execute_reply.started":"2022-12-09T14:00:01.019050Z","shell.execute_reply":"2022-12-09T14:00:01.019078Z"}}},{"cell_type":"markdown","source":"print(stacked_valid_1.shape)\nstacked_valid_1.head()","metadata":{"execution":{"iopub.status.busy":"2022-12-09T14:00:01.020708Z","iopub.status.idle":"2022-12-09T14:00:01.021449Z","shell.execute_reply.started":"2022-12-09T14:00:01.021131Z","shell.execute_reply":"2022-12-09T14:00:01.021159Z"}}},{"cell_type":"markdown","source":"stacked_valid_1.to_csv('stacked_valid_1', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-12-09T14:00:01.023234Z","iopub.status.idle":"2022-12-09T14:00:01.024047Z","shell.execute_reply.started":"2022-12-09T14:00:01.023737Z","shell.execute_reply":"2022-12-09T14:00:01.023766Z"}}},{"cell_type":"markdown","source":"## **6. Stacker_2를 이용하여 valid_set transform해주어 stacked_valid_2를 만듦 => stacked_valid_2 저장 후 호출하는 함수 만들기**","metadata":{}},{"cell_type":"markdown","source":"X_valid_test_col = X_valid[test_col]\nX_valid_test_col","metadata":{"execution":{"iopub.status.busy":"2022-12-09T14:00:01.025610Z","iopub.status.idle":"2022-12-09T14:00:01.026415Z","shell.execute_reply.started":"2022-12-09T14:00:01.026212Z","shell.execute_reply":"2022-12-09T14:00:01.026232Z"}}},{"cell_type":"markdown","source":"stacked_valid_2 = Stacker_2.transform(X_valid_test_col)","metadata":{"execution":{"iopub.status.busy":"2022-12-09T14:00:01.027503Z","iopub.status.idle":"2022-12-09T14:00:01.028514Z","shell.execute_reply.started":"2022-12-09T14:00:01.028295Z","shell.execute_reply":"2022-12-09T14:00:01.028317Z"}}},{"cell_type":"markdown","source":"print(stacked_valid_2.shape)\nstacked_valid_2.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-12-09T14:00:01.029578Z","iopub.status.idle":"2022-12-09T14:00:01.030042Z","shell.execute_reply.started":"2022-12-09T14:00:01.029853Z","shell.execute_reply":"2022-12-09T14:00:01.029871Z"}}},{"cell_type":"markdown","source":"stacked_valid_2.to_csv('stacked_valid_2', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-12-09T14:00:01.031618Z","iopub.status.idle":"2022-12-09T14:00:01.031985Z","shell.execute_reply.started":"2022-12-09T14:00:01.031804Z","shell.execute_reply":"2022-12-09T14:00:01.031821Z"}}},{"cell_type":"markdown","source":"## **7. stacked_train_1을 이용하여 ml_1모델(1차적으로 LGBM Tuner로, 최종적으로 앙상블한 최종 모델 사용할 예정)을 훈련시킨 후 stacked_valid_1을 이용하여 valid_score를 작성=> valid_score 저장**","metadata":{}},{"cell_type":"markdown","source":"'''/kaggle/input/michaels-order1/Stacker_2.pkl\n/kaggle/input/michaels-order1/Stacker_1.pkl\n/kaggle/input/michaels-order1/stacked_valid_2\n/kaggle/input/michaels-order1/stacked_valid_1\n/kaggle/input/michaels-order1/stacked_train_1\n/kaggle/input/michaels-order1/stacked_train_2'''","metadata":{"execution":{"iopub.status.busy":"2022-12-09T14:00:01.033296Z","iopub.status.idle":"2022-12-09T14:00:01.033681Z","shell.execute_reply.started":"2022-12-09T14:00:01.033480Z","shell.execute_reply":"2022-12-09T14:00:01.033497Z"}}},{"cell_type":"markdown","source":"Stacker_2 = joblib.load('/kaggle/input/michaels-order1/Stacker_2.pkl')\nStacker_1 = joblib.load('/kaggle/input/michaels-order1/Stacker_1.pkl')","metadata":{"execution":{"iopub.status.busy":"2022-12-09T14:00:01.035280Z","iopub.status.idle":"2022-12-09T14:00:01.035679Z","shell.execute_reply.started":"2022-12-09T14:00:01.035473Z","shell.execute_reply":"2022-12-09T14:00:01.035490Z"}}},{"cell_type":"markdown","source":"stacked_valid_2 = pd.read_csv('/kaggle/input/michaels-order1/stacked_valid_2')\nstacked_valid_1 = pd.read_csv('/kaggle/input/michaels-order1/stacked_valid_1')\nstacked_train_1 = pd.read_csv('/kaggle/input/michaels-order1/stacked_train_1')\nstacked_train_2 = pd.read_csv('/kaggle/input/michaels-order1/stacked_train_2')","metadata":{"execution":{"iopub.status.busy":"2022-12-09T14:00:01.036822Z","iopub.status.idle":"2022-12-09T14:00:01.037580Z","shell.execute_reply.started":"2022-12-09T14:00:01.037363Z","shell.execute_reply":"2022-12-09T14:00:01.037382Z"}}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}